{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [2 3]\n",
      " [3 4]\n",
      " [4 5]\n",
      " [5 6]]\n",
      "[[ 3]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# x_data=np.array([1,2,3,4,5]).reshape(5,1)\n",
    "x_data=np.array([[1,2],[2,3],[3,4],[4,5],[5,6]]).reshape(5,2)\n",
    "t_data=np.array([3,5,7,9,11]).reshape(5,1) #->y=2x+1의 식\n",
    "print(x_data)\n",
    "print(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.9385281 ],\n",
       "        [0.31741384]]),\n",
       " array([0.63251634]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=np.random.rand(2,1)\n",
    "b=np.random.rand(1)\n",
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비용 함수(Cost function) 또는 손실 함수(Loss function) \\\n",
    "기계 학습에서 모델의 예측값과 실제 관측값(라벨 또는 타겟 값) 사이의 차이를 측정하는 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수 수식\n",
    "# 평균 제곱 오차(Mean Squared Error, MSE)를 계산 \n",
    "# MSE는 회귀 문제에서 주로 사용되는 손실 함수로, 모델이 예측한 값과 실제 라벨 간의 차이를 측정하는 데 사용\n",
    "\n",
    "#x:입력값 t;정답\n",
    "def loss_func(x,t): #y=xw+b\n",
    "    y=np.dot(x,w)+b\n",
    "    \n",
    "    return (np.sum((t-y)**2))/len(x)\n",
    "\n",
    "#def loss_func(x, t):: 손실 함수 loss_func는 두 개의 인자 x와 t를 받습니다. 여기서 x는 입력 데이터의 배열이고, t는 해당 입력 데이터에 대한 실제 라벨 또는 타겟값의 배열입니다.\n",
    "# y = np.dot(x, w) + b: 주어진 입력 데이터 x와 가중치 w를 내적하고 편향 b를 더하여 예측값 y를 계산합니다. 이 과정은 선형 회귀 모델의 예측을 나타냅니다.\n",
    "# (np.sum((t - y)**2)) / len(x): 예측값 y와 실제 라벨 t 간의 평균 제곱 오차(MSE)를 계산합니다.\n",
    "# 먼저 (t - y)**2를 통해 각 데이터 포인트에서 오차를 제곱합니다. 그런 다음 np.sum을 사용하여 모든 데이터 포인트의 제곱 오차를 더합니다. 최종적으로 len(x)로 나누어 데이터 포인트의 수로 평균을 구합니다.\n",
    "# 최종적으로 계산된 평균 제곱 오차(MSE)를 반환합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 수치 미분 구현\n",
    "def numerical_derivative(f,x):\n",
    "    delta_x=1e-4\n",
    "    grad=np.zeros_like(x)\n",
    "    \n",
    "    it=np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx=it.multi_index\n",
    "        tmp_val=x[idx]\n",
    "        x[idx]=float(tmp_val)+delta_x\n",
    "        fx1=f(x) # f(x+delta_x)\n",
    "\n",
    "        x[idx]=float(tmp_val)-delta_x\n",
    "        fx2=f(x) # f(x-delta_x)\n",
    "        grad[idx]=(fx1-fx2)/(2*delta_x)\n",
    "\n",
    "        x[idx]=tmp_val\n",
    "        it.iternext()\n",
    "\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    y=np.dot(x,w)+b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value= 6.315882529382806 \n",
      " Initial w= [[0.9385281 ]\n",
      " [0.31741384]] \n",
      " b= [0.63251634] \n",
      "\n",
      "step= 0 \n",
      " error value= 1.102111832671793 \n",
      " w= [[1.10522506]\n",
      " [0.52975568]] \n",
      " b= [0.65117206] \n",
      "\n",
      "step= 400 \n",
      " error value= 0.0008348093353165959 \n",
      " w= [[1.36615324]\n",
      " [0.61477101]] \n",
      " b= [0.45280229] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 800 \n",
      " error value= 3.199482534229455e-05 \n",
      " w= [[1.39427396]\n",
      " [0.60199158]] \n",
      " b= [0.41123725] \n",
      "\n",
      "step= 1200 \n",
      " error value= 1.2262307156572416e-06 \n",
      " w= [[1.39977916]\n",
      " [0.59948975]] \n",
      " b= [0.40310006] \n",
      "\n",
      "step= 1600 \n",
      " error value= 4.699640494809314e-08 \n",
      " w= [[1.40085691]\n",
      " [0.59899997]] \n",
      " b= [0.40150704] \n",
      "\n",
      "step= 2000 \n",
      " error value= 1.8011798675946086e-09 \n",
      " w= [[1.4010679 ]\n",
      " [0.59890408]] \n",
      " b= [0.40119518] \n",
      "\n",
      "step= 2400 \n",
      " error value= 6.903185293973326e-11 \n",
      " w= [[1.40110921]\n",
      " [0.59888531]] \n",
      " b= [0.40113412] \n",
      "\n",
      "step= 2800 \n",
      " error value= 2.645708408777801e-12 \n",
      " w= [[1.40111729]\n",
      " [0.59888163]] \n",
      " b= [0.40112217] \n",
      "\n",
      "step= 3200 \n",
      " error value= 1.0139917574599505e-13 \n",
      " w= [[1.40111888]\n",
      " [0.59888091]] \n",
      " b= [0.40111983] \n",
      "\n",
      "step= 3600 \n",
      " error value= 3.886215476493704e-15 \n",
      " w= [[1.40111918]\n",
      " [0.59888077]] \n",
      " b= [0.40111937] \n",
      "\n",
      "step= 4000 \n",
      " error value= 1.4894273135678165e-16 \n",
      " w= [[1.40111925]\n",
      " [0.59888075]] \n",
      " b= [0.40111928] \n",
      "\n",
      "step= 4400 \n",
      " error value= 5.70836545353888e-18 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 4800 \n",
      " error value= 2.187785727309389e-19 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 5200 \n",
      " error value= 8.384809506449442e-21 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 5600 \n",
      " error value= 3.2134897011578643e-22 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 6000 \n",
      " error value= 1.2316902660077565e-23 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 6400 \n",
      " error value= 4.727400435830715e-25 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 6800 \n",
      " error value= 1.8206594148138863e-26 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 7200 \n",
      " error value= 6.626826034309109e-28 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 7600 \n",
      " error value= 7.98327236083664e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 8000 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 8400 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 8800 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 9200 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 9600 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 10000 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 10400 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 10800 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 11200 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 11600 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 12000 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 12400 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 12800 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 13200 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 13600 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 14000 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 14400 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 14800 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 15200 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 15600 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 16000 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 16400 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 16800 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 17200 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 17600 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 18000 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 18400 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 18800 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 19200 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 19600 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n",
      "step= 20000 \n",
      " error value= 2.9542840900526894e-29 \n",
      " w= [[1.40111926]\n",
      " [0.59888074]] \n",
      " b= [0.40111926] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-2\n",
    "\n",
    "f=lambda x: loss_func(x_data, t_data)\n",
    "\n",
    "print(\"Initial error value=\", loss_func(x_data, t_data),\"\\n\", \"Initial w=\", w,\"\\n\", \"b=\",b,\"\\n\")\n",
    "\n",
    "for step in range(20001):\n",
    "    w-=learning_rate*numerical_derivative(f, w)\n",
    "    b-=learning_rate*numerical_derivative(f, b)\n",
    "\n",
    "    if step % 400==0:\n",
    "        print(\"step=\", step, \"\\n\",\"error value=\", loss_func(x_data, t_data),\"\\n\", \"w=\",w, \"\\n\",\"b=\",b,\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
