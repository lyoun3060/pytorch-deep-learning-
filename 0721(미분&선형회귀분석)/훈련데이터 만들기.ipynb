{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2d513239d10>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.FloatTensor([[1],[2],[3],[4],[5]])\n",
    "t_data = torch.FloatTensor([[3],[5],[7],[9],[11]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치(w)\n",
    "    - 가중치는 입력 데이터와 곱해져서 모델의 예측값을 계산하는 데 사용됩니다.\\\n",
    "     입력과 가중치의 곱은 데이터의 특성(feature)을 나타내며, 이러한 가중치들은 모델이 데이터를 학습하는 과정에서 조정됩니다. \\\n",
    "     가중치는 모델이 입력 데이터의 각 특성을 얼마나 중요하게 생각하는지를 결정하며, 학습을 통해 최적화되는 값입니다.\\\n",
    "     예를 들어, 선형 회귀 모델에서 가중치는 각 입력 특성과 곱해져서 출력을 만듭니다. \\\n",
    "     만약 입력 데이터가 (x1, x2)이고 가중치가 (w1, w2)라면, 모델의 출력은 y = w1x1 + w2x2와 같이 계산됩니다. \\\n",
    "     가중치 w1과 w2는 학습 과정에서 입력 데이터에 가장 적합한 값을 찾게 됩니다\\\\\n",
    "\n",
    "- 편향(b)\n",
    "    - 편향은 모델이 입력 데이터를 얼마나 잘 처리하는지를 나타내는 상수 값입니다. \\\n",
    "       가중치와 달리 입력 데이터와 직접 곱해지지 않고, 각 뉴런(또는 출력)에 더해집니다. \\\n",
    "       편향은 모델이 학습 데이터의 평균을 중심으로 예측을 수행할 수 있도록 도와줍니다.\\\n",
    "       선형 회귀에서 편향은 모델의 예측값에 상수 값을 더해줍니다. \\\n",
    "       예를 들어, y = w1x1 + w2x2 + b와 같이 편향 b가 더해집니다. 편향은 학습을 통해 데이터의 평균과 가까운 값을 가지도록 최적화됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True) tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w=torch.zeros(1,requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x_data*w+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(57., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost=torch.mean((t_data-y)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#경사하강법으로 w,b업데이트를 하기위한 최적화하는식\n",
    "optimizer=optim.SGD([w,b], lr=0.01)  #GD =Gradient Disecent 통계적 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH    0 w:0.500, b:0.140 Cost:57.000000\n",
      "EPOCH  100 w:2.082, b:0.706 Cost:0.015866\n",
      "EPOCH  200 w:2.058, b:0.790 Cost:0.008060\n",
      "EPOCH  300 w:2.041, b:0.851 Cost:0.004094\n",
      "EPOCH  400 w:2.030, b:0.893 Cost:0.002080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH  500 w:2.021, b:0.924 Cost:0.001056\n",
      "EPOCH  600 w:2.015, b:0.946 Cost:0.000537\n",
      "EPOCH  700 w:2.011, b:0.961 Cost:0.000273\n",
      "EPOCH  800 w:2.008, b:0.973 Cost:0.000138\n",
      "EPOCH  900 w:2.005, b:0.980 Cost:0.000070\n",
      "EPOCH 1000 w:2.004, b:0.986 Cost:0.000036\n",
      "EPOCH 1100 w:2.003, b:0.990 Cost:0.000018\n",
      "EPOCH 1200 w:2.002, b:0.993 Cost:0.000009\n",
      "EPOCH 1300 w:2.001, b:0.995 Cost:0.000005\n",
      "EPOCH 1400 w:2.001, b:0.996 Cost:0.000002\n",
      "EPOCH 1500 w:2.001, b:0.997 Cost:0.000001\n",
      "EPOCH 1600 w:2.001, b:0.998 Cost:0.000001\n",
      "EPOCH 1700 w:2.000, b:0.999 Cost:0.000000\n",
      "EPOCH 1800 w:2.000, b:0.999 Cost:0.000000\n",
      "EPOCH 1900 w:2.000, b:0.999 Cost:0.000000\n",
      "EPOCH 2000 w:2.000, b:1.000 Cost:0.000000\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000    #2000번 훈련할거다\n",
    "for epoch in range(nb_epochs+1): #+1하는이유 = 그래야 2000번째것이 나오니깐\n",
    "    \n",
    "    #비용 함수(cost)는 모델의 예측값(y)과 실제 라벨(t_data) 간의 차이를 측정    \n",
    "    y=x_data*w+b #y=예측값 \n",
    "    cost=torch.mean((y-t_data)**2)#cost = 비용함수\n",
    "\n",
    "    optimizer.zero_grad()  #gradient를 초기화하는것\n",
    "    cost.backward() #비용함수 계산\n",
    "    optimizer.step()  #w,b업데이트\n",
    "\n",
    "    if epoch % 100==0:\n",
    "        print('EPOCH {:4d} w:{:.3f}, b:{:.3f} Cost:{:.6f}'\\\n",
    "                .format(epoch, w.item(), b.item(), cost.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  80.,  75.],\n",
      "        [ 93.,  88.,  93.],\n",
      "        [ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]])\n",
      "tensor([[152.],\n",
      "        [185.],\n",
      "        [180.],\n",
      "        [196.],\n",
      "        [142.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data =torch.FloatTensor( [[73., 80., 75.],\n",
    "          [93., 88., 93.],\n",
    "          [89., 91., 90.],\n",
    "          [96., 98., 100.],\n",
    "          [73., 66., 70.]])\n",
    "t_data = y_train = torch.FloatTensor([[152.],[185.],[180.],[196.],[142.]])\n",
    "\n",
    "print(x_data)\n",
    "print(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "w= torch.zeros((3,1), requires_grad=True)\n",
    "b= torch.zeros(1, requires_grad=True)\n",
    "\n",
    "y= x_data.matmul(w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH    0/2000, Hypothesis:tensor([151.8199, 184.3557, 180.6764, 196.7939, 141.0506]), Cost:0.487348\n",
      "EPOCH  100/2000, Hypothesis:tensor([151.8183, 184.3568, 180.6759, 196.7934, 141.0522]), Cost:0.486287\n",
      "EPOCH  200/2000, Hypothesis:tensor([151.8167, 184.3580, 180.6755, 196.7929, 141.0538]), Cost:0.485228\n",
      "EPOCH  300/2000, Hypothesis:tensor([151.8152, 184.3591, 180.6750, 196.7924, 141.0554]), Cost:0.484174\n",
      "EPOCH  400/2000, Hypothesis:tensor([151.8136, 184.3602, 180.6746, 196.7919, 141.0570]), Cost:0.483116\n",
      "EPOCH  500/2000, Hypothesis:tensor([151.8120, 184.3613, 180.6741, 196.7913, 141.0586]), Cost:0.482055\n",
      "EPOCH  600/2000, Hypothesis:tensor([151.8104, 184.3623, 180.6736, 196.7908, 141.0602]), Cost:0.481011\n",
      "EPOCH  700/2000, Hypothesis:tensor([151.8088, 184.3634, 180.6731, 196.7903, 141.0617]), Cost:0.479965\n",
      "EPOCH  800/2000, Hypothesis:tensor([151.8072, 184.3645, 180.6727, 196.7897, 141.0633]), Cost:0.478934\n",
      "EPOCH  900/2000, Hypothesis:tensor([151.8057, 184.3655, 180.6722, 196.7892, 141.0648]), Cost:0.477919\n",
      "EPOCH 1000/2000, Hypothesis:tensor([151.8042, 184.3666, 180.6718, 196.7888, 141.0664]), Cost:0.476905\n",
      "EPOCH 1100/2000, Hypothesis:tensor([151.8026, 184.3677, 180.6714, 196.7883, 141.0680]), Cost:0.475890\n",
      "EPOCH 1200/2000, Hypothesis:tensor([151.8011, 184.3688, 180.6709, 196.7878, 141.0696]), Cost:0.474888\n",
      "EPOCH 1300/2000, Hypothesis:tensor([151.7996, 184.3699, 180.6705, 196.7873, 141.0711]), Cost:0.473877\n",
      "EPOCH 1400/2000, Hypothesis:tensor([151.7980, 184.3710, 180.6701, 196.7868, 141.0727]), Cost:0.472880\n",
      "EPOCH 1500/2000, Hypothesis:tensor([151.7965, 184.3721, 180.6696, 196.7863, 141.0743]), Cost:0.471876\n",
      "EPOCH 1600/2000, Hypothesis:tensor([151.7950, 184.3731, 180.6692, 196.7858, 141.0758]), Cost:0.470879\n",
      "EPOCH 1700/2000, Hypothesis:tensor([151.7934, 184.3742, 180.6687, 196.7853, 141.0774]), Cost:0.469878\n",
      "EPOCH 1800/2000, Hypothesis:tensor([151.7919, 184.3752, 180.6682, 196.7847, 141.0789]), Cost:0.468900\n",
      "EPOCH 1900/2000, Hypothesis:tensor([151.7904, 184.3763, 180.6678, 196.7843, 141.0804]), Cost:0.467936\n",
      "EPOCH 2000/2000, Hypothesis:tensor([151.7889, 184.3773, 180.6674, 196.7838, 141.0819]), Cost:0.466976\n"
     ]
    }
   ],
   "source": [
    "optimizer= optim.SGD([w,b], lr=1e-6)\n",
    "nb_epochs= 2000\n",
    "\n",
    "for epoch in range(nb_epochs+1): #+1하는이유 = 그래야 2000번째것이 나오니깐\n",
    "\n",
    "    y=x_data.matmul(w)+b #y=예측값 \n",
    "    cost=torch.mean((y-t_data)**2) #cost = 비용함수\n",
    "\n",
    "    optimizer.zero_grad()  #gradient를 초기화하는것\n",
    "    cost.backward() #비용함수 계산\n",
    "    optimizer.step()  #w,b업데이트\n",
    "\n",
    "    if epoch % 100==0:\n",
    "        print('EPOCH {:4d}/{}, Hypothesis:{}, Cost:{:.6f}'\\\n",
    "                .format(epoch, nb_epochs, y.squeeze().detach(), cost.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
