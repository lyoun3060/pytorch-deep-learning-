{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2d513239d10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.FloatTensor([[1],[2],[3],[4],[5]])\n",
    "t_data = torch.FloatTensor([[3],[5],[7],[9],[11]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치(w)\n",
    "    - 가중치는 입력 데이터와 곱해져서 모델의 예측값을 계산하는 데 사용됩니다.\\\n",
    "     입력과 가중치의 곱은 데이터의 특성(feature)을 나타내며, 이러한 가중치들은 모델이 데이터를 학습하는 과정에서 조정됩니다. \\\n",
    "     가중치는 모델이 입력 데이터의 각 특성을 얼마나 중요하게 생각하는지를 결정하며, 학습을 통해 최적화되는 값입니다.\\\n",
    "     예를 들어, 선형 회귀 모델에서 가중치는 각 입력 특성과 곱해져서 출력을 만듭니다. \\\n",
    "     만약 입력 데이터가 (x1, x2)이고 가중치가 (w1, w2)라면, 모델의 출력은 y = w1x1 + w2x2와 같이 계산됩니다. \\\n",
    "     가중치 w1과 w2는 학습 과정에서 입력 데이터에 가장 적합한 값을 찾게 됩니다\\\\\n",
    "\n",
    "- 편향(b)\n",
    "    - 편향은 모델이 입력 데이터를 얼마나 잘 처리하는지를 나타내는 상수 값입니다. \\\n",
    "       가중치와 달리 입력 데이터와 직접 곱해지지 않고, 각 뉴런(또는 출력)에 더해집니다. \\\n",
    "       편향은 모델이 학습 데이터의 평균을 중심으로 예측을 수행할 수 있도록 도와줍니다.\\\n",
    "       선형 회귀에서 편향은 모델의 예측값에 상수 값을 더해줍니다. \\\n",
    "       예를 들어, y = w1x1 + w2x2 + b와 같이 편향 b가 더해집니다. 편향은 학습을 통해 데이터의 평균과 가까운 값을 가지도록 최적화됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True) tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w=torch.zeros(1,requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x_data*w+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(57., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost=torch.mean((t_data-y)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#경사하강법으로 w,b업데이트를 하기위한 최적화하는식\n",
    "optimizer=optim.SGD([w,b], lr=0.01)  #GD =Gradient Disecent 통계적 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH    0 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH  100 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH  200 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH  300 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH  400 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH  500 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH  600 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH  700 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH  800 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH  900 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 1000 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 1100 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 1200 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 1300 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 1400 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 1500 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 1600 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 1700 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 1800 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 1900 w:2.000, b:1.000 Cost:0.000000\n",
      "EPOCH 2000 w:2.000, b:1.000 Cost:0.000000\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000    #2000번 훈련할거다\n",
    "for epoch in range(nb_epochs+1): #+1하는이유 = 그래야 2000번째것이 나오니깐\n",
    "    \n",
    "    #비용 함수(cost)는 모델의 예측값(y)과 실제 라벨(t_data) 간의 차이를 측정    \n",
    "    y=x_data*w+b #y=예측값 \n",
    "    cost=torch.mean((y-t_data)**2)#cost = 비용함수\n",
    "\n",
    "    optimizer.zero_grad()  #gradient를 초기화하는것\n",
    "    cost.backward() #비용함수 계산\n",
    "    optimizer.step()  #w,b업데이트\n",
    "\n",
    "    if epoch % 100==0:\n",
    "        print('EPOCH {:4d} w:{:.3f}, b:{:.3f} Cost:{:.6f}'\\\n",
    "                .format(epoch, w.item(), b.item(), cost.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  80.,  75.],\n",
      "        [ 93.,  88.,  93.],\n",
      "        [ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]])\n",
      "tensor([[152.],\n",
      "        [185.],\n",
      "        [180.],\n",
      "        [196.],\n",
      "        [142.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data =torch.FloatTensor( [[73., 80., 75.],\n",
    "          [93., 88., 93.],\n",
    "          [89., 91., 90.],\n",
    "          [96., 98., 100.],\n",
    "          [73., 66., 70.]])\n",
    "t_data = y_train = torch.FloatTensor([[152.],[185.],[180.],[196.],[142.]])\n",
    "\n",
    "print(x_data)\n",
    "print(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "w= torch.zeros((3,1), requires_grad=True)\n",
    "b= torch.zeros(1, requires_grad=True)\n",
    "\n",
    "y= x_data.matmul(w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matmul' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m nb_epochs\u001b[39m=\u001b[39m \u001b[39m2000\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nb_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m): \u001b[39m#+1하는이유 = 그래야 2000번째것이 나오니깐\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m     \u001b[39m#비용 함수(cost)는 모델의 예측값(y)과 실제 라벨(t_data) 간의 차이를 측정\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     y\u001b[39m=\u001b[39mx_data\u001b[39m*\u001b[39mmatmul(w)\u001b[39m+\u001b[39mb \u001b[39m#y=예측값 \u001b[39;00m\n\u001b[0;32m      8\u001b[0m     cost\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mmean((y\u001b[39m-\u001b[39mt_data)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m#cost = 비용함수\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m#gradient를 초기화하는것\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'matmul' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer= optim.SGD([w,b], lr=1e-6)\n",
    "nb_epochs= 2000\n",
    "\n",
    "for epoch in range(nb_epochs+1): #+1하는이유 = 그래야 2000번째것이 나오니깐\n",
    "\n",
    "    y=x_data.matmul(w)+b #y=예측값 \n",
    "    cost=torch.mean((y-t_data)**2) #cost = 비용함수\n",
    "\n",
    "    optimizer.zero_grad()  #gradient를 초기화하는것\n",
    "    cost.backward() #비용함수 계산\n",
    "    optimizer.step()  #w,b업데이트\n",
    "\n",
    "    if epoch % 100==0:\n",
    "        print('EPOCH {:4d}/{}, y:{}, w:{:.3f}, b:{:.3f} Cost:{:.6f}'\\\n",
    "                .format(epoch, nb_epochs, y.squeeze().detach(), cost.item(), w.item(), b.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
