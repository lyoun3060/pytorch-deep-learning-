{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1898a1c7050>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.FloatTensor([[1],[2],[3],[4],[5]])\n",
    "t_data = torch.FloatTensor([[3],[5],[7],[9],[11]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치(w)\n",
    "    - 가중치는 입력 데이터와 곱해져서 모델의 예측값을 계산하는 데 사용됩니다.\\\n",
    "     입력과 가중치의 곱은 데이터의 특성(feature)을 나타내며, 이러한 가중치들은 모델이 데이터를 학습하는 과정에서 조정됩니다. \\\n",
    "     가중치는 모델이 입력 데이터의 각 특성을 얼마나 중요하게 생각하는지를 결정하며, 학습을 통해 최적화되는 값입니다.\\\n",
    "     예를 들어, 선형 회귀 모델에서 가중치는 각 입력 특성과 곱해져서 출력을 만듭니다. \\\n",
    "     만약 입력 데이터가 (x1, x2)이고 가중치가 (w1, w2)라면, 모델의 출력은 y = w1x1 + w2x2와 같이 계산됩니다. \\\n",
    "     가중치 w1과 w2는 학습 과정에서 입력 데이터에 가장 적합한 값을 찾게 됩니다\\\\\n",
    "\n",
    "- 편향(b)\n",
    "    - 편향은 모델이 입력 데이터를 얼마나 잘 처리하는지를 나타내는 상수 값입니다. \\\n",
    "       가중치와 달리 입력 데이터와 직접 곱해지지 않고, 각 뉴런(또는 출력)에 더해집니다. \\\n",
    "       편향은 모델이 학습 데이터의 평균을 중심으로 예측을 수행할 수 있도록 도와줍니다.\\\n",
    "       선형 회귀에서 편향은 모델의 예측값에 상수 값을 더해줍니다. \\\n",
    "       예를 들어, y = w1x1 + w2x2 + b와 같이 편향 b가 더해집니다. 편향은 학습을 통해 데이터의 평균과 가까운 값을 가지도록 최적화됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True) tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w=torch.zeros(1,requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x_data*w+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(57., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost=torch.mean((t_data-y)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#경사하강법으로 w,b업데이트를 하기위한 최적화하는식\n",
    "optimizer=optim.SGD([w,b], lr=0.01)  #GD =Gradient Disecent 통계적 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH    0 w:0.500, b:0.140 Cost:57.000000 Hypothesis:tensor([0., 0., 0., 0., 0.])\n",
      "EPOCH  100 w:2.082, b:0.706 Cost:0.015866 Hypothesis:tensor([ 2.7865,  4.8683,  6.9501,  9.0319, 11.1136])\n",
      "EPOCH  200 w:2.058, b:0.790 Cost:0.008060 Hypothesis:tensor([ 2.8479,  4.9061,  6.9644,  9.0227, 11.0810])\n",
      "EPOCH  300 w:2.041, b:0.851 Cost:0.004094 Hypothesis:tensor([ 2.8916,  4.9331,  6.9746,  9.0162, 11.0577])\n",
      "EPOCH  400 w:2.030, b:0.893 Cost:0.002080 Hypothesis:tensor([ 2.9227,  4.9523,  6.9819,  9.0115, 11.0411])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH  500 w:2.021, b:0.924 Cost:0.001056 Hypothesis:tensor([ 2.9449,  4.9660,  6.9871,  9.0082, 11.0293])\n",
      "EPOCH  600 w:2.015, b:0.946 Cost:0.000537 Hypothesis:tensor([ 2.9607,  4.9758,  6.9908,  9.0059, 11.0209])\n",
      "EPOCH  700 w:2.011, b:0.961 Cost:0.000273 Hypothesis:tensor([ 2.9720,  4.9827,  6.9935,  9.0042, 11.0149])\n",
      "EPOCH  800 w:2.008, b:0.973 Cost:0.000138 Hypothesis:tensor([ 2.9801,  4.9877,  6.9953,  9.0030, 11.0106])\n",
      "EPOCH  900 w:2.005, b:0.980 Cost:0.000070 Hypothesis:tensor([ 2.9858,  4.9912,  6.9967,  9.0021, 11.0076])\n",
      "EPOCH 1000 w:2.004, b:0.986 Cost:0.000036 Hypothesis:tensor([ 2.9899,  4.9938,  6.9976,  9.0015, 11.0054])\n",
      "EPOCH 1100 w:2.003, b:0.990 Cost:0.000018 Hypothesis:tensor([ 2.9928,  4.9955,  6.9983,  9.0011, 11.0038])\n",
      "EPOCH 1200 w:2.002, b:0.993 Cost:0.000009 Hypothesis:tensor([ 2.9949,  4.9968,  6.9988,  9.0008, 11.0027])\n",
      "EPOCH 1300 w:2.001, b:0.995 Cost:0.000005 Hypothesis:tensor([ 2.9963,  4.9977,  6.9991,  9.0005, 11.0020])\n",
      "EPOCH 1400 w:2.001, b:0.996 Cost:0.000002 Hypothesis:tensor([ 2.9974,  4.9984,  6.9994,  9.0004, 11.0014])\n",
      "EPOCH 1500 w:2.001, b:0.997 Cost:0.000001 Hypothesis:tensor([ 2.9981,  4.9989,  6.9996,  9.0003, 11.0010])\n",
      "EPOCH 1600 w:2.001, b:0.998 Cost:0.000001 Hypothesis:tensor([ 2.9987,  4.9992,  6.9997,  9.0002, 11.0007])\n",
      "EPOCH 1700 w:2.000, b:0.999 Cost:0.000000 Hypothesis:tensor([ 2.9991,  4.9994,  6.9998,  9.0001, 11.0005])\n",
      "EPOCH 1800 w:2.000, b:0.999 Cost:0.000000 Hypothesis:tensor([ 2.9993,  4.9996,  6.9998,  9.0001, 11.0004])\n",
      "EPOCH 1900 w:2.000, b:0.999 Cost:0.000000 Hypothesis:tensor([ 2.9995,  4.9997,  6.9999,  9.0001, 11.0003])\n",
      "EPOCH 2000 w:2.000, b:1.000 Cost:0.000000 Hypothesis:tensor([ 2.9997,  4.9998,  6.9999,  9.0001, 11.0002])\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000    #2000번 훈련할거다\n",
    "for epoch in range(nb_epochs+1): #+1하는이유 = 그래야 2000번째것이 나오니깐\n",
    "    \n",
    "    #비용 함수(cost)는 모델의 예측값(y)과 실제 라벨(t_data) 간의 차이를 측정    \n",
    "    y=x_data*w+b #y=예측값 \n",
    "    cost=torch.mean((y-t_data)**2)#cost = 비용함수\n",
    "\n",
    "    optimizer.zero_grad()  #gradient를 초기화하는것\n",
    "    cost.backward() #비용함수 계산\n",
    "    optimizer.step()  #w,b업데이트\n",
    "\n",
    "    if epoch % 100==0:\n",
    "        print('EPOCH {:4d} w:{:.3f}, b:{:.3f} Cost:{:.6f} Hypothesis:{}'\\\n",
    "                .format(epoch, w.item(), b.item(), cost.item(), y.squeeze().detach()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  80.,  75.],\n",
      "        [ 93.,  88.,  93.],\n",
      "        [ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]])\n",
      "tensor([[152.],\n",
      "        [185.],\n",
      "        [180.],\n",
      "        [196.],\n",
      "        [142.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data =torch.FloatTensor( [[73., 80., 75.],\n",
    "          [93., 88., 93.],\n",
    "          [89., 91., 90.],\n",
    "          [96., 98., 100.],\n",
    "          [73., 66., 70.]])\n",
    "t_data = y_train = torch.FloatTensor([[152.],[185.],[180.],[196.],[142.]])\n",
    "\n",
    "print(x_data)\n",
    "print(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "W= torch.zeros((3,1), requires_grad=True)\n",
    "b= torch.zeros(1, requires_grad=True)\n",
    "\n",
    "y= x_data.matmul(W)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " Hypothesis: tensor([[152.3668],\n",
      "        [183.9763],\n",
      "        [180.8384],\n",
      "        [196.9546],\n",
      "        [140.5159]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9998589754104614 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6034],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009511060081422329\n",
      "Epoch: 1 \n",
      " Hypothesis: tensor([[152.3668],\n",
      "        [183.9763],\n",
      "        [180.8384],\n",
      "        [196.9546],\n",
      "        [140.5159]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9998301267623901 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6034],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009511198848485947\n",
      "Epoch: 2 \n",
      " Hypothesis: tensor([[152.3668],\n",
      "        [183.9764],\n",
      "        [180.8384],\n",
      "        [196.9545],\n",
      "        [140.5159]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9997782707214355 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6034],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009511337615549564\n",
      "Epoch: 3 \n",
      " Hypothesis: tensor([[152.3667],\n",
      "        [183.9764],\n",
      "        [180.8384],\n",
      "        [196.9546],\n",
      "        [140.5160]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9997501373291016 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6034],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009511476382613182\n",
      "Epoch: 4 \n",
      " Hypothesis: tensor([[152.3667],\n",
      "        [183.9764],\n",
      "        [180.8384],\n",
      "        [196.9545],\n",
      "        [140.5160]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9997001886367798 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6034],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.0095116151496768\n",
      "Epoch: 5 \n",
      " Hypothesis: tensor([[152.3667],\n",
      "        [183.9764],\n",
      "        [180.8384],\n",
      "        [196.9545],\n",
      "        [140.5160]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9996613264083862 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6034],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009511753916740417\n",
      "Epoch: 6 \n",
      " Hypothesis: tensor([[152.3666],\n",
      "        [183.9764],\n",
      "        [180.8384],\n",
      "        [196.9545],\n",
      "        [140.5161]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.999627411365509 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6034],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009511892683804035\n",
      "Epoch: 7 \n",
      " Hypothesis: tensor([[152.3666],\n",
      "        [183.9765],\n",
      "        [180.8384],\n",
      "        [196.9545],\n",
      "        [140.5161]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9995949864387512 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6034],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009512031450867653\n",
      "Epoch: 8 \n",
      " Hypothesis: tensor([[152.3666],\n",
      "        [183.9765],\n",
      "        [180.8383],\n",
      "        [196.9545],\n",
      "        [140.5161]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9995490312576294 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00951217021793127\n",
      "Epoch: 9 \n",
      " Hypothesis: tensor([[152.3665],\n",
      "        [183.9765],\n",
      "        [180.8383],\n",
      "        [196.9545],\n",
      "        [140.5161]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.999512791633606 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009512308984994888\n",
      "Epoch: 10 \n",
      " Hypothesis: tensor([[152.3665],\n",
      "        [183.9765],\n",
      "        [180.8383],\n",
      "        [196.9545],\n",
      "        [140.5162]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9994776844978333 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009512447752058506\n",
      "Epoch: 11 \n",
      " Hypothesis: tensor([[152.3665],\n",
      "        [183.9765],\n",
      "        [180.8383],\n",
      "        [196.9545],\n",
      "        [140.5162]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9994386434555054 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009512586519122124\n",
      "Epoch: 12 \n",
      " Hypothesis: tensor([[152.3664],\n",
      "        [183.9766],\n",
      "        [180.8383],\n",
      "        [196.9545],\n",
      "        [140.5162]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9993977546691895 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009512725286185741\n",
      "Epoch: 13 \n",
      " Hypothesis: tensor([[152.3664],\n",
      "        [183.9766],\n",
      "        [180.8383],\n",
      "        [196.9545],\n",
      "        [140.5163]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.999378502368927 \n",
      " W: tensor([[0.7265],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009512864053249359\n",
      "Epoch: 14 \n",
      " Hypothesis: tensor([[152.3664],\n",
      "        [183.9766],\n",
      "        [180.8383],\n",
      "        [196.9544],\n",
      "        [140.5163]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9993324279785156 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009513002820312977\n",
      "Epoch: 15 \n",
      " Hypothesis: tensor([[152.3663],\n",
      "        [183.9766],\n",
      "        [180.8383],\n",
      "        [196.9544],\n",
      "        [140.5163]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9992925524711609 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009513141587376595\n",
      "Epoch: 16 \n",
      " Hypothesis: tensor([[152.3663],\n",
      "        [183.9767],\n",
      "        [180.8383],\n",
      "        [196.9544],\n",
      "        [140.5163]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9992574453353882 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009513280354440212\n",
      "Epoch: 17 \n",
      " Hypothesis: tensor([[152.3663],\n",
      "        [183.9767],\n",
      "        [180.8383],\n",
      "        [196.9544],\n",
      "        [140.5164]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9992145299911499 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00951341912150383\n",
      "Epoch: 18 \n",
      " Hypothesis: tensor([[152.3663],\n",
      "        [183.9767],\n",
      "        [180.8382],\n",
      "        [196.9544],\n",
      "        [140.5164]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9991689920425415 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009513557888567448\n",
      "Epoch: 19 \n",
      " Hypothesis: tensor([[152.3662],\n",
      "        [183.9767],\n",
      "        [180.8382],\n",
      "        [196.9544],\n",
      "        [140.5164]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9991429448127747 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009513696655631065\n",
      "Epoch: 20 \n",
      " Hypothesis: tensor([[152.3662],\n",
      "        [183.9767],\n",
      "        [180.8382],\n",
      "        [196.9544],\n",
      "        [140.5165]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9991089105606079 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009513835422694683\n",
      "Epoch: 21 \n",
      " Hypothesis: tensor([[152.3662],\n",
      "        [183.9768],\n",
      "        [180.8382],\n",
      "        [196.9544],\n",
      "        [140.5165]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9990628957748413 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.0095139741897583\n",
      "Epoch: 22 \n",
      " Hypothesis: tensor([[152.3661],\n",
      "        [183.9768],\n",
      "        [180.8382],\n",
      "        [196.9544],\n",
      "        [140.5165]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9990239143371582 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009514112956821918\n",
      "Epoch: 23 \n",
      " Hypothesis: tensor([[152.3661],\n",
      "        [183.9768],\n",
      "        [180.8382],\n",
      "        [196.9544],\n",
      "        [140.5166]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9989973902702332 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009514251723885536\n",
      "Epoch: 24 \n",
      " Hypothesis: tensor([[152.3661],\n",
      "        [183.9768],\n",
      "        [180.8382],\n",
      "        [196.9543],\n",
      "        [140.5166]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9989444613456726 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009514390490949154\n",
      "Epoch: 25 \n",
      " Hypothesis: tensor([[152.3660],\n",
      "        [183.9768],\n",
      "        [180.8382],\n",
      "        [196.9543],\n",
      "        [140.5166]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9989104270935059 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009514529258012772\n",
      "Epoch: 26 \n",
      " Hypothesis: tensor([[152.3660],\n",
      "        [183.9769],\n",
      "        [180.8382],\n",
      "        [196.9543],\n",
      "        [140.5167]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9988616108894348 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00951466802507639\n",
      "Epoch: 27 \n",
      " Hypothesis: tensor([[152.3660],\n",
      "        [183.9769],\n",
      "        [180.8382],\n",
      "        [196.9543],\n",
      "        [140.5167]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9988366365432739 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009514806792140007\n",
      "Epoch: 28 \n",
      " Hypothesis: tensor([[152.3659],\n",
      "        [183.9769],\n",
      "        [180.8382],\n",
      "        [196.9543],\n",
      "        [140.5167]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9988027811050415 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009514945559203625\n",
      "Epoch: 29 \n",
      " Hypothesis: tensor([[152.3659],\n",
      "        [183.9769],\n",
      "        [180.8381],\n",
      "        [196.9543],\n",
      "        [140.5167]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9987624883651733 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009515084326267242\n",
      "Epoch: 30 \n",
      " Hypothesis: tensor([[152.3659],\n",
      "        [183.9769],\n",
      "        [180.8381],\n",
      "        [196.9543],\n",
      "        [140.5168]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9987231492996216 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6033],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00951522309333086\n",
      "Epoch: 31 \n",
      " Hypothesis: tensor([[152.3658],\n",
      "        [183.9770],\n",
      "        [180.8381],\n",
      "        [196.9543],\n",
      "        [140.5168]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9986882209777832 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009515361860394478\n",
      "Epoch: 32 \n",
      " Hypothesis: tensor([[152.3658],\n",
      "        [183.9770],\n",
      "        [180.8381],\n",
      "        [196.9543],\n",
      "        [140.5168]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9986479878425598 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009515500627458096\n",
      "Epoch: 33 \n",
      " Hypothesis: tensor([[152.3658],\n",
      "        [183.9770],\n",
      "        [180.8381],\n",
      "        [196.9543],\n",
      "        [140.5168]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9986165761947632 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009515639394521713\n",
      "Epoch: 34 \n",
      " Hypothesis: tensor([[152.3658],\n",
      "        [183.9770],\n",
      "        [180.8381],\n",
      "        [196.9543],\n",
      "        [140.5169]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9985725283622742 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009515778161585331\n",
      "Epoch: 35 \n",
      " Hypothesis: tensor([[152.3657],\n",
      "        [183.9771],\n",
      "        [180.8381],\n",
      "        [196.9543],\n",
      "        [140.5169]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9985312223434448 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009515916928648949\n",
      "Epoch: 36 \n",
      " Hypothesis: tensor([[152.3657],\n",
      "        [183.9771],\n",
      "        [180.8381],\n",
      "        [196.9542],\n",
      "        [140.5169]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9984993934631348 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009516055695712566\n",
      "Epoch: 37 \n",
      " Hypothesis: tensor([[152.3657],\n",
      "        [183.9771],\n",
      "        [180.8381],\n",
      "        [196.9542],\n",
      "        [140.5170]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.998461902141571 \n",
      " W: tensor([[0.7266],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009516194462776184\n",
      "Epoch: 38 \n",
      " Hypothesis: tensor([[152.3656],\n",
      "        [183.9771],\n",
      "        [180.8381],\n",
      "        [196.9542],\n",
      "        [140.5170]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9984308481216431 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009516333229839802\n",
      "Epoch: 39 \n",
      " Hypothesis: tensor([[152.3656],\n",
      "        [183.9771],\n",
      "        [180.8380],\n",
      "        [196.9542],\n",
      "        [140.5170]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9983909726142883 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00951647199690342\n",
      "Epoch: 40 \n",
      " Hypothesis: tensor([[152.3656],\n",
      "        [183.9772],\n",
      "        [180.8380],\n",
      "        [196.9542],\n",
      "        [140.5171]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.99835604429245 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009516610763967037\n",
      "Epoch: 41 \n",
      " Hypothesis: tensor([[152.3655],\n",
      "        [183.9772],\n",
      "        [180.8380],\n",
      "        [196.9542],\n",
      "        [140.5171]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9983048439025879 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009516749531030655\n",
      "Epoch: 42 \n",
      " Hypothesis: tensor([[152.3655],\n",
      "        [183.9772],\n",
      "        [180.8380],\n",
      "        [196.9542],\n",
      "        [140.5171]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9982712864875793 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009516888298094273\n",
      "Epoch: 43 \n",
      " Hypothesis: tensor([[152.3655],\n",
      "        [183.9772],\n",
      "        [180.8380],\n",
      "        [196.9542],\n",
      "        [140.5172]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9982415437698364 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00951702706515789\n",
      "Epoch: 44 \n",
      " Hypothesis: tensor([[152.3654],\n",
      "        [183.9772],\n",
      "        [180.8380],\n",
      "        [196.9542],\n",
      "        [140.5172]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9981943368911743 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009517165832221508\n",
      "Epoch: 45 \n",
      " Hypothesis: tensor([[152.3654],\n",
      "        [183.9773],\n",
      "        [180.8380],\n",
      "        [196.9542],\n",
      "        [140.5172]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9981614947319031 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009517304599285126\n",
      "Epoch: 46 \n",
      " Hypothesis: tensor([[152.3654],\n",
      "        [183.9773],\n",
      "        [180.8380],\n",
      "        [196.9541],\n",
      "        [140.5172]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9981195330619812 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009517443366348743\n",
      "Epoch: 47 \n",
      " Hypothesis: tensor([[152.3653],\n",
      "        [183.9773],\n",
      "        [180.8380],\n",
      "        [196.9541],\n",
      "        [140.5173]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9980867505073547 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009517582133412361\n",
      "Epoch: 48 \n",
      " Hypothesis: tensor([[152.3653],\n",
      "        [183.9773],\n",
      "        [180.8380],\n",
      "        [196.9541],\n",
      "        [140.5173]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9980447888374329 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009517720900475979\n",
      "Epoch: 49 \n",
      " Hypothesis: tensor([[152.3653],\n",
      "        [183.9774],\n",
      "        [180.8380],\n",
      "        [196.9541],\n",
      "        [140.5173]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9980096817016602 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009517859667539597\n",
      "Epoch: 50 \n",
      " Hypothesis: tensor([[152.3652],\n",
      "        [183.9774],\n",
      "        [180.8379],\n",
      "        [196.9541],\n",
      "        [140.5174]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9979642033576965 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009517998434603214\n",
      "Epoch: 51 \n",
      " Hypothesis: tensor([[152.3652],\n",
      "        [183.9774],\n",
      "        [180.8379],\n",
      "        [196.9541],\n",
      "        [140.5174]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9979303479194641 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009518137201666832\n",
      "Epoch: 52 \n",
      " Hypothesis: tensor([[152.3652],\n",
      "        [183.9774],\n",
      "        [180.8379],\n",
      "        [196.9541],\n",
      "        [140.5174]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9978843927383423 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6032],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00951827596873045\n",
      "Epoch: 53 \n",
      " Hypothesis: tensor([[152.3651],\n",
      "        [183.9774],\n",
      "        [180.8379],\n",
      "        [196.9541],\n",
      "        [140.5175]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.997855544090271 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009518414735794067\n",
      "Epoch: 54 \n",
      " Hypothesis: tensor([[152.3651],\n",
      "        [183.9774],\n",
      "        [180.8379],\n",
      "        [196.9541],\n",
      "        [140.5175]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9978248476982117 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009518553502857685\n",
      "Epoch: 55 \n",
      " Hypothesis: tensor([[152.3651],\n",
      "        [183.9775],\n",
      "        [180.8379],\n",
      "        [196.9541],\n",
      "        [140.5175]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9977859258651733 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009518692269921303\n",
      "Epoch: 56 \n",
      " Hypothesis: tensor([[152.3651],\n",
      "        [183.9775],\n",
      "        [180.8379],\n",
      "        [196.9541],\n",
      "        [140.5175]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9977468252182007 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00951883103698492\n",
      "Epoch: 57 \n",
      " Hypothesis: tensor([[152.3650],\n",
      "        [183.9775],\n",
      "        [180.8379],\n",
      "        [196.9540],\n",
      "        [140.5176]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9977104067802429 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009518969804048538\n",
      "Epoch: 58 \n",
      " Hypothesis: tensor([[152.3650],\n",
      "        [183.9775],\n",
      "        [180.8379],\n",
      "        [196.9540],\n",
      "        [140.5176]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9976662397384644 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009519108571112156\n",
      "Epoch: 59 \n",
      " Hypothesis: tensor([[152.3650],\n",
      "        [183.9776],\n",
      "        [180.8378],\n",
      "        [196.9540],\n",
      "        [140.5176]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9976351857185364 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009519247338175774\n",
      "Epoch: 60 \n",
      " Hypothesis: tensor([[152.3649],\n",
      "        [183.9776],\n",
      "        [180.8378],\n",
      "        [196.9540],\n",
      "        [140.5177]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.997595489025116 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009519386105239391\n",
      "Epoch: 61 \n",
      " Hypothesis: tensor([[152.3649],\n",
      "        [183.9776],\n",
      "        [180.8378],\n",
      "        [196.9540],\n",
      "        [140.5177]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9975677728652954 \n",
      " W: tensor([[0.7267],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009519524872303009\n",
      "Epoch: 62 \n",
      " Hypothesis: tensor([[152.3649],\n",
      "        [183.9776],\n",
      "        [180.8378],\n",
      "        [196.9540],\n",
      "        [140.5177]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9975149035453796 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009519663639366627\n",
      "Epoch: 63 \n",
      " Hypothesis: tensor([[152.3648],\n",
      "        [183.9776],\n",
      "        [180.8378],\n",
      "        [196.9540],\n",
      "        [140.5177]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9974861145019531 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009519802406430244\n",
      "Epoch: 64 \n",
      " Hypothesis: tensor([[152.3648],\n",
      "        [183.9777],\n",
      "        [180.8378],\n",
      "        [196.9540],\n",
      "        [140.5178]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9974462389945984 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009519941173493862\n",
      "Epoch: 65 \n",
      " Hypothesis: tensor([[152.3648],\n",
      "        [183.9777],\n",
      "        [180.8378],\n",
      "        [196.9540],\n",
      "        [140.5178]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9974061846733093 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952007994055748\n",
      "Epoch: 66 \n",
      " Hypothesis: tensor([[152.3647],\n",
      "        [183.9777],\n",
      "        [180.8378],\n",
      "        [196.9540],\n",
      "        [140.5178]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9973713159561157 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009520218707621098\n",
      "Epoch: 67 \n",
      " Hypothesis: tensor([[152.3647],\n",
      "        [183.9777],\n",
      "        [180.8378],\n",
      "        [196.9539],\n",
      "        [140.5179]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9973324537277222 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009520357474684715\n",
      "Epoch: 68 \n",
      " Hypothesis: tensor([[152.3647],\n",
      "        [183.9778],\n",
      "        [180.8378],\n",
      "        [196.9539],\n",
      "        [140.5179]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.997298538684845 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009520496241748333\n",
      "Epoch: 69 \n",
      " Hypothesis: tensor([[152.3646],\n",
      "        [183.9778],\n",
      "        [180.8378],\n",
      "        [196.9539],\n",
      "        [140.5179]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9972697496414185 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952063500881195\n",
      "Epoch: 70 \n",
      " Hypothesis: tensor([[152.3646],\n",
      "        [183.9778],\n",
      "        [180.8377],\n",
      "        [196.9539],\n",
      "        [140.5180]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9972147941589355 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009520773775875568\n",
      "Epoch: 71 \n",
      " Hypothesis: tensor([[152.3646],\n",
      "        [183.9778],\n",
      "        [180.8377],\n",
      "        [196.9539],\n",
      "        [140.5180]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9971914291381836 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009520912542939186\n",
      "Epoch: 72 \n",
      " Hypothesis: tensor([[152.3645],\n",
      "        [183.9778],\n",
      "        [180.8377],\n",
      "        [196.9539],\n",
      "        [140.5180]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9971491098403931 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009521051310002804\n",
      "Epoch: 73 \n",
      " Hypothesis: tensor([[152.3645],\n",
      "        [183.9779],\n",
      "        [180.8377],\n",
      "        [196.9539],\n",
      "        [140.5181]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9971061944961548 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009521190077066422\n",
      "Epoch: 74 \n",
      " Hypothesis: tensor([[152.3645],\n",
      "        [183.9779],\n",
      "        [180.8377],\n",
      "        [196.9539],\n",
      "        [140.5181]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9970744252204895 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6031],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952132884413004\n",
      "Epoch: 75 \n",
      " Hypothesis: tensor([[152.3645],\n",
      "        [183.9779],\n",
      "        [180.8377],\n",
      "        [196.9539],\n",
      "        [140.5181]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.997039794921875 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009521467611193657\n",
      "Epoch: 76 \n",
      " Hypothesis: tensor([[152.3644],\n",
      "        [183.9779],\n",
      "        [180.8377],\n",
      "        [196.9539],\n",
      "        [140.5181]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9969905614852905 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009521606378257275\n",
      "Epoch: 77 \n",
      " Hypothesis: tensor([[152.3644],\n",
      "        [183.9779],\n",
      "        [180.8377],\n",
      "        [196.9539],\n",
      "        [140.5182]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9969719648361206 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009521745145320892\n",
      "Epoch: 78 \n",
      " Hypothesis: tensor([[152.3643],\n",
      "        [183.9780],\n",
      "        [180.8377],\n",
      "        [196.9539],\n",
      "        [140.5182]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9969230890274048 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952188391238451\n",
      "Epoch: 79 \n",
      " Hypothesis: tensor([[152.3643],\n",
      "        [183.9780],\n",
      "        [180.8377],\n",
      "        [196.9538],\n",
      "        [140.5182]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9968793988227844 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009522022679448128\n",
      "Epoch: 80 \n",
      " Hypothesis: tensor([[152.3643],\n",
      "        [183.9780],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5182]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9968370199203491 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009522161446511745\n",
      "Epoch: 81 \n",
      " Hypothesis: tensor([[152.3643],\n",
      "        [183.9780],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5183]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9968105554580688 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009522300213575363\n",
      "Epoch: 82 \n",
      " Hypothesis: tensor([[152.3642],\n",
      "        [183.9781],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5183]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9967684745788574 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952243898063898\n",
      "Epoch: 83 \n",
      " Hypothesis: tensor([[152.3642],\n",
      "        [183.9781],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5183]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9967229962348938 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009522577747702599\n",
      "Epoch: 84 \n",
      " Hypothesis: tensor([[152.3642],\n",
      "        [183.9781],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5184]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9966942667961121 \n",
      " W: tensor([[0.7268],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009522716514766216\n",
      "Epoch: 85 \n",
      " Hypothesis: tensor([[152.3641],\n",
      "        [183.9781],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5184]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9966541528701782 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009522855281829834\n",
      "Epoch: 86 \n",
      " Hypothesis: tensor([[152.3641],\n",
      "        [183.9781],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5184]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9966191053390503 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009522994048893452\n",
      "Epoch: 87 \n",
      " Hypothesis: tensor([[152.3641],\n",
      "        [183.9781],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5185]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9965857267379761 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952313281595707\n",
      "Epoch: 88 \n",
      " Hypothesis: tensor([[152.3640],\n",
      "        [183.9782],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5185]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9965397715568542 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009523271583020687\n",
      "Epoch: 89 \n",
      " Hypothesis: tensor([[152.3640],\n",
      "        [183.9782],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5185]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9964995384216309 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009523410350084305\n",
      "Epoch: 90 \n",
      " Hypothesis: tensor([[152.3640],\n",
      "        [183.9782],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5186]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9964709281921387 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009523549117147923\n",
      "Epoch: 91 \n",
      " Hypothesis: tensor([[152.3640],\n",
      "        [183.9783],\n",
      "        [180.8376],\n",
      "        [196.9538],\n",
      "        [140.5186]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9964300394058228 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952368788421154\n",
      "Epoch: 92 \n",
      " Hypothesis: tensor([[152.3639],\n",
      "        [183.9783],\n",
      "        [180.8375],\n",
      "        [196.9537],\n",
      "        [140.5186]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9963942766189575 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009523826651275158\n",
      "Epoch: 93 \n",
      " Hypothesis: tensor([[152.3639],\n",
      "        [183.9783],\n",
      "        [180.8375],\n",
      "        [196.9537],\n",
      "        [140.5186]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9963655471801758 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009523965418338776\n",
      "Epoch: 94 \n",
      " Hypothesis: tensor([[152.3639],\n",
      "        [183.9783],\n",
      "        [180.8375],\n",
      "        [196.9537],\n",
      "        [140.5187]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9963305592536926 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009524104185402393\n",
      "Epoch: 95 \n",
      " Hypothesis: tensor([[152.3638],\n",
      "        [183.9783],\n",
      "        [180.8375],\n",
      "        [196.9537],\n",
      "        [140.5187]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9962776899337769 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009524242952466011\n",
      "Epoch: 96 \n",
      " Hypothesis: tensor([[152.3638],\n",
      "        [183.9783],\n",
      "        [180.8375],\n",
      "        [196.9537],\n",
      "        [140.5187]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.996251106262207 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009524381719529629\n",
      "Epoch: 97 \n",
      " Hypothesis: tensor([[152.3638],\n",
      "        [183.9784],\n",
      "        [180.8375],\n",
      "        [196.9537],\n",
      "        [140.5188]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9962142705917358 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6030],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009524520486593246\n",
      "Epoch: 98 \n",
      " Hypothesis: tensor([[152.3637],\n",
      "        [183.9784],\n",
      "        [180.8375],\n",
      "        [196.9537],\n",
      "        [140.5188]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9961832165718079 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009524659253656864\n",
      "Epoch: 99 \n",
      " Hypothesis: tensor([[152.3637],\n",
      "        [183.9784],\n",
      "        [180.8375],\n",
      "        [196.9537],\n",
      "        [140.5188]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9961283802986145 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009524798020720482\n",
      "Epoch: 100 \n",
      " Hypothesis: tensor([[152.3637],\n",
      "        [183.9784],\n",
      "        [180.8375],\n",
      "        [196.9537],\n",
      "        [140.5188]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9961017370223999 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.0095249367877841\n",
      "Epoch: 101 \n",
      " Hypothesis: tensor([[152.3636],\n",
      "        [183.9785],\n",
      "        [180.8374],\n",
      "        [196.9537],\n",
      "        [140.5189]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9960594177246094 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009525075554847717\n",
      "Epoch: 102 \n",
      " Hypothesis: tensor([[152.3636],\n",
      "        [183.9785],\n",
      "        [180.8374],\n",
      "        [196.9536],\n",
      "        [140.5189]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9960212707519531 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009525214321911335\n",
      "Epoch: 103 \n",
      " Hypothesis: tensor([[152.3636],\n",
      "        [183.9785],\n",
      "        [180.8374],\n",
      "        [196.9536],\n",
      "        [140.5189]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9959812164306641 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009525353088974953\n",
      "Epoch: 104 \n",
      " Hypothesis: tensor([[152.3635],\n",
      "        [183.9785],\n",
      "        [180.8374],\n",
      "        [196.9536],\n",
      "        [140.5190]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9959545135498047 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952549185603857\n",
      "Epoch: 105 \n",
      " Hypothesis: tensor([[152.3635],\n",
      "        [183.9785],\n",
      "        [180.8374],\n",
      "        [196.9536],\n",
      "        [140.5190]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9959083795547485 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009525630623102188\n",
      "Epoch: 106 \n",
      " Hypothesis: tensor([[152.3635],\n",
      "        [183.9786],\n",
      "        [180.8374],\n",
      "        [196.9536],\n",
      "        [140.5190]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9958736300468445 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009525769390165806\n",
      "Epoch: 107 \n",
      " Hypothesis: tensor([[152.3634],\n",
      "        [183.9786],\n",
      "        [180.8374],\n",
      "        [196.9536],\n",
      "        [140.5190]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9958375692367554 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009525908157229424\n",
      "Epoch: 108 \n",
      " Hypothesis: tensor([[152.3634],\n",
      "        [183.9786],\n",
      "        [180.8374],\n",
      "        [196.9536],\n",
      "        [140.5191]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9957962036132812 \n",
      " W: tensor([[0.7269],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009526046924293041\n",
      "Epoch: 109 \n",
      " Hypothesis: tensor([[152.3634],\n",
      "        [183.9786],\n",
      "        [180.8374],\n",
      "        [196.9536],\n",
      "        [140.5191]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9957650899887085 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009526185691356659\n",
      "Epoch: 110 \n",
      " Hypothesis: tensor([[152.3634],\n",
      "        [183.9787],\n",
      "        [180.8374],\n",
      "        [196.9536],\n",
      "        [140.5191]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9957214593887329 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009526324458420277\n",
      "Epoch: 111 \n",
      " Hypothesis: tensor([[152.3633],\n",
      "        [183.9787],\n",
      "        [180.8374],\n",
      "        [196.9536],\n",
      "        [140.5192]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9956966638565063 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009526463225483894\n",
      "Epoch: 112 \n",
      " Hypothesis: tensor([[152.3633],\n",
      "        [183.9787],\n",
      "        [180.8373],\n",
      "        [196.9536],\n",
      "        [140.5192]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9956566095352173 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009526601992547512\n",
      "Epoch: 113 \n",
      " Hypothesis: tensor([[152.3633],\n",
      "        [183.9787],\n",
      "        [180.8373],\n",
      "        [196.9536],\n",
      "        [140.5192]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9956008791923523 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952674075961113\n",
      "Epoch: 114 \n",
      " Hypothesis: tensor([[152.3632],\n",
      "        [183.9787],\n",
      "        [180.8373],\n",
      "        [196.9535],\n",
      "        [140.5193]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9955679774284363 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009526879526674747\n",
      "Epoch: 115 \n",
      " Hypothesis: tensor([[152.3632],\n",
      "        [183.9788],\n",
      "        [180.8373],\n",
      "        [196.9535],\n",
      "        [140.5193]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9955342411994934 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009527018293738365\n",
      "Epoch: 116 \n",
      " Hypothesis: tensor([[152.3632],\n",
      "        [183.9788],\n",
      "        [180.8373],\n",
      "        [196.9535],\n",
      "        [140.5193]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9955013990402222 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009527157060801983\n",
      "Epoch: 117 \n",
      " Hypothesis: tensor([[152.3631],\n",
      "        [183.9788],\n",
      "        [180.8373],\n",
      "        [196.9535],\n",
      "        [140.5194]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.995450496673584 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.0095272958278656\n",
      "Epoch: 118 \n",
      " Hypothesis: tensor([[152.3631],\n",
      "        [183.9788],\n",
      "        [180.8373],\n",
      "        [196.9535],\n",
      "        [140.5194]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9954307675361633 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009527434594929218\n",
      "Epoch: 119 \n",
      " Hypothesis: tensor([[152.3631],\n",
      "        [183.9789],\n",
      "        [180.8373],\n",
      "        [196.9535],\n",
      "        [140.5194]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9953790903091431 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6029],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009527573361992836\n",
      "Epoch: 120 \n",
      " Hypothesis: tensor([[152.3631],\n",
      "        [183.9789],\n",
      "        [180.8373],\n",
      "        [196.9535],\n",
      "        [140.5195]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9953373670578003 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009527712129056454\n",
      "Epoch: 121 \n",
      " Hypothesis: tensor([[152.3630],\n",
      "        [183.9789],\n",
      "        [180.8373],\n",
      "        [196.9535],\n",
      "        [140.5195]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9953103065490723 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009527850896120071\n",
      "Epoch: 122 \n",
      " Hypothesis: tensor([[152.3630],\n",
      "        [183.9789],\n",
      "        [180.8372],\n",
      "        [196.9535],\n",
      "        [140.5195]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9952702522277832 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009527989663183689\n",
      "Epoch: 123 \n",
      " Hypothesis: tensor([[152.3629],\n",
      "        [183.9789],\n",
      "        [180.8372],\n",
      "        [196.9535],\n",
      "        [140.5195]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9952298998832703 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009528128430247307\n",
      "Epoch: 124 \n",
      " Hypothesis: tensor([[152.3629],\n",
      "        [183.9790],\n",
      "        [180.8372],\n",
      "        [196.9535],\n",
      "        [140.5196]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9951949119567871 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009528267197310925\n",
      "Epoch: 125 \n",
      " Hypothesis: tensor([[152.3629],\n",
      "        [183.9790],\n",
      "        [180.8372],\n",
      "        [196.9535],\n",
      "        [140.5196]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9951621890068054 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009528405964374542\n",
      "Epoch: 126 \n",
      " Hypothesis: tensor([[152.3629],\n",
      "        [183.9790],\n",
      "        [180.8372],\n",
      "        [196.9534],\n",
      "        [140.5196]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9951213598251343 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952854473143816\n",
      "Epoch: 127 \n",
      " Hypothesis: tensor([[152.3628],\n",
      "        [183.9790],\n",
      "        [180.8372],\n",
      "        [196.9534],\n",
      "        [140.5197]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9950886964797974 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009528683498501778\n",
      "Epoch: 128 \n",
      " Hypothesis: tensor([[152.3628],\n",
      "        [183.9790],\n",
      "        [180.8372],\n",
      "        [196.9534],\n",
      "        [140.5197]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9950467348098755 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009528822265565395\n",
      "Epoch: 129 \n",
      " Hypothesis: tensor([[152.3628],\n",
      "        [183.9791],\n",
      "        [180.8372],\n",
      "        [196.9534],\n",
      "        [140.5197]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9950088262557983 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009528961032629013\n",
      "Epoch: 130 \n",
      " Hypothesis: tensor([[152.3627],\n",
      "        [183.9791],\n",
      "        [180.8372],\n",
      "        [196.9534],\n",
      "        [140.5197]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9949779510498047 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952909979969263\n",
      "Epoch: 131 \n",
      " Hypothesis: tensor([[152.3627],\n",
      "        [183.9791],\n",
      "        [180.8372],\n",
      "        [196.9534],\n",
      "        [140.5198]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9949272871017456 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009529238566756248\n",
      "Epoch: 132 \n",
      " Hypothesis: tensor([[152.3627],\n",
      "        [183.9791],\n",
      "        [180.8372],\n",
      "        [196.9534],\n",
      "        [140.5198]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9948965311050415 \n",
      " W: tensor([[0.7270],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009529377333819866\n",
      "Epoch: 133 \n",
      " Hypothesis: tensor([[152.3627],\n",
      "        [183.9792],\n",
      "        [180.8372],\n",
      "        [196.9534],\n",
      "        [140.5199]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9948626756668091 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009529516100883484\n",
      "Epoch: 134 \n",
      " Hypothesis: tensor([[152.3626],\n",
      "        [183.9792],\n",
      "        [180.8371],\n",
      "        [196.9534],\n",
      "        [140.5199]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.994810938835144 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009529654867947102\n",
      "Epoch: 135 \n",
      " Hypothesis: tensor([[152.3626],\n",
      "        [183.9792],\n",
      "        [180.8371],\n",
      "        [196.9534],\n",
      "        [140.5199]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9947781562805176 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00952979363501072\n",
      "Epoch: 136 \n",
      " Hypothesis: tensor([[152.3625],\n",
      "        [183.9792],\n",
      "        [180.8371],\n",
      "        [196.9534],\n",
      "        [140.5199]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9947489500045776 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009529932402074337\n",
      "Epoch: 137 \n",
      " Hypothesis: tensor([[152.3625],\n",
      "        [183.9792],\n",
      "        [180.8371],\n",
      "        [196.9534],\n",
      "        [140.5200]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9947097897529602 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009530071169137955\n",
      "Epoch: 138 \n",
      " Hypothesis: tensor([[152.3625],\n",
      "        [183.9793],\n",
      "        [180.8371],\n",
      "        [196.9534],\n",
      "        [140.5200]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.994674801826477 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009530209936201572\n",
      "Epoch: 139 \n",
      " Hypothesis: tensor([[152.3625],\n",
      "        [183.9793],\n",
      "        [180.8371],\n",
      "        [196.9533],\n",
      "        [140.5200]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9946383237838745 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00953034870326519\n",
      "Epoch: 140 \n",
      " Hypothesis: tensor([[152.3624],\n",
      "        [183.9793],\n",
      "        [180.8371],\n",
      "        [196.9533],\n",
      "        [140.5201]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9945950508117676 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009530487470328808\n",
      "Epoch: 141 \n",
      " Hypothesis: tensor([[152.3624],\n",
      "        [183.9793],\n",
      "        [180.8371],\n",
      "        [196.9533],\n",
      "        [140.5201]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9945605397224426 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009530626237392426\n",
      "Epoch: 142 \n",
      " Hypothesis: tensor([[152.3624],\n",
      "        [183.9794],\n",
      "        [180.8371],\n",
      "        [196.9533],\n",
      "        [140.5201]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9945318102836609 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6028],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009530765004456043\n",
      "Epoch: 143 \n",
      " Hypothesis: tensor([[152.3623],\n",
      "        [183.9794],\n",
      "        [180.8371],\n",
      "        [196.9533],\n",
      "        [140.5201]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9944888353347778 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009530903771519661\n",
      "Epoch: 144 \n",
      " Hypothesis: tensor([[152.3623],\n",
      "        [183.9794],\n",
      "        [180.8371],\n",
      "        [196.9533],\n",
      "        [140.5202]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9944401979446411 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009531042538583279\n",
      "Epoch: 145 \n",
      " Hypothesis: tensor([[152.3623],\n",
      "        [183.9794],\n",
      "        [180.8371],\n",
      "        [196.9533],\n",
      "        [140.5202]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9944113492965698 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009531181305646896\n",
      "Epoch: 146 \n",
      " Hypothesis: tensor([[152.3622],\n",
      "        [183.9794],\n",
      "        [180.8370],\n",
      "        [196.9533],\n",
      "        [140.5202]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9943712949752808 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009531320072710514\n",
      "Epoch: 147 \n",
      " Hypothesis: tensor([[152.3622],\n",
      "        [183.9795],\n",
      "        [180.8370],\n",
      "        [196.9533],\n",
      "        [140.5203]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9943317174911499 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009531458839774132\n",
      "Epoch: 148 \n",
      " Hypothesis: tensor([[152.3622],\n",
      "        [183.9795],\n",
      "        [180.8370],\n",
      "        [196.9533],\n",
      "        [140.5203]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9942992925643921 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00953159760683775\n",
      "Epoch: 149 \n",
      " Hypothesis: tensor([[152.3622],\n",
      "        [183.9795],\n",
      "        [180.8370],\n",
      "        [196.9533],\n",
      "        [140.5203]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9942644834518433 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009531736373901367\n",
      "Epoch: 150 \n",
      " Hypothesis: tensor([[152.3621],\n",
      "        [183.9795],\n",
      "        [180.8370],\n",
      "        [196.9533],\n",
      "        [140.5203]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9942374229431152 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009531875140964985\n",
      "Epoch: 151 \n",
      " Hypothesis: tensor([[152.3621],\n",
      "        [183.9796],\n",
      "        [180.8370],\n",
      "        [196.9533],\n",
      "        [140.5204]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9941973686218262 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009532013908028603\n",
      "Epoch: 152 \n",
      " Hypothesis: tensor([[152.3621],\n",
      "        [183.9796],\n",
      "        [180.8370],\n",
      "        [196.9532],\n",
      "        [140.5204]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9941566586494446 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00953215267509222\n",
      "Epoch: 153 \n",
      " Hypothesis: tensor([[152.3620],\n",
      "        [183.9796],\n",
      "        [180.8370],\n",
      "        [196.9532],\n",
      "        [140.5204]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9941169619560242 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009532291442155838\n",
      "Epoch: 154 \n",
      " Hypothesis: tensor([[152.3620],\n",
      "        [183.9796],\n",
      "        [180.8370],\n",
      "        [196.9532],\n",
      "        [140.5205]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9940814971923828 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009532430209219456\n",
      "Epoch: 155 \n",
      " Hypothesis: tensor([[152.3620],\n",
      "        [183.9796],\n",
      "        [180.8370],\n",
      "        [196.9532],\n",
      "        [140.5205]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9940349459648132 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009532568976283073\n",
      "Epoch: 156 \n",
      " Hypothesis: tensor([[152.3619],\n",
      "        [183.9797],\n",
      "        [180.8370],\n",
      "        [196.9532],\n",
      "        [140.5205]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.994013786315918 \n",
      " W: tensor([[0.7271],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009532707743346691\n",
      "Epoch: 157 \n",
      " Hypothesis: tensor([[152.3619],\n",
      "        [183.9797],\n",
      "        [180.8370],\n",
      "        [196.9532],\n",
      "        [140.5206]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9939729571342468 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009532846510410309\n",
      "Epoch: 158 \n",
      " Hypothesis: tensor([[152.3619],\n",
      "        [183.9797],\n",
      "        [180.8369],\n",
      "        [196.9532],\n",
      "        [140.5206]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9939391016960144 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009532985277473927\n",
      "Epoch: 159 \n",
      " Hypothesis: tensor([[152.3619],\n",
      "        [183.9797],\n",
      "        [180.8369],\n",
      "        [196.9532],\n",
      "        [140.5206]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9938955307006836 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009533124044537544\n",
      "Epoch: 160 \n",
      " Hypothesis: tensor([[152.3618],\n",
      "        [183.9798],\n",
      "        [180.8369],\n",
      "        [196.9532],\n",
      "        [140.5206]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9938610196113586 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009533262811601162\n",
      "Epoch: 161 \n",
      " Hypothesis: tensor([[152.3618],\n",
      "        [183.9798],\n",
      "        [180.8369],\n",
      "        [196.9532],\n",
      "        [140.5207]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9938329458236694 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00953340157866478\n",
      "Epoch: 162 \n",
      " Hypothesis: tensor([[152.3618],\n",
      "        [183.9798],\n",
      "        [180.8369],\n",
      "        [196.9532],\n",
      "        [140.5207]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9937871098518372 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009533540345728397\n",
      "Epoch: 163 \n",
      " Hypothesis: tensor([[152.3617],\n",
      "        [183.9798],\n",
      "        [180.8369],\n",
      "        [196.9532],\n",
      "        [140.5207]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9937503933906555 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009533679112792015\n",
      "Epoch: 164 \n",
      " Hypothesis: tensor([[152.3617],\n",
      "        [183.9798],\n",
      "        [180.8369],\n",
      "        [196.9531],\n",
      "        [140.5208]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.993704617023468 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6027],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009533817879855633\n",
      "Epoch: 165 \n",
      " Hypothesis: tensor([[152.3617],\n",
      "        [183.9799],\n",
      "        [180.8369],\n",
      "        [196.9531],\n",
      "        [140.5208]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.993678092956543 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00953395664691925\n",
      "Epoch: 166 \n",
      " Hypothesis: tensor([[152.3616],\n",
      "        [183.9799],\n",
      "        [180.8369],\n",
      "        [196.9531],\n",
      "        [140.5208]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9936443567276001 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009534095413982868\n",
      "Epoch: 167 \n",
      " Hypothesis: tensor([[152.3616],\n",
      "        [183.9799],\n",
      "        [180.8369],\n",
      "        [196.9531],\n",
      "        [140.5209]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.993597686290741 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009534234181046486\n",
      "Epoch: 168 \n",
      " Hypothesis: tensor([[152.3616],\n",
      "        [183.9799],\n",
      "        [180.8369],\n",
      "        [196.9531],\n",
      "        [140.5209]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9935577511787415 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009534372948110104\n",
      "Epoch: 169 \n",
      " Hypothesis: tensor([[152.3615],\n",
      "        [183.9799],\n",
      "        [180.8368],\n",
      "        [196.9531],\n",
      "        [140.5209]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9935216903686523 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009534511715173721\n",
      "Epoch: 170 \n",
      " Hypothesis: tensor([[152.3615],\n",
      "        [183.9800],\n",
      "        [180.8368],\n",
      "        [196.9531],\n",
      "        [140.5210]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9934871792793274 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009534650482237339\n",
      "Epoch: 171 \n",
      " Hypothesis: tensor([[152.3615],\n",
      "        [183.9800],\n",
      "        [180.8368],\n",
      "        [196.9531],\n",
      "        [140.5210]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9934436082839966 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009534789249300957\n",
      "Epoch: 172 \n",
      " Hypothesis: tensor([[152.3615],\n",
      "        [183.9800],\n",
      "        [180.8368],\n",
      "        [196.9531],\n",
      "        [140.5210]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9934097528457642 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009534928016364574\n",
      "Epoch: 173 \n",
      " Hypothesis: tensor([[152.3614],\n",
      "        [183.9800],\n",
      "        [180.8368],\n",
      "        [196.9531],\n",
      "        [140.5210]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9933748245239258 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009535066783428192\n",
      "Epoch: 174 \n",
      " Hypothesis: tensor([[152.3614],\n",
      "        [183.9801],\n",
      "        [180.8368],\n",
      "        [196.9531],\n",
      "        [140.5211]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9933409690856934 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00953520555049181\n",
      "Epoch: 175 \n",
      " Hypothesis: tensor([[152.3614],\n",
      "        [183.9801],\n",
      "        [180.8368],\n",
      "        [196.9531],\n",
      "        [140.5211]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9932980537414551 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009535344317555428\n",
      "Epoch: 176 \n",
      " Hypothesis: tensor([[152.3613],\n",
      "        [183.9801],\n",
      "        [180.8368],\n",
      "        [196.9530],\n",
      "        [140.5211]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9932635426521301 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009535483084619045\n",
      "Epoch: 177 \n",
      " Hypothesis: tensor([[152.3613],\n",
      "        [183.9801],\n",
      "        [180.8368],\n",
      "        [196.9530],\n",
      "        [140.5212]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9932320713996887 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009535621851682663\n",
      "Epoch: 178 \n",
      " Hypothesis: tensor([[152.3613],\n",
      "        [183.9801],\n",
      "        [180.8368],\n",
      "        [196.9530],\n",
      "        [140.5212]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9931970834732056 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00953576061874628\n",
      "Epoch: 179 \n",
      " Hypothesis: tensor([[152.3613],\n",
      "        [183.9802],\n",
      "        [180.8368],\n",
      "        [196.9530],\n",
      "        [140.5212]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9931513071060181 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009535899385809898\n",
      "Epoch: 180 \n",
      " Hypothesis: tensor([[152.3612],\n",
      "        [183.9802],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5213]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9931116104125977 \n",
      " W: tensor([[0.7272],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009536038152873516\n",
      "Epoch: 181 \n",
      " Hypothesis: tensor([[152.3612],\n",
      "        [183.9802],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5213]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9930828809738159 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009536176919937134\n",
      "Epoch: 182 \n",
      " Hypothesis: tensor([[152.3611],\n",
      "        [183.9802],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5213]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9930458068847656 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009536315687000751\n",
      "Epoch: 183 \n",
      " Hypothesis: tensor([[152.3611],\n",
      "        [183.9803],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5213]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9929974675178528 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00953645445406437\n",
      "Epoch: 184 \n",
      " Hypothesis: tensor([[152.3611],\n",
      "        [183.9803],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5214]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.992962658405304 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009536593221127987\n",
      "Epoch: 185 \n",
      " Hypothesis: tensor([[152.3611],\n",
      "        [183.9803],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5214]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9929339289665222 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009536731988191605\n",
      "Epoch: 186 \n",
      " Hypothesis: tensor([[152.3610],\n",
      "        [183.9803],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5214]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9928880929946899 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009536870755255222\n",
      "Epoch: 187 \n",
      " Hypothesis: tensor([[152.3610],\n",
      "        [183.9803],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5215]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9928535223007202 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6026],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00953700952231884\n",
      "Epoch: 188 \n",
      " Hypothesis: tensor([[152.3610],\n",
      "        [183.9804],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5215]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9928199052810669 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009537148289382458\n",
      "Epoch: 189 \n",
      " Hypothesis: tensor([[152.3609],\n",
      "        [183.9804],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5215]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9927797317504883 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009537287056446075\n",
      "Epoch: 190 \n",
      " Hypothesis: tensor([[152.3609],\n",
      "        [183.9804],\n",
      "        [180.8367],\n",
      "        [196.9530],\n",
      "        [140.5216]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9927449226379395 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009537425823509693\n",
      "Epoch: 191 \n",
      " Hypothesis: tensor([[152.3609],\n",
      "        [183.9804],\n",
      "        [180.8367],\n",
      "        [196.9529],\n",
      "        [140.5216]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9926995038986206 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.00953756459057331\n",
      "Epoch: 192 \n",
      " Hypothesis: tensor([[152.3609],\n",
      "        [183.9805],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5216]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9926656484603882 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009537703357636929\n",
      "Epoch: 193 \n",
      " Hypothesis: tensor([[152.3608],\n",
      "        [183.9805],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5217]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9926307797431946 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009537842124700546\n",
      "Epoch: 194 \n",
      " Hypothesis: tensor([[152.3608],\n",
      "        [183.9805],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5217]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9925853610038757 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6807]], requires_grad=True) \n",
      " b: 0.009537980891764164\n",
      "Epoch: 195 \n",
      " Hypothesis: tensor([[152.3608],\n",
      "        [183.9805],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5217]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9925504922866821 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009538119658827782\n",
      "Epoch: 196 \n",
      " Hypothesis: tensor([[152.3607],\n",
      "        [183.9805],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5217]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9925166964530945 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.0095382584258914\n",
      "Epoch: 197 \n",
      " Hypothesis: tensor([[152.3607],\n",
      "        [183.9806],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5218]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9924759864807129 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009538397192955017\n",
      "Epoch: 198 \n",
      " Hypothesis: tensor([[152.3607],\n",
      "        [183.9806],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5218]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.99244225025177 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009538535960018635\n",
      "Epoch: 199 \n",
      " Hypothesis: tensor([[152.3606],\n",
      "        [183.9806],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5218]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9924076795578003 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009538674727082253\n",
      "Epoch: 200 \n",
      " Hypothesis: tensor([[152.3606],\n",
      "        [183.9806],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5219]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9923677444458008 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00953881349414587\n",
      "Epoch: 201 \n",
      " Hypothesis: tensor([[152.3606],\n",
      "        [183.9807],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5219]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9923367500305176 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009538952261209488\n",
      "Epoch: 202 \n",
      " Hypothesis: tensor([[152.3605],\n",
      "        [183.9807],\n",
      "        [180.8366],\n",
      "        [196.9529],\n",
      "        [140.5219]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9923022985458374 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009539091028273106\n",
      "Epoch: 203 \n",
      " Hypothesis: tensor([[152.3605],\n",
      "        [183.9807],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5219]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9922626614570618 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009539229795336723\n",
      "Epoch: 204 \n",
      " Hypothesis: tensor([[152.3605],\n",
      "        [183.9807],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5220]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9922226667404175 \n",
      " W: tensor([[0.7273],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009539368562400341\n",
      "Epoch: 205 \n",
      " Hypothesis: tensor([[152.3605],\n",
      "        [183.9807],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5220]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9921849966049194 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009539507329463959\n",
      "Epoch: 206 \n",
      " Hypothesis: tensor([[152.3604],\n",
      "        [183.9808],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5220]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9921450614929199 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009539646096527576\n",
      "Epoch: 207 \n",
      " Hypothesis: tensor([[152.3604],\n",
      "        [183.9808],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5221]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9921137690544128 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009539784863591194\n",
      "Epoch: 208 \n",
      " Hypothesis: tensor([[152.3604],\n",
      "        [183.9808],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5221]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9920738339424133 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009539923630654812\n",
      "Epoch: 209 \n",
      " Hypothesis: tensor([[152.3603],\n",
      "        [183.9808],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5221]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9920451045036316 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954006239771843\n",
      "Epoch: 210 \n",
      " Hypothesis: tensor([[152.3603],\n",
      "        [183.9809],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5222]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9920048713684082 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6025],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009540201164782047\n",
      "Epoch: 211 \n",
      " Hypothesis: tensor([[152.3603],\n",
      "        [183.9809],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5222]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9919597506523132 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009540339931845665\n",
      "Epoch: 212 \n",
      " Hypothesis: tensor([[152.3602],\n",
      "        [183.9809],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5222]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9919248819351196 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009540478698909283\n",
      "Epoch: 213 \n",
      " Hypothesis: tensor([[152.3602],\n",
      "        [183.9809],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5222]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9918904304504395 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.0095406174659729\n",
      "Epoch: 214 \n",
      " Hypothesis: tensor([[152.3602],\n",
      "        [183.9809],\n",
      "        [180.8365],\n",
      "        [196.9528],\n",
      "        [140.5223]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9918566942214966 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009540756233036518\n",
      "Epoch: 215 \n",
      " Hypothesis: tensor([[152.3602],\n",
      "        [183.9810],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5223]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9918107986450195 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009540895000100136\n",
      "Epoch: 216 \n",
      " Hypothesis: tensor([[152.3601],\n",
      "        [183.9810],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5223]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9917820692062378 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009541033767163754\n",
      "Epoch: 217 \n",
      " Hypothesis: tensor([[152.3601],\n",
      "        [183.9810],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5224]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9917422533035278 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009541172534227371\n",
      "Epoch: 218 \n",
      " Hypothesis: tensor([[152.3601],\n",
      "        [183.9810],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5224]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9917078018188477 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009541311301290989\n",
      "Epoch: 219 \n",
      " Hypothesis: tensor([[152.3600],\n",
      "        [183.9810],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5224]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9916731715202332 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009541450068354607\n",
      "Epoch: 220 \n",
      " Hypothesis: tensor([[152.3600],\n",
      "        [183.9811],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5225]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9916275143623352 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009541588835418224\n",
      "Epoch: 221 \n",
      " Hypothesis: tensor([[152.3600],\n",
      "        [183.9811],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5225]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9916046857833862 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009541727602481842\n",
      "Epoch: 222 \n",
      " Hypothesis: tensor([[152.3599],\n",
      "        [183.9811],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5225]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.991553008556366 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954186636954546\n",
      "Epoch: 223 \n",
      " Hypothesis: tensor([[152.3599],\n",
      "        [183.9812],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5226]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9915130734443665 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009542005136609077\n",
      "Epoch: 224 \n",
      " Hypothesis: tensor([[152.3599],\n",
      "        [183.9812],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5226]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9914843440055847 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009542143903672695\n",
      "Epoch: 225 \n",
      " Hypothesis: tensor([[152.3598],\n",
      "        [183.9812],\n",
      "        [180.8364],\n",
      "        [196.9527],\n",
      "        [140.5226]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9914447665214539 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009542282670736313\n",
      "Epoch: 226 \n",
      " Hypothesis: tensor([[152.3598],\n",
      "        [183.9812],\n",
      "        [180.8363],\n",
      "        [196.9527],\n",
      "        [140.5226]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9914142489433289 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954242143779993\n",
      "Epoch: 227 \n",
      " Hypothesis: tensor([[152.3598],\n",
      "        [183.9812],\n",
      "        [180.8363],\n",
      "        [196.9527],\n",
      "        [140.5227]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9913795590400696 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009542560204863548\n",
      "Epoch: 228 \n",
      " Hypothesis: tensor([[152.3598],\n",
      "        [183.9813],\n",
      "        [180.8363],\n",
      "        [196.9527],\n",
      "        [140.5227]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9913395047187805 \n",
      " W: tensor([[0.7274],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009542698971927166\n",
      "Epoch: 229 \n",
      " Hypothesis: tensor([[152.3597],\n",
      "        [183.9813],\n",
      "        [180.8363],\n",
      "        [196.9526],\n",
      "        [140.5227]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9912959933280945 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009542837738990784\n",
      "Epoch: 230 \n",
      " Hypothesis: tensor([[152.3597],\n",
      "        [183.9813],\n",
      "        [180.8363],\n",
      "        [196.9526],\n",
      "        [140.5228]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9912561178207397 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009542976506054401\n",
      "Epoch: 231 \n",
      " Hypothesis: tensor([[152.3597],\n",
      "        [183.9813],\n",
      "        [180.8363],\n",
      "        [196.9526],\n",
      "        [140.5228]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9912216067314148 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009543115273118019\n",
      "Epoch: 232 \n",
      " Hypothesis: tensor([[152.3596],\n",
      "        [183.9813],\n",
      "        [180.8363],\n",
      "        [196.9526],\n",
      "        [140.5228]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9911878705024719 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6024],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009543254040181637\n",
      "Epoch: 233 \n",
      " Hypothesis: tensor([[152.3596],\n",
      "        [183.9814],\n",
      "        [180.8363],\n",
      "        [196.9526],\n",
      "        [140.5228]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9911569356918335 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009543392807245255\n",
      "Epoch: 234 \n",
      " Hypothesis: tensor([[152.3596],\n",
      "        [183.9814],\n",
      "        [180.8363],\n",
      "        [196.9526],\n",
      "        [140.5229]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9911134839057922 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009543531574308872\n",
      "Epoch: 235 \n",
      " Hypothesis: tensor([[152.3595],\n",
      "        [183.9814],\n",
      "        [180.8363],\n",
      "        [196.9526],\n",
      "        [140.5229]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9910829663276672 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954367034137249\n",
      "Epoch: 236 \n",
      " Hypothesis: tensor([[152.3595],\n",
      "        [183.9814],\n",
      "        [180.8363],\n",
      "        [196.9526],\n",
      "        [140.5229]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9910371899604797 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009543809108436108\n",
      "Epoch: 237 \n",
      " Hypothesis: tensor([[152.3595],\n",
      "        [183.9815],\n",
      "        [180.8363],\n",
      "        [196.9526],\n",
      "        [140.5230]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9910081624984741 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009543947875499725\n",
      "Epoch: 238 \n",
      " Hypothesis: tensor([[152.3595],\n",
      "        [183.9815],\n",
      "        [180.8362],\n",
      "        [196.9526],\n",
      "        [140.5230]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9909628033638 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009544086642563343\n",
      "Epoch: 239 \n",
      " Hypothesis: tensor([[152.3594],\n",
      "        [183.9815],\n",
      "        [180.8362],\n",
      "        [196.9526],\n",
      "        [140.5230]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9909341931343079 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954422540962696\n",
      "Epoch: 240 \n",
      " Hypothesis: tensor([[152.3594],\n",
      "        [183.9815],\n",
      "        [180.8362],\n",
      "        [196.9526],\n",
      "        [140.5231]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9908903241157532 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009544364176690578\n",
      "Epoch: 241 \n",
      " Hypothesis: tensor([[152.3594],\n",
      "        [183.9815],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5231]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9908598065376282 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009544502943754196\n",
      "Epoch: 242 \n",
      " Hypothesis: tensor([[152.3593],\n",
      "        [183.9816],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5231]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9908231496810913 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009544641710817814\n",
      "Epoch: 243 \n",
      " Hypothesis: tensor([[152.3593],\n",
      "        [183.9816],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5232]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9907773733139038 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009544780477881432\n",
      "Epoch: 244 \n",
      " Hypothesis: tensor([[152.3593],\n",
      "        [183.9816],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5232]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9907432794570923 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954491924494505\n",
      "Epoch: 245 \n",
      " Hypothesis: tensor([[152.3593],\n",
      "        [183.9816],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5232]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.990697979927063 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009545058012008667\n",
      "Epoch: 246 \n",
      " Hypothesis: tensor([[152.3592],\n",
      "        [183.9817],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5233]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9906619787216187 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009545196779072285\n",
      "Epoch: 247 \n",
      " Hypothesis: tensor([[152.3592],\n",
      "        [183.9817],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5233]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9906355142593384 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009545335546135902\n",
      "Epoch: 248 \n",
      " Hypothesis: tensor([[152.3592],\n",
      "        [183.9817],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5233]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9905948638916016 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954547431319952\n",
      "Epoch: 249 \n",
      " Hypothesis: tensor([[152.3591],\n",
      "        [183.9817],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5233]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9905554056167603 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009545613080263138\n",
      "Epoch: 250 \n",
      " Hypothesis: tensor([[152.3591],\n",
      "        [183.9818],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5234]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.990515410900116 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009545751847326756\n",
      "Epoch: 251 \n",
      " Hypothesis: tensor([[152.3591],\n",
      "        [183.9818],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5234]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.990486741065979 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009545890614390373\n",
      "Epoch: 252 \n",
      " Hypothesis: tensor([[152.3590],\n",
      "        [183.9818],\n",
      "        [180.8362],\n",
      "        [196.9525],\n",
      "        [140.5234]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9904522895812988 \n",
      " W: tensor([[0.7275],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009546029381453991\n",
      "Epoch: 253 \n",
      " Hypothesis: tensor([[152.3590],\n",
      "        [183.9818],\n",
      "        [180.8361],\n",
      "        [196.9525],\n",
      "        [140.5235]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9904014468193054 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009546168148517609\n",
      "Epoch: 254 \n",
      " Hypothesis: tensor([[152.3590],\n",
      "        [183.9818],\n",
      "        [180.8361],\n",
      "        [196.9525],\n",
      "        [140.5235]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9903666377067566 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009546306915581226\n",
      "Epoch: 255 \n",
      " Hypothesis: tensor([[152.3589],\n",
      "        [183.9819],\n",
      "        [180.8361],\n",
      "        [196.9524],\n",
      "        [140.5235]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9903322458267212 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6023],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009546445682644844\n",
      "Epoch: 256 \n",
      " Hypothesis: tensor([[152.3589],\n",
      "        [183.9819],\n",
      "        [180.8361],\n",
      "        [196.9524],\n",
      "        [140.5236]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9902926683425903 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009546584449708462\n",
      "Epoch: 257 \n",
      " Hypothesis: tensor([[152.3589],\n",
      "        [183.9819],\n",
      "        [180.8361],\n",
      "        [196.9524],\n",
      "        [140.5236]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.990258514881134 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954672321677208\n",
      "Epoch: 258 \n",
      " Hypothesis: tensor([[152.3589],\n",
      "        [183.9819],\n",
      "        [180.8361],\n",
      "        [196.9524],\n",
      "        [140.5236]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9902240633964539 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009546861983835697\n",
      "Epoch: 259 \n",
      " Hypothesis: tensor([[152.3588],\n",
      "        [183.9819],\n",
      "        [180.8361],\n",
      "        [196.9524],\n",
      "        [140.5237]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9901782870292664 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009547000750899315\n",
      "Epoch: 260 \n",
      " Hypothesis: tensor([[152.3588],\n",
      "        [183.9820],\n",
      "        [180.8361],\n",
      "        [196.9524],\n",
      "        [140.5237]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.990143895149231 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009547139517962933\n",
      "Epoch: 261 \n",
      " Hypothesis: tensor([[152.3588],\n",
      "        [183.9820],\n",
      "        [180.8361],\n",
      "        [196.9524],\n",
      "        [140.5237]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9901091456413269 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954727828502655\n",
      "Epoch: 262 \n",
      " Hypothesis: tensor([[152.3587],\n",
      "        [183.9820],\n",
      "        [180.8361],\n",
      "        [196.9524],\n",
      "        [140.5237]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.990075409412384 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009547417052090168\n",
      "Epoch: 263 \n",
      " Hypothesis: tensor([[152.3587],\n",
      "        [183.9820],\n",
      "        [180.8361],\n",
      "        [196.9524],\n",
      "        [140.5238]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9900351762771606 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009547555819153786\n",
      "Epoch: 264 \n",
      " Hypothesis: tensor([[152.3587],\n",
      "        [183.9821],\n",
      "        [180.8360],\n",
      "        [196.9524],\n",
      "        [140.5238]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9900010824203491 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009547694586217403\n",
      "Epoch: 265 \n",
      " Hypothesis: tensor([[152.3586],\n",
      "        [183.9821],\n",
      "        [180.8360],\n",
      "        [196.9524],\n",
      "        [140.5238]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9899552464485168 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009547833353281021\n",
      "Epoch: 266 \n",
      " Hypothesis: tensor([[152.3586],\n",
      "        [183.9821],\n",
      "        [180.8360],\n",
      "        [196.9523],\n",
      "        [140.5239]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9899207949638367 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009547972120344639\n",
      "Epoch: 267 \n",
      " Hypothesis: tensor([[152.3586],\n",
      "        [183.9821],\n",
      "        [180.8360],\n",
      "        [196.9523],\n",
      "        [140.5239]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9898903965950012 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009548110887408257\n",
      "Epoch: 268 \n",
      " Hypothesis: tensor([[152.3586],\n",
      "        [183.9821],\n",
      "        [180.8360],\n",
      "        [196.9523],\n",
      "        [140.5239]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9898504018783569 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009548249654471874\n",
      "Epoch: 269 \n",
      " Hypothesis: tensor([[152.3585],\n",
      "        [183.9822],\n",
      "        [180.8360],\n",
      "        [196.9523],\n",
      "        [140.5240]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9898127317428589 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009548388421535492\n",
      "Epoch: 270 \n",
      " Hypothesis: tensor([[152.3585],\n",
      "        [183.9822],\n",
      "        [180.8360],\n",
      "        [196.9523],\n",
      "        [140.5240]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9897729158401489 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954852718859911\n",
      "Epoch: 271 \n",
      " Hypothesis: tensor([[152.3585],\n",
      "        [183.9822],\n",
      "        [180.8360],\n",
      "        [196.9523],\n",
      "        [140.5240]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9897384643554688 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009548665955662727\n",
      "Epoch: 272 \n",
      " Hypothesis: tensor([[152.3584],\n",
      "        [183.9822],\n",
      "        [180.8360],\n",
      "        [196.9523],\n",
      "        [140.5240]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9896928071975708 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009548804722726345\n",
      "Epoch: 273 \n",
      " Hypothesis: tensor([[152.3584],\n",
      "        [183.9823],\n",
      "        [180.8360],\n",
      "        [196.9523],\n",
      "        [140.5241]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9896731376647949 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009548943489789963\n",
      "Epoch: 274 \n",
      " Hypothesis: tensor([[152.3584],\n",
      "        [183.9823],\n",
      "        [180.8360],\n",
      "        [196.9523],\n",
      "        [140.5241]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.989630401134491 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00954908225685358\n",
      "Epoch: 275 \n",
      " Hypothesis: tensor([[152.3583],\n",
      "        [183.9823],\n",
      "        [180.8359],\n",
      "        [196.9523],\n",
      "        [140.5241]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.989584743976593 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009549221023917198\n",
      "Epoch: 276 \n",
      " Hypothesis: tensor([[152.3583],\n",
      "        [183.9823],\n",
      "        [180.8359],\n",
      "        [196.9523],\n",
      "        [140.5242]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9895440936088562 \n",
      " W: tensor([[0.7276],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009549359790980816\n",
      "Epoch: 277 \n",
      " Hypothesis: tensor([[152.3583],\n",
      "        [183.9823],\n",
      "        [180.8359],\n",
      "        [196.9523],\n",
      "        [140.5242]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.989519476890564 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009549498558044434\n",
      "Epoch: 278 \n",
      " Hypothesis: tensor([[152.3582],\n",
      "        [183.9824],\n",
      "        [180.8359],\n",
      "        [196.9523],\n",
      "        [140.5242]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9894857406616211 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6022],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009549637325108051\n",
      "Epoch: 279 \n",
      " Hypothesis: tensor([[152.3582],\n",
      "        [183.9824],\n",
      "        [180.8359],\n",
      "        [196.9522],\n",
      "        [140.5242]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9894393086433411 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009549776092171669\n",
      "Epoch: 280 \n",
      " Hypothesis: tensor([[152.3582],\n",
      "        [183.9824],\n",
      "        [180.8359],\n",
      "        [196.9523],\n",
      "        [140.5243]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9894075393676758 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009549914859235287\n",
      "Epoch: 281 \n",
      " Hypothesis: tensor([[152.3582],\n",
      "        [183.9824],\n",
      "        [180.8359],\n",
      "        [196.9522],\n",
      "        [140.5243]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9893617630004883 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009550053626298904\n",
      "Epoch: 282 \n",
      " Hypothesis: tensor([[152.3581],\n",
      "        [183.9825],\n",
      "        [180.8359],\n",
      "        [196.9522],\n",
      "        [140.5244]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.989322304725647 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009550192393362522\n",
      "Epoch: 283 \n",
      " Hypothesis: tensor([[152.3581],\n",
      "        [183.9825],\n",
      "        [180.8359],\n",
      "        [196.9522],\n",
      "        [140.5244]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9892905950546265 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955033116042614\n",
      "Epoch: 284 \n",
      " Hypothesis: tensor([[152.3581],\n",
      "        [183.9825],\n",
      "        [180.8359],\n",
      "        [196.9522],\n",
      "        [140.5244]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9892569780349731 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009550469927489758\n",
      "Epoch: 285 \n",
      " Hypothesis: tensor([[152.3580],\n",
      "        [183.9825],\n",
      "        [180.8359],\n",
      "        [196.9522],\n",
      "        [140.5244]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9892284274101257 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009550608694553375\n",
      "Epoch: 286 \n",
      " Hypothesis: tensor([[152.3580],\n",
      "        [183.9825],\n",
      "        [180.8358],\n",
      "        [196.9522],\n",
      "        [140.5245]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9891883730888367 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009550747461616993\n",
      "Epoch: 287 \n",
      " Hypothesis: tensor([[152.3580],\n",
      "        [183.9826],\n",
      "        [180.8358],\n",
      "        [196.9522],\n",
      "        [140.5245]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9891427159309387 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955088622868061\n",
      "Epoch: 288 \n",
      " Hypothesis: tensor([[152.3579],\n",
      "        [183.9826],\n",
      "        [180.8358],\n",
      "        [196.9522],\n",
      "        [140.5245]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9891082644462585 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009551024995744228\n",
      "Epoch: 289 \n",
      " Hypothesis: tensor([[152.3579],\n",
      "        [183.9826],\n",
      "        [180.8358],\n",
      "        [196.9522],\n",
      "        [140.5246]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9890796542167664 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009551163762807846\n",
      "Epoch: 290 \n",
      " Hypothesis: tensor([[152.3579],\n",
      "        [183.9826],\n",
      "        [180.8358],\n",
      "        [196.9522],\n",
      "        [140.5246]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9890397787094116 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009551302529871464\n",
      "Epoch: 291 \n",
      " Hypothesis: tensor([[152.3578],\n",
      "        [183.9827],\n",
      "        [180.8358],\n",
      "        [196.9521],\n",
      "        [140.5246]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9889996647834778 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009551441296935081\n",
      "Epoch: 292 \n",
      " Hypothesis: tensor([[152.3578],\n",
      "        [183.9827],\n",
      "        [180.8358],\n",
      "        [196.9521],\n",
      "        [140.5246]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9889596700668335 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.0095515800639987\n",
      "Epoch: 293 \n",
      " Hypothesis: tensor([[152.3578],\n",
      "        [183.9827],\n",
      "        [180.8358],\n",
      "        [196.9521],\n",
      "        [140.5247]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9889260530471802 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009551718831062317\n",
      "Epoch: 294 \n",
      " Hypothesis: tensor([[152.3578],\n",
      "        [183.9827],\n",
      "        [180.8358],\n",
      "        [196.9521],\n",
      "        [140.5247]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9888854026794434 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009551857598125935\n",
      "Epoch: 295 \n",
      " Hypothesis: tensor([[152.3577],\n",
      "        [183.9827],\n",
      "        [180.8358],\n",
      "        [196.9521],\n",
      "        [140.5247]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9888509511947632 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009551996365189552\n",
      "Epoch: 296 \n",
      " Hypothesis: tensor([[152.3577],\n",
      "        [183.9828],\n",
      "        [180.8358],\n",
      "        [196.9521],\n",
      "        [140.5248]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.988812267780304 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955213513225317\n",
      "Epoch: 297 \n",
      " Hypothesis: tensor([[152.3577],\n",
      "        [183.9828],\n",
      "        [180.8358],\n",
      "        [196.9521],\n",
      "        [140.5248]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9887774586677551 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009552273899316788\n",
      "Epoch: 298 \n",
      " Hypothesis: tensor([[152.3576],\n",
      "        [183.9828],\n",
      "        [180.8358],\n",
      "        [196.9521],\n",
      "        [140.5248]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9887368083000183 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009552412666380405\n",
      "Epoch: 299 \n",
      " Hypothesis: tensor([[152.3576],\n",
      "        [183.9828],\n",
      "        [180.8357],\n",
      "        [196.9521],\n",
      "        [140.5249]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.988697350025177 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009552551433444023\n",
      "Epoch: 300 \n",
      " Hypothesis: tensor([[152.3576],\n",
      "        [183.9828],\n",
      "        [180.8357],\n",
      "        [196.9521],\n",
      "        [140.5249]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9886636734008789 \n",
      " W: tensor([[0.7277],\n",
      "        [0.6021],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955269020050764\n",
      "Epoch: 301 \n",
      " Hypothesis: tensor([[152.3575],\n",
      "        [183.9829],\n",
      "        [180.8357],\n",
      "        [196.9521],\n",
      "        [140.5249]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9886288642883301 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009552828967571259\n",
      "Epoch: 302 \n",
      " Hypothesis: tensor([[152.3575],\n",
      "        [183.9829],\n",
      "        [180.8357],\n",
      "        [196.9521],\n",
      "        [140.5249]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9886041879653931 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009552967734634876\n",
      "Epoch: 303 \n",
      " Hypothesis: tensor([[152.3575],\n",
      "        [183.9829],\n",
      "        [180.8357],\n",
      "        [196.9521],\n",
      "        [140.5250]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9885698556900024 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009553106501698494\n",
      "Epoch: 304 \n",
      " Hypothesis: tensor([[152.3575],\n",
      "        [183.9829],\n",
      "        [180.8357],\n",
      "        [196.9521],\n",
      "        [140.5250]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9885150194168091 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009553245268762112\n",
      "Epoch: 305 \n",
      " Hypothesis: tensor([[152.3574],\n",
      "        [183.9830],\n",
      "        [180.8357],\n",
      "        [196.9521],\n",
      "        [140.5250]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.988480269908905 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955338403582573\n",
      "Epoch: 306 \n",
      " Hypothesis: tensor([[152.3574],\n",
      "        [183.9830],\n",
      "        [180.8357],\n",
      "        [196.9521],\n",
      "        [140.5251]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9884465932846069 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009553522802889347\n",
      "Epoch: 307 \n",
      " Hypothesis: tensor([[152.3574],\n",
      "        [183.9830],\n",
      "        [180.8357],\n",
      "        [196.9520],\n",
      "        [140.5251]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9884014129638672 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009553661569952965\n",
      "Epoch: 308 \n",
      " Hypothesis: tensor([[152.3573],\n",
      "        [183.9830],\n",
      "        [180.8357],\n",
      "        [196.9520],\n",
      "        [140.5251]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9883664846420288 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009553800337016582\n",
      "Epoch: 309 \n",
      " Hypothesis: tensor([[152.3573],\n",
      "        [183.9831],\n",
      "        [180.8356],\n",
      "        [196.9520],\n",
      "        [140.5252]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9883266687393188 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.0095539391040802\n",
      "Epoch: 310 \n",
      " Hypothesis: tensor([[152.3573],\n",
      "        [183.9831],\n",
      "        [180.8356],\n",
      "        [196.9520],\n",
      "        [140.5252]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9883025288581848 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009554077871143818\n",
      "Epoch: 311 \n",
      " Hypothesis: tensor([[152.3572],\n",
      "        [183.9831],\n",
      "        [180.8356],\n",
      "        [196.9520],\n",
      "        [140.5252]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.988261878490448 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009554216638207436\n",
      "Epoch: 312 \n",
      " Hypothesis: tensor([[152.3572],\n",
      "        [183.9831],\n",
      "        [180.8356],\n",
      "        [196.9520],\n",
      "        [140.5252]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9882332682609558 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009554355405271053\n",
      "Epoch: 313 \n",
      " Hypothesis: tensor([[152.3572],\n",
      "        [183.9831],\n",
      "        [180.8356],\n",
      "        [196.9520],\n",
      "        [140.5253]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9881933927536011 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009554494172334671\n",
      "Epoch: 314 \n",
      " Hypothesis: tensor([[152.3571],\n",
      "        [183.9832],\n",
      "        [180.8356],\n",
      "        [196.9520],\n",
      "        [140.5253]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9881387948989868 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009554632939398289\n",
      "Epoch: 315 \n",
      " Hypothesis: tensor([[152.3571],\n",
      "        [183.9832],\n",
      "        [180.8356],\n",
      "        [196.9520],\n",
      "        [140.5253]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9881043434143066 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009554771706461906\n",
      "Epoch: 316 \n",
      " Hypothesis: tensor([[152.3571],\n",
      "        [183.9832],\n",
      "        [180.8356],\n",
      "        [196.9520],\n",
      "        [140.5254]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9880785942077637 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009554910473525524\n",
      "Epoch: 317 \n",
      " Hypothesis: tensor([[152.3571],\n",
      "        [183.9832],\n",
      "        [180.8356],\n",
      "        [196.9520],\n",
      "        [140.5254]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9880459904670715 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009555049240589142\n",
      "Epoch: 318 \n",
      " Hypothesis: tensor([[152.3570],\n",
      "        [183.9832],\n",
      "        [180.8356],\n",
      "        [196.9519],\n",
      "        [140.5254]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9879996180534363 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955518800765276\n",
      "Epoch: 319 \n",
      " Hypothesis: tensor([[152.3570],\n",
      "        [183.9833],\n",
      "        [180.8356],\n",
      "        [196.9519],\n",
      "        [140.5255]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9879648089408875 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009555326774716377\n",
      "Epoch: 320 \n",
      " Hypothesis: tensor([[152.3570],\n",
      "        [183.9833],\n",
      "        [180.8356],\n",
      "        [196.9519],\n",
      "        [140.5255]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9879311323165894 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009555465541779995\n",
      "Epoch: 321 \n",
      " Hypothesis: tensor([[152.3569],\n",
      "        [183.9833],\n",
      "        [180.8355],\n",
      "        [196.9519],\n",
      "        [140.5255]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9878854751586914 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009555604308843613\n",
      "Epoch: 322 \n",
      " Hypothesis: tensor([[152.3569],\n",
      "        [183.9833],\n",
      "        [180.8355],\n",
      "        [196.9519],\n",
      "        [140.5255]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9878511428833008 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955574307590723\n",
      "Epoch: 323 \n",
      " Hypothesis: tensor([[152.3569],\n",
      "        [183.9834],\n",
      "        [180.8355],\n",
      "        [196.9519],\n",
      "        [140.5256]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.987811267375946 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6020],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009555881842970848\n",
      "Epoch: 324 \n",
      " Hypothesis: tensor([[152.3568],\n",
      "        [183.9834],\n",
      "        [180.8355],\n",
      "        [196.9519],\n",
      "        [140.5256]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9877889752388 \n",
      " W: tensor([[0.7278],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009556020610034466\n",
      "Epoch: 325 \n",
      " Hypothesis: tensor([[152.3568],\n",
      "        [183.9834],\n",
      "        [180.8355],\n",
      "        [196.9519],\n",
      "        [140.5256]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9877483248710632 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009556159377098083\n",
      "Epoch: 326 \n",
      " Hypothesis: tensor([[152.3568],\n",
      "        [183.9834],\n",
      "        [180.8355],\n",
      "        [196.9519],\n",
      "        [140.5257]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9877084493637085 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009556298144161701\n",
      "Epoch: 327 \n",
      " Hypothesis: tensor([[152.3568],\n",
      "        [183.9834],\n",
      "        [180.8355],\n",
      "        [196.9519],\n",
      "        [140.5257]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9876740574836731 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009556436911225319\n",
      "Epoch: 328 \n",
      " Hypothesis: tensor([[152.3567],\n",
      "        [183.9835],\n",
      "        [180.8355],\n",
      "        [196.9519],\n",
      "        [140.5257]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9876405000686646 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009556575678288937\n",
      "Epoch: 329 \n",
      " Hypothesis: tensor([[152.3567],\n",
      "        [183.9835],\n",
      "        [180.8355],\n",
      "        [196.9519],\n",
      "        [140.5258]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9876006245613098 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009556714445352554\n",
      "Epoch: 330 \n",
      " Hypothesis: tensor([[152.3567],\n",
      "        [183.9835],\n",
      "        [180.8355],\n",
      "        [196.9519],\n",
      "        [140.5258]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.987559974193573 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009556853212416172\n",
      "Epoch: 331 \n",
      " Hypothesis: tensor([[152.3566],\n",
      "        [183.9835],\n",
      "        [180.8355],\n",
      "        [196.9518],\n",
      "        [140.5258]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9875268936157227 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955699197947979\n",
      "Epoch: 332 \n",
      " Hypothesis: tensor([[152.3566],\n",
      "        [183.9836],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5258]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9874868392944336 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009557130746543407\n",
      "Epoch: 333 \n",
      " Hypothesis: tensor([[152.3566],\n",
      "        [183.9836],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5259]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9874521493911743 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009557269513607025\n",
      "Epoch: 334 \n",
      " Hypothesis: tensor([[152.3565],\n",
      "        [183.9836],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5259]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9874069094657898 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009557408280670643\n",
      "Epoch: 335 \n",
      " Hypothesis: tensor([[152.3565],\n",
      "        [183.9836],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5259]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9873743057250977 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955754704773426\n",
      "Epoch: 336 \n",
      " Hypothesis: tensor([[152.3565],\n",
      "        [183.9836],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5260]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9873385429382324 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009557685814797878\n",
      "Epoch: 337 \n",
      " Hypothesis: tensor([[152.3564],\n",
      "        [183.9837],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5260]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9872980117797852 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009557824581861496\n",
      "Epoch: 338 \n",
      " Hypothesis: tensor([[152.3564],\n",
      "        [183.9837],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5260]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9872523546218872 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009557963348925114\n",
      "Epoch: 339 \n",
      " Hypothesis: tensor([[152.3564],\n",
      "        [183.9837],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5261]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9872190356254578 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009558102115988731\n",
      "Epoch: 340 \n",
      " Hypothesis: tensor([[152.3564],\n",
      "        [183.9837],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5261]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9871901273727417 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009558240883052349\n",
      "Epoch: 341 \n",
      " Hypothesis: tensor([[152.3563],\n",
      "        [183.9838],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5261]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9871465563774109 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009558379650115967\n",
      "Epoch: 342 \n",
      " Hypothesis: tensor([[152.3563],\n",
      "        [183.9838],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5262]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9871050119400024 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009558518417179585\n",
      "Epoch: 343 \n",
      " Hypothesis: tensor([[152.3563],\n",
      "        [183.9838],\n",
      "        [180.8354],\n",
      "        [196.9518],\n",
      "        [140.5262]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.987076461315155 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009558657184243202\n",
      "Epoch: 344 \n",
      " Hypothesis: tensor([[152.3562],\n",
      "        [183.9838],\n",
      "        [180.8354],\n",
      "        [196.9517],\n",
      "        [140.5262]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.987035870552063 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955879595130682\n",
      "Epoch: 345 \n",
      " Hypothesis: tensor([[152.3562],\n",
      "        [183.9839],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5262]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9869901537895203 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009558934718370438\n",
      "Epoch: 346 \n",
      " Hypothesis: tensor([[152.3562],\n",
      "        [183.9839],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5263]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9869627952575684 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6019],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009559073485434055\n",
      "Epoch: 347 \n",
      " Hypothesis: tensor([[152.3562],\n",
      "        [183.9839],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5263]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9869301915168762 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009559212252497673\n",
      "Epoch: 348 \n",
      " Hypothesis: tensor([[152.3561],\n",
      "        [183.9839],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5263]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.986888587474823 \n",
      " W: tensor([[0.7279],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00955935101956129\n",
      "Epoch: 349 \n",
      " Hypothesis: tensor([[152.3561],\n",
      "        [183.9839],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5264]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9868480563163757 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009559489786624908\n",
      "Epoch: 350 \n",
      " Hypothesis: tensor([[152.3560],\n",
      "        [183.9839],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5264]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9868143796920776 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009559628553688526\n",
      "Epoch: 351 \n",
      " Hypothesis: tensor([[152.3560],\n",
      "        [183.9840],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5264]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9867745637893677 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009559767320752144\n",
      "Epoch: 352 \n",
      " Hypothesis: tensor([[152.3560],\n",
      "        [183.9840],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5265]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9867377281188965 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009559906087815762\n",
      "Epoch: 353 \n",
      " Hypothesis: tensor([[152.3560],\n",
      "        [183.9840],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5265]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9867207407951355 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956004485487938\n",
      "Epoch: 354 \n",
      " Hypothesis: tensor([[152.3559],\n",
      "        [183.9841],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5265]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.986663818359375 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009560183621942997\n",
      "Epoch: 355 \n",
      " Hypothesis: tensor([[152.3559],\n",
      "        [183.9841],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5266]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9866294860839844 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009560322389006615\n",
      "Epoch: 356 \n",
      " Hypothesis: tensor([[152.3559],\n",
      "        [183.9841],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5266]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9865955114364624 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009560461156070232\n",
      "Epoch: 357 \n",
      " Hypothesis: tensor([[152.3559],\n",
      "        [183.9841],\n",
      "        [180.8353],\n",
      "        [196.9517],\n",
      "        [140.5266]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.986549973487854 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956059992313385\n",
      "Epoch: 358 \n",
      " Hypothesis: tensor([[152.3558],\n",
      "        [183.9841],\n",
      "        [180.8353],\n",
      "        [196.9516],\n",
      "        [140.5267]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9865156412124634 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009560738690197468\n",
      "Epoch: 359 \n",
      " Hypothesis: tensor([[152.3558],\n",
      "        [183.9842],\n",
      "        [180.8353],\n",
      "        [196.9516],\n",
      "        [140.5267]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9864829778671265 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009560877457261086\n",
      "Epoch: 360 \n",
      " Hypothesis: tensor([[152.3558],\n",
      "        [183.9842],\n",
      "        [180.8352],\n",
      "        [196.9516],\n",
      "        [140.5267]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9864457845687866 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009561016224324703\n",
      "Epoch: 361 \n",
      " Hypothesis: tensor([[152.3557],\n",
      "        [183.9842],\n",
      "        [180.8352],\n",
      "        [196.9516],\n",
      "        [140.5267]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9864059686660767 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009561154991388321\n",
      "Epoch: 362 \n",
      " Hypothesis: tensor([[152.3557],\n",
      "        [183.9842],\n",
      "        [180.8352],\n",
      "        [196.9516],\n",
      "        [140.5268]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.986371636390686 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009561293758451939\n",
      "Epoch: 363 \n",
      " Hypothesis: tensor([[152.3557],\n",
      "        [183.9842],\n",
      "        [180.8352],\n",
      "        [196.9516],\n",
      "        [140.5268]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9863321185112 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009561432525515556\n",
      "Epoch: 364 \n",
      " Hypothesis: tensor([[152.3556],\n",
      "        [183.9843],\n",
      "        [180.8352],\n",
      "        [196.9516],\n",
      "        [140.5268]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9862974286079407 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009561571292579174\n",
      "Epoch: 365 \n",
      " Hypothesis: tensor([[152.3556],\n",
      "        [183.9843],\n",
      "        [180.8352],\n",
      "        [196.9516],\n",
      "        [140.5269]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9862638711929321 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009561710059642792\n",
      "Epoch: 366 \n",
      " Hypothesis: tensor([[152.3556],\n",
      "        [183.9843],\n",
      "        [180.8352],\n",
      "        [196.9516],\n",
      "        [140.5269]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9862381219863892 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956184882670641\n",
      "Epoch: 367 \n",
      " Hypothesis: tensor([[152.3555],\n",
      "        [183.9843],\n",
      "        [180.8352],\n",
      "        [196.9516],\n",
      "        [140.5269]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9861924052238464 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009561987593770027\n",
      "Epoch: 368 \n",
      " Hypothesis: tensor([[152.3555],\n",
      "        [183.9844],\n",
      "        [180.8351],\n",
      "        [196.9516],\n",
      "        [140.5269]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.986143946647644 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6018],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009562126360833645\n",
      "Epoch: 369 \n",
      " Hypothesis: tensor([[152.3555],\n",
      "        [183.9844],\n",
      "        [180.8351],\n",
      "        [196.9516],\n",
      "        [140.5270]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.986121654510498 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009562265127897263\n",
      "Epoch: 370 \n",
      " Hypothesis: tensor([[152.3555],\n",
      "        [183.9844],\n",
      "        [180.8351],\n",
      "        [196.9516],\n",
      "        [140.5270]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9860818982124329 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956240389496088\n",
      "Epoch: 371 \n",
      " Hypothesis: tensor([[152.3554],\n",
      "        [183.9844],\n",
      "        [180.8351],\n",
      "        [196.9515],\n",
      "        [140.5270]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9860413670539856 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009562542662024498\n",
      "Epoch: 372 \n",
      " Hypothesis: tensor([[152.3554],\n",
      "        [183.9844],\n",
      "        [180.8351],\n",
      "        [196.9515],\n",
      "        [140.5271]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9860108494758606 \n",
      " W: tensor([[0.7280],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009562681429088116\n",
      "Epoch: 373 \n",
      " Hypothesis: tensor([[152.3554],\n",
      "        [183.9845],\n",
      "        [180.8351],\n",
      "        [196.9515],\n",
      "        [140.5271]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9859716296195984 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009562820196151733\n",
      "Epoch: 374 \n",
      " Hypothesis: tensor([[152.3553],\n",
      "        [183.9845],\n",
      "        [180.8351],\n",
      "        [196.9515],\n",
      "        [140.5271]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9859369397163391 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009562958963215351\n",
      "Epoch: 375 \n",
      " Hypothesis: tensor([[152.3553],\n",
      "        [183.9845],\n",
      "        [180.8351],\n",
      "        [196.9515],\n",
      "        [140.5271]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.985897421836853 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009563097730278969\n",
      "Epoch: 376 \n",
      " Hypothesis: tensor([[152.3553],\n",
      "        [183.9845],\n",
      "        [180.8351],\n",
      "        [196.9515],\n",
      "        [140.5272]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9858518838882446 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009563236497342587\n",
      "Epoch: 377 \n",
      " Hypothesis: tensor([[152.3552],\n",
      "        [183.9845],\n",
      "        [180.8351],\n",
      "        [196.9515],\n",
      "        [140.5272]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.985817551612854 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009563375264406204\n",
      "Epoch: 378 \n",
      " Hypothesis: tensor([[152.3552],\n",
      "        [183.9846],\n",
      "        [180.8351],\n",
      "        [196.9515],\n",
      "        [140.5272]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9857776761054993 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009563514031469822\n",
      "Epoch: 379 \n",
      " Hypothesis: tensor([[152.3552],\n",
      "        [183.9846],\n",
      "        [180.8350],\n",
      "        [196.9515],\n",
      "        [140.5273]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.985741913318634 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956365279853344\n",
      "Epoch: 380 \n",
      " Hypothesis: tensor([[152.3551],\n",
      "        [183.9846],\n",
      "        [180.8350],\n",
      "        [196.9514],\n",
      "        [140.5273]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9857097864151001 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009563791565597057\n",
      "Epoch: 381 \n",
      " Hypothesis: tensor([[152.3551],\n",
      "        [183.9846],\n",
      "        [180.8350],\n",
      "        [196.9514],\n",
      "        [140.5273]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9856652021408081 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009563930332660675\n",
      "Epoch: 382 \n",
      " Hypothesis: tensor([[152.3551],\n",
      "        [183.9846],\n",
      "        [180.8350],\n",
      "        [196.9514],\n",
      "        [140.5273]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9856435060501099 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009564069099724293\n",
      "Epoch: 383 \n",
      " Hypothesis: tensor([[152.3550],\n",
      "        [183.9847],\n",
      "        [180.8350],\n",
      "        [196.9514],\n",
      "        [140.5274]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9855979084968567 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956420786678791\n",
      "Epoch: 384 \n",
      " Hypothesis: tensor([[152.3550],\n",
      "        [183.9847],\n",
      "        [180.8350],\n",
      "        [196.9514],\n",
      "        [140.5274]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9855573773384094 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009564346633851528\n",
      "Epoch: 385 \n",
      " Hypothesis: tensor([[152.3550],\n",
      "        [183.9847],\n",
      "        [180.8350],\n",
      "        [196.9514],\n",
      "        [140.5274]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9855237007141113 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009564485400915146\n",
      "Epoch: 386 \n",
      " Hypothesis: tensor([[152.3550],\n",
      "        [183.9847],\n",
      "        [180.8350],\n",
      "        [196.9514],\n",
      "        [140.5275]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9854884147644043 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009564624167978764\n",
      "Epoch: 387 \n",
      " Hypothesis: tensor([[152.3549],\n",
      "        [183.9847],\n",
      "        [180.8350],\n",
      "        [196.9514],\n",
      "        [140.5275]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9854568243026733 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009564762935042381\n",
      "Epoch: 388 \n",
      " Hypothesis: tensor([[152.3549],\n",
      "        [183.9848],\n",
      "        [180.8349],\n",
      "        [196.9514],\n",
      "        [140.5275]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9854192733764648 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009564901702105999\n",
      "Epoch: 389 \n",
      " Hypothesis: tensor([[152.3549],\n",
      "        [183.9848],\n",
      "        [180.8349],\n",
      "        [196.9514],\n",
      "        [140.5276]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9853760600090027 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009565040469169617\n",
      "Epoch: 390 \n",
      " Hypothesis: tensor([[152.3548],\n",
      "        [183.9848],\n",
      "        [180.8349],\n",
      "        [196.9514],\n",
      "        [140.5276]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.985336184501648 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009565179236233234\n",
      "Epoch: 391 \n",
      " Hypothesis: tensor([[152.3548],\n",
      "        [183.9848],\n",
      "        [180.8349],\n",
      "        [196.9513],\n",
      "        [140.5276]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9852906465530396 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6017],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009565318003296852\n",
      "Epoch: 392 \n",
      " Hypothesis: tensor([[152.3548],\n",
      "        [183.9849],\n",
      "        [180.8349],\n",
      "        [196.9513],\n",
      "        [140.5276]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9852620959281921 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956545677036047\n",
      "Epoch: 393 \n",
      " Hypothesis: tensor([[152.3547],\n",
      "        [183.9849],\n",
      "        [180.8349],\n",
      "        [196.9513],\n",
      "        [140.5277]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.985235333442688 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009565595537424088\n",
      "Epoch: 394 \n",
      " Hypothesis: tensor([[152.3547],\n",
      "        [183.9849],\n",
      "        [180.8349],\n",
      "        [196.9513],\n",
      "        [140.5277]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9851969480514526 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009565734304487705\n",
      "Epoch: 395 \n",
      " Hypothesis: tensor([[152.3547],\n",
      "        [183.9849],\n",
      "        [180.8349],\n",
      "        [196.9513],\n",
      "        [140.5277]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9851616024971008 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009565873071551323\n",
      "Epoch: 396 \n",
      " Hypothesis: tensor([[152.3546],\n",
      "        [183.9849],\n",
      "        [180.8349],\n",
      "        [196.9513],\n",
      "        [140.5278]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9851210713386536 \n",
      " W: tensor([[0.7281],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956601183861494\n",
      "Epoch: 397 \n",
      " Hypothesis: tensor([[152.3546],\n",
      "        [183.9850],\n",
      "        [180.8349],\n",
      "        [196.9513],\n",
      "        [140.5278]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.985081672668457 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009566150605678558\n",
      "Epoch: 398 \n",
      " Hypothesis: tensor([[152.3546],\n",
      "        [183.9850],\n",
      "        [180.8349],\n",
      "        [196.9513],\n",
      "        [140.5278]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9850481152534485 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009566289372742176\n",
      "Epoch: 399 \n",
      " Hypothesis: tensor([[152.3545],\n",
      "        [183.9850],\n",
      "        [180.8349],\n",
      "        [196.9513],\n",
      "        [140.5278]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9850075840950012 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009566428139805794\n",
      "Epoch: 400 \n",
      " Hypothesis: tensor([[152.3545],\n",
      "        [183.9850],\n",
      "        [180.8348],\n",
      "        [196.9513],\n",
      "        [140.5279]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9849736094474792 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009566566906869411\n",
      "Epoch: 401 \n",
      " Hypothesis: tensor([[152.3545],\n",
      "        [183.9850],\n",
      "        [180.8348],\n",
      "        [196.9512],\n",
      "        [140.5279]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9849346280097961 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956670567393303\n",
      "Epoch: 402 \n",
      " Hypothesis: tensor([[152.3544],\n",
      "        [183.9851],\n",
      "        [180.8348],\n",
      "        [196.9512],\n",
      "        [140.5279]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9848998785018921 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009566844440996647\n",
      "Epoch: 403 \n",
      " Hypothesis: tensor([[152.3544],\n",
      "        [183.9851],\n",
      "        [180.8348],\n",
      "        [196.9512],\n",
      "        [140.5280]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9848623275756836 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009566983208060265\n",
      "Epoch: 404 \n",
      " Hypothesis: tensor([[152.3544],\n",
      "        [183.9851],\n",
      "        [180.8348],\n",
      "        [196.9512],\n",
      "        [140.5280]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9848302006721497 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009567121975123882\n",
      "Epoch: 405 \n",
      " Hypothesis: tensor([[152.3544],\n",
      "        [183.9851],\n",
      "        [180.8348],\n",
      "        [196.9512],\n",
      "        [140.5280]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9847954511642456 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.0095672607421875\n",
      "Epoch: 406 \n",
      " Hypothesis: tensor([[152.3543],\n",
      "        [183.9851],\n",
      "        [180.8348],\n",
      "        [196.9512],\n",
      "        [140.5280]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9847509264945984 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009567399509251118\n",
      "Epoch: 407 \n",
      " Hypothesis: tensor([[152.3543],\n",
      "        [183.9852],\n",
      "        [180.8348],\n",
      "        [196.9512],\n",
      "        [140.5281]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9847220182418823 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009567538276314735\n",
      "Epoch: 408 \n",
      " Hypothesis: tensor([[152.3543],\n",
      "        [183.9852],\n",
      "        [180.8348],\n",
      "        [196.9512],\n",
      "        [140.5281]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9846819639205933 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009567677043378353\n",
      "Epoch: 409 \n",
      " Hypothesis: tensor([[152.3542],\n",
      "        [183.9852],\n",
      "        [180.8347],\n",
      "        [196.9512],\n",
      "        [140.5281]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9846490621566772 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956781581044197\n",
      "Epoch: 410 \n",
      " Hypothesis: tensor([[152.3542],\n",
      "        [183.9852],\n",
      "        [180.8347],\n",
      "        [196.9512],\n",
      "        [140.5282]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9846028089523315 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009567954577505589\n",
      "Epoch: 411 \n",
      " Hypothesis: tensor([[152.3542],\n",
      "        [183.9852],\n",
      "        [180.8347],\n",
      "        [196.9512],\n",
      "        [140.5282]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9845684766769409 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009568093344569206\n",
      "Epoch: 412 \n",
      " Hypothesis: tensor([[152.3541],\n",
      "        [183.9853],\n",
      "        [180.8347],\n",
      "        [196.9512],\n",
      "        [140.5282]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9845337867736816 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009568232111632824\n",
      "Epoch: 413 \n",
      " Hypothesis: tensor([[152.3541],\n",
      "        [183.9853],\n",
      "        [180.8347],\n",
      "        [196.9511],\n",
      "        [140.5283]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9844897985458374 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009568370878696442\n",
      "Epoch: 414 \n",
      " Hypothesis: tensor([[152.3541],\n",
      "        [183.9853],\n",
      "        [180.8347],\n",
      "        [196.9511],\n",
      "        [140.5283]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9844619035720825 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6016],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956850964576006\n",
      "Epoch: 415 \n",
      " Hypothesis: tensor([[152.3540],\n",
      "        [183.9853],\n",
      "        [180.8347],\n",
      "        [196.9511],\n",
      "        [140.5283]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9844242930412292 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009568648412823677\n",
      "Epoch: 416 \n",
      " Hypothesis: tensor([[152.3540],\n",
      "        [183.9854],\n",
      "        [180.8347],\n",
      "        [196.9511],\n",
      "        [140.5283]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9843958020210266 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009568787179887295\n",
      "Epoch: 417 \n",
      " Hypothesis: tensor([[152.3540],\n",
      "        [183.9854],\n",
      "        [180.8347],\n",
      "        [196.9511],\n",
      "        [140.5284]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9843541979789734 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009568925946950912\n",
      "Epoch: 418 \n",
      " Hypothesis: tensor([[152.3539],\n",
      "        [183.9854],\n",
      "        [180.8347],\n",
      "        [196.9511],\n",
      "        [140.5284]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9843088388442993 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00956906471401453\n",
      "Epoch: 419 \n",
      " Hypothesis: tensor([[152.3539],\n",
      "        [183.9854],\n",
      "        [180.8347],\n",
      "        [196.9511],\n",
      "        [140.5284]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9842802286148071 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009569203481078148\n",
      "Epoch: 420 \n",
      " Hypothesis: tensor([[152.3539],\n",
      "        [183.9854],\n",
      "        [180.8346],\n",
      "        [196.9511],\n",
      "        [140.5285]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9842289090156555 \n",
      " W: tensor([[0.7282],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009569342248141766\n",
      "Epoch: 421 \n",
      " Hypothesis: tensor([[152.3539],\n",
      "        [183.9855],\n",
      "        [180.8346],\n",
      "        [196.9511],\n",
      "        [140.5285]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9842025637626648 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009569481015205383\n",
      "Epoch: 422 \n",
      " Hypothesis: tensor([[152.3538],\n",
      "        [183.9855],\n",
      "        [180.8346],\n",
      "        [196.9511],\n",
      "        [140.5285]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9841595888137817 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009569619782269001\n",
      "Epoch: 423 \n",
      " Hypothesis: tensor([[152.3538],\n",
      "        [183.9855],\n",
      "        [180.8346],\n",
      "        [196.9511],\n",
      "        [140.5285]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9841333627700806 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009569758549332619\n",
      "Epoch: 424 \n",
      " Hypothesis: tensor([[152.3537],\n",
      "        [183.9855],\n",
      "        [180.8346],\n",
      "        [196.9510],\n",
      "        [140.5286]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9840887188911438 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009569897316396236\n",
      "Epoch: 425 \n",
      " Hypothesis: tensor([[152.3537],\n",
      "        [183.9855],\n",
      "        [180.8346],\n",
      "        [196.9510],\n",
      "        [140.5286]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9840602874755859 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009570036083459854\n",
      "Epoch: 426 \n",
      " Hypothesis: tensor([[152.3537],\n",
      "        [183.9855],\n",
      "        [180.8346],\n",
      "        [196.9510],\n",
      "        [140.5286]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9840208888053894 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009570174850523472\n",
      "Epoch: 427 \n",
      " Hypothesis: tensor([[152.3537],\n",
      "        [183.9856],\n",
      "        [180.8346],\n",
      "        [196.9510],\n",
      "        [140.5287]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9839752316474915 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957031361758709\n",
      "Epoch: 428 \n",
      " Hypothesis: tensor([[152.3536],\n",
      "        [183.9856],\n",
      "        [180.8346],\n",
      "        [196.9510],\n",
      "        [140.5287]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9839490056037903 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009570452384650707\n",
      "Epoch: 429 \n",
      " Hypothesis: tensor([[152.3536],\n",
      "        [183.9856],\n",
      "        [180.8345],\n",
      "        [196.9510],\n",
      "        [140.5287]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.983900249004364 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009570591151714325\n",
      "Epoch: 430 \n",
      " Hypothesis: tensor([[152.3536],\n",
      "        [183.9856],\n",
      "        [180.8345],\n",
      "        [196.9510],\n",
      "        [140.5287]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9838709831237793 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009570729918777943\n",
      "Epoch: 431 \n",
      " Hypothesis: tensor([[152.3535],\n",
      "        [183.9857],\n",
      "        [180.8345],\n",
      "        [196.9510],\n",
      "        [140.5288]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.98383629322052 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957086868584156\n",
      "Epoch: 432 \n",
      " Hypothesis: tensor([[152.3535],\n",
      "        [183.9857],\n",
      "        [180.8345],\n",
      "        [196.9510],\n",
      "        [140.5288]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9837937355041504 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009571007452905178\n",
      "Epoch: 433 \n",
      " Hypothesis: tensor([[152.3535],\n",
      "        [183.9857],\n",
      "        [180.8345],\n",
      "        [196.9510],\n",
      "        [140.5288]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9837595224380493 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009571146219968796\n",
      "Epoch: 434 \n",
      " Hypothesis: tensor([[152.3535],\n",
      "        [183.9857],\n",
      "        [180.8345],\n",
      "        [196.9509],\n",
      "        [140.5289]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9837160110473633 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009571284987032413\n",
      "Epoch: 435 \n",
      " Hypothesis: tensor([[152.3534],\n",
      "        [183.9857],\n",
      "        [180.8345],\n",
      "        [196.9509],\n",
      "        [140.5289]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9836893081665039 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009571423754096031\n",
      "Epoch: 436 \n",
      " Hypothesis: tensor([[152.3534],\n",
      "        [183.9858],\n",
      "        [180.8345],\n",
      "        [196.9509],\n",
      "        [140.5289]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9836492538452148 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6015],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009571562521159649\n",
      "Epoch: 437 \n",
      " Hypothesis: tensor([[152.3533],\n",
      "        [183.9858],\n",
      "        [180.8345],\n",
      "        [196.9509],\n",
      "        [140.5289]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9836094975471497 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009571701288223267\n",
      "Epoch: 438 \n",
      " Hypothesis: tensor([[152.3533],\n",
      "        [183.9858],\n",
      "        [180.8345],\n",
      "        [196.9509],\n",
      "        [140.5290]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9835759401321411 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009571840055286884\n",
      "Epoch: 439 \n",
      " Hypothesis: tensor([[152.3533],\n",
      "        [183.9858],\n",
      "        [180.8345],\n",
      "        [196.9509],\n",
      "        [140.5290]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9835475087165833 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009571978822350502\n",
      "Epoch: 440 \n",
      " Hypothesis: tensor([[152.3533],\n",
      "        [183.9859],\n",
      "        [180.8345],\n",
      "        [196.9509],\n",
      "        [140.5290]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9835070371627808 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957211758941412\n",
      "Epoch: 441 \n",
      " Hypothesis: tensor([[152.3532],\n",
      "        [183.9859],\n",
      "        [180.8344],\n",
      "        [196.9509],\n",
      "        [140.5291]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9834625124931335 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009572256356477737\n",
      "Epoch: 442 \n",
      " Hypothesis: tensor([[152.3532],\n",
      "        [183.9859],\n",
      "        [180.8344],\n",
      "        [196.9509],\n",
      "        [140.5291]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9834280014038086 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009572395123541355\n",
      "Epoch: 443 \n",
      " Hypothesis: tensor([[152.3531],\n",
      "        [183.9859],\n",
      "        [180.8344],\n",
      "        [196.9509],\n",
      "        [140.5291]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9833806753158569 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009572533890604973\n",
      "Epoch: 444 \n",
      " Hypothesis: tensor([[152.3531],\n",
      "        [183.9859],\n",
      "        [180.8344],\n",
      "        [196.9509],\n",
      "        [140.5292]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9833492040634155 \n",
      " W: tensor([[0.7283],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957267265766859\n",
      "Epoch: 445 \n",
      " Hypothesis: tensor([[152.3531],\n",
      "        [183.9859],\n",
      "        [180.8344],\n",
      "        [196.9509],\n",
      "        [140.5292]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9833208322525024 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009572811424732208\n",
      "Epoch: 446 \n",
      " Hypothesis: tensor([[152.3531],\n",
      "        [183.9860],\n",
      "        [180.8344],\n",
      "        [196.9509],\n",
      "        [140.5292]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.983281135559082 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009572950191795826\n",
      "Epoch: 447 \n",
      " Hypothesis: tensor([[152.3530],\n",
      "        [183.9860],\n",
      "        [180.8344],\n",
      "        [196.9508],\n",
      "        [140.5292]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9832485318183899 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009573088958859444\n",
      "Epoch: 448 \n",
      " Hypothesis: tensor([[152.3530],\n",
      "        [183.9860],\n",
      "        [180.8344],\n",
      "        [196.9508],\n",
      "        [140.5293]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.983214259147644 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009573227725923061\n",
      "Epoch: 449 \n",
      " Hypothesis: tensor([[152.3530],\n",
      "        [183.9860],\n",
      "        [180.8344],\n",
      "        [196.9508],\n",
      "        [140.5293]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9831687211990356 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009573366492986679\n",
      "Epoch: 450 \n",
      " Hypothesis: tensor([[152.3530],\n",
      "        [183.9861],\n",
      "        [180.8343],\n",
      "        [196.9508],\n",
      "        [140.5293]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9831225275993347 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009573505260050297\n",
      "Epoch: 451 \n",
      " Hypothesis: tensor([[152.3529],\n",
      "        [183.9861],\n",
      "        [180.8343],\n",
      "        [196.9508],\n",
      "        [140.5294]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9830878973007202 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009573644027113914\n",
      "Epoch: 452 \n",
      " Hypothesis: tensor([[152.3529],\n",
      "        [183.9861],\n",
      "        [180.8343],\n",
      "        [196.9508],\n",
      "        [140.5294]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9830701947212219 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009573782794177532\n",
      "Epoch: 453 \n",
      " Hypothesis: tensor([[152.3528],\n",
      "        [183.9861],\n",
      "        [180.8343],\n",
      "        [196.9508],\n",
      "        [140.5294]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9830271005630493 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957392156124115\n",
      "Epoch: 454 \n",
      " Hypothesis: tensor([[152.3528],\n",
      "        [183.9861],\n",
      "        [180.8343],\n",
      "        [196.9508],\n",
      "        [140.5294]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.982993483543396 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009574060328304768\n",
      "Epoch: 455 \n",
      " Hypothesis: tensor([[152.3528],\n",
      "        [183.9862],\n",
      "        [180.8343],\n",
      "        [196.9508],\n",
      "        [140.5295]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9829529523849487 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009574199095368385\n",
      "Epoch: 456 \n",
      " Hypothesis: tensor([[152.3527],\n",
      "        [183.9862],\n",
      "        [180.8343],\n",
      "        [196.9507],\n",
      "        [140.5295]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9829154014587402 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009574337862432003\n",
      "Epoch: 457 \n",
      " Hypothesis: tensor([[152.3527],\n",
      "        [183.9862],\n",
      "        [180.8343],\n",
      "        [196.9507],\n",
      "        [140.5295]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9828810691833496 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957447662949562\n",
      "Epoch: 458 \n",
      " Hypothesis: tensor([[152.3527],\n",
      "        [183.9862],\n",
      "        [180.8343],\n",
      "        [196.9507],\n",
      "        [140.5296]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9828397035598755 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009574615396559238\n",
      "Epoch: 459 \n",
      " Hypothesis: tensor([[152.3526],\n",
      "        [183.9863],\n",
      "        [180.8343],\n",
      "        [196.9507],\n",
      "        [140.5296]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9827951192855835 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6014],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009574754163622856\n",
      "Epoch: 460 \n",
      " Hypothesis: tensor([[152.3526],\n",
      "        [183.9863],\n",
      "        [180.8342],\n",
      "        [196.9507],\n",
      "        [140.5296]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9827637672424316 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009574892930686474\n",
      "Epoch: 461 \n",
      " Hypothesis: tensor([[152.3526],\n",
      "        [183.9863],\n",
      "        [180.8343],\n",
      "        [196.9507],\n",
      "        [140.5297]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9827332496643066 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009575031697750092\n",
      "Epoch: 462 \n",
      " Hypothesis: tensor([[152.3526],\n",
      "        [183.9863],\n",
      "        [180.8342],\n",
      "        [196.9507],\n",
      "        [140.5297]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9826924204826355 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957517046481371\n",
      "Epoch: 463 \n",
      " Hypothesis: tensor([[152.3525],\n",
      "        [183.9863],\n",
      "        [180.8342],\n",
      "        [196.9507],\n",
      "        [140.5297]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9826582074165344 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009575309231877327\n",
      "Epoch: 464 \n",
      " Hypothesis: tensor([[152.3525],\n",
      "        [183.9864],\n",
      "        [180.8342],\n",
      "        [196.9507],\n",
      "        [140.5298]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9826127886772156 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009575447998940945\n",
      "Epoch: 465 \n",
      " Hypothesis: tensor([[152.3525],\n",
      "        [183.9864],\n",
      "        [180.8342],\n",
      "        [196.9507],\n",
      "        [140.5298]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.982582688331604 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009575586766004562\n",
      "Epoch: 466 \n",
      " Hypothesis: tensor([[152.3524],\n",
      "        [183.9864],\n",
      "        [180.8342],\n",
      "        [196.9507],\n",
      "        [140.5298]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9825459718704224 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957572553306818\n",
      "Epoch: 467 \n",
      " Hypothesis: tensor([[152.3524],\n",
      "        [183.9864],\n",
      "        [180.8342],\n",
      "        [196.9506],\n",
      "        [140.5298]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9824954271316528 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009575864300131798\n",
      "Epoch: 468 \n",
      " Hypothesis: tensor([[152.3524],\n",
      "        [183.9864],\n",
      "        [180.8342],\n",
      "        [196.9506],\n",
      "        [140.5299]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9824632406234741 \n",
      " W: tensor([[0.7284],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009576003067195415\n",
      "Epoch: 469 \n",
      " Hypothesis: tensor([[152.3524],\n",
      "        [183.9865],\n",
      "        [180.8342],\n",
      "        [196.9506],\n",
      "        [140.5299]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.982434868812561 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009576141834259033\n",
      "Epoch: 470 \n",
      " Hypothesis: tensor([[152.3523],\n",
      "        [183.9865],\n",
      "        [180.8341],\n",
      "        [196.9506],\n",
      "        [140.5299]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9824030995368958 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009576280601322651\n",
      "Epoch: 471 \n",
      " Hypothesis: tensor([[152.3523],\n",
      "        [183.9865],\n",
      "        [180.8341],\n",
      "        [196.9506],\n",
      "        [140.5299]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9823683500289917 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009576419368386269\n",
      "Epoch: 472 \n",
      " Hypothesis: tensor([[152.3522],\n",
      "        [183.9865],\n",
      "        [180.8341],\n",
      "        [196.9506],\n",
      "        [140.5300]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9823232889175415 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009576558135449886\n",
      "Epoch: 473 \n",
      " Hypothesis: tensor([[152.3522],\n",
      "        [183.9865],\n",
      "        [180.8341],\n",
      "        [196.9506],\n",
      "        [140.5300]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9822948575019836 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009576696902513504\n",
      "Epoch: 474 \n",
      " Hypothesis: tensor([[152.3522],\n",
      "        [183.9866],\n",
      "        [180.8341],\n",
      "        [196.9506],\n",
      "        [140.5300]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9822403788566589 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009576835669577122\n",
      "Epoch: 475 \n",
      " Hypothesis: tensor([[152.3522],\n",
      "        [183.9866],\n",
      "        [180.8341],\n",
      "        [196.9506],\n",
      "        [140.5301]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9822151064872742 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957697443664074\n",
      "Epoch: 476 \n",
      " Hypothesis: tensor([[152.3521],\n",
      "        [183.9866],\n",
      "        [180.8341],\n",
      "        [196.9506],\n",
      "        [140.5301]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9821754693984985 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009577113203704357\n",
      "Epoch: 477 \n",
      " Hypothesis: tensor([[152.3521],\n",
      "        [183.9866],\n",
      "        [180.8341],\n",
      "        [196.9505],\n",
      "        [140.5301]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9821364283561707 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009577251970767975\n",
      "Epoch: 478 \n",
      " Hypothesis: tensor([[152.3521],\n",
      "        [183.9866],\n",
      "        [180.8341],\n",
      "        [196.9505],\n",
      "        [140.5302]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9821017980575562 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009577390737831593\n",
      "Epoch: 479 \n",
      " Hypothesis: tensor([[152.3520],\n",
      "        [183.9867],\n",
      "        [180.8341],\n",
      "        [196.9505],\n",
      "        [140.5302]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9820600748062134 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957752950489521\n",
      "Epoch: 480 \n",
      " Hypothesis: tensor([[152.3520],\n",
      "        [183.9867],\n",
      "        [180.8341],\n",
      "        [196.9505],\n",
      "        [140.5302]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9820316433906555 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009577668271958828\n",
      "Epoch: 481 \n",
      " Hypothesis: tensor([[152.3520],\n",
      "        [183.9867],\n",
      "        [180.8340],\n",
      "        [196.9505],\n",
      "        [140.5302]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9819982647895813 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009577807039022446\n",
      "Epoch: 482 \n",
      " Hypothesis: tensor([[152.3519],\n",
      "        [183.9867],\n",
      "        [180.8340],\n",
      "        [196.9505],\n",
      "        [140.5303]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9819467663764954 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6013],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009577945806086063\n",
      "Epoch: 483 \n",
      " Hypothesis: tensor([[152.3519],\n",
      "        [183.9867],\n",
      "        [180.8340],\n",
      "        [196.9505],\n",
      "        [140.5303]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9819133877754211 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009578084573149681\n",
      "Epoch: 484 \n",
      " Hypothesis: tensor([[152.3519],\n",
      "        [183.9868],\n",
      "        [180.8340],\n",
      "        [196.9505],\n",
      "        [140.5303]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9818849563598633 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009578223340213299\n",
      "Epoch: 485 \n",
      " Hypothesis: tensor([[152.3518],\n",
      "        [183.9868],\n",
      "        [180.8340],\n",
      "        [196.9505],\n",
      "        [140.5303]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9818425178527832 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009578362107276917\n",
      "Epoch: 486 \n",
      " Hypothesis: tensor([[152.3518],\n",
      "        [183.9868],\n",
      "        [180.8340],\n",
      "        [196.9505],\n",
      "        [140.5304]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9818152189254761 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009578500874340534\n",
      "Epoch: 487 \n",
      " Hypothesis: tensor([[152.3518],\n",
      "        [183.9868],\n",
      "        [180.8340],\n",
      "        [196.9505],\n",
      "        [140.5304]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9817806482315063 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009578639641404152\n",
      "Epoch: 488 \n",
      " Hypothesis: tensor([[152.3517],\n",
      "        [183.9868],\n",
      "        [180.8340],\n",
      "        [196.9505],\n",
      "        [140.5304]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.981735110282898 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957877840846777\n",
      "Epoch: 489 \n",
      " Hypothesis: tensor([[152.3517],\n",
      "        [183.9869],\n",
      "        [180.8340],\n",
      "        [196.9504],\n",
      "        [140.5305]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9817008972167969 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009578917175531387\n",
      "Epoch: 490 \n",
      " Hypothesis: tensor([[152.3517],\n",
      "        [183.9869],\n",
      "        [180.8340],\n",
      "        [196.9504],\n",
      "        [140.5305]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9816611409187317 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009579055942595005\n",
      "Epoch: 491 \n",
      " Hypothesis: tensor([[152.3517],\n",
      "        [183.9869],\n",
      "        [180.8339],\n",
      "        [196.9504],\n",
      "        [140.5305]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.981624960899353 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009579194709658623\n",
      "Epoch: 492 \n",
      " Hypothesis: tensor([[152.3516],\n",
      "        [183.9869],\n",
      "        [180.8339],\n",
      "        [196.9504],\n",
      "        [140.5305]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9815915822982788 \n",
      " W: tensor([[0.7285],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00957933347672224\n",
      "Epoch: 493 \n",
      " Hypothesis: tensor([[152.3516],\n",
      "        [183.9870],\n",
      "        [180.8339],\n",
      "        [196.9504],\n",
      "        [140.5306]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9815570712089539 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009579472243785858\n",
      "Epoch: 494 \n",
      " Hypothesis: tensor([[152.3516],\n",
      "        [183.9870],\n",
      "        [180.8339],\n",
      "        [196.9504],\n",
      "        [140.5306]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9815235137939453 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009579611010849476\n",
      "Epoch: 495 \n",
      " Hypothesis: tensor([[152.3515],\n",
      "        [183.9870],\n",
      "        [180.8339],\n",
      "        [196.9504],\n",
      "        [140.5306]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9814780354499817 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009579749777913094\n",
      "Epoch: 496 \n",
      " Hypothesis: tensor([[152.3515],\n",
      "        [183.9870],\n",
      "        [180.8339],\n",
      "        [196.9504],\n",
      "        [140.5307]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9814427495002747 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009579888544976711\n",
      "Epoch: 497 \n",
      " Hypothesis: tensor([[152.3515],\n",
      "        [183.9870],\n",
      "        [180.8339],\n",
      "        [196.9504],\n",
      "        [140.5307]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9814044237136841 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009580027312040329\n",
      "Epoch: 498 \n",
      " Hypothesis: tensor([[152.3514],\n",
      "        [183.9871],\n",
      "        [180.8339],\n",
      "        [196.9503],\n",
      "        [140.5307]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9813640713691711 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009580166079103947\n",
      "Epoch: 499 \n",
      " Hypothesis: tensor([[152.3514],\n",
      "        [183.9871],\n",
      "        [180.8339],\n",
      "        [196.9503],\n",
      "        [140.5308]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9813345670700073 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009580304846167564\n",
      "Epoch: 500 \n",
      " Hypothesis: tensor([[152.3514],\n",
      "        [183.9871],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5308]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9812971353530884 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009580443613231182\n",
      "Epoch: 501 \n",
      " Hypothesis: tensor([[152.3513],\n",
      "        [183.9871],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5308]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.981258749961853 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.0095805823802948\n",
      "Epoch: 502 \n",
      " Hypothesis: tensor([[152.3513],\n",
      "        [183.9871],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5308]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9812303781509399 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009580721147358418\n",
      "Epoch: 503 \n",
      " Hypothesis: tensor([[152.3513],\n",
      "        [183.9872],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5309]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9811906814575195 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009580859914422035\n",
      "Epoch: 504 \n",
      " Hypothesis: tensor([[152.3512],\n",
      "        [183.9872],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5309]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9811363220214844 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6012],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009580998681485653\n",
      "Epoch: 505 \n",
      " Hypothesis: tensor([[152.3512],\n",
      "        [183.9872],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5309]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9811021685600281 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958113744854927\n",
      "Epoch: 506 \n",
      " Hypothesis: tensor([[152.3512],\n",
      "        [183.9872],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5310]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9810624122619629 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009581276215612888\n",
      "Epoch: 507 \n",
      " Hypothesis: tensor([[152.3512],\n",
      "        [183.9872],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5310]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9810429811477661 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009581414982676506\n",
      "Epoch: 508 \n",
      " Hypothesis: tensor([[152.3511],\n",
      "        [183.9872],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5310]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9810019731521606 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009581553749740124\n",
      "Epoch: 509 \n",
      " Hypothesis: tensor([[152.3511],\n",
      "        [183.9873],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5311]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9809673428535461 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009581692516803741\n",
      "Epoch: 510 \n",
      " Hypothesis: tensor([[152.3511],\n",
      "        [183.9873],\n",
      "        [180.8338],\n",
      "        [196.9503],\n",
      "        [140.5311]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9809298515319824 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958183128386736\n",
      "Epoch: 511 \n",
      " Hypothesis: tensor([[152.3510],\n",
      "        [183.9873],\n",
      "        [180.8337],\n",
      "        [196.9502],\n",
      "        [140.5311]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9808915853500366 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009581970050930977\n",
      "Epoch: 512 \n",
      " Hypothesis: tensor([[152.3510],\n",
      "        [183.9873],\n",
      "        [180.8337],\n",
      "        [196.9502],\n",
      "        [140.5311]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9808581471443176 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009582108817994595\n",
      "Epoch: 513 \n",
      " Hypothesis: tensor([[152.3510],\n",
      "        [183.9874],\n",
      "        [180.8337],\n",
      "        [196.9502],\n",
      "        [140.5312]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.980812668800354 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009582247585058212\n",
      "Epoch: 514 \n",
      " Hypothesis: tensor([[152.3509],\n",
      "        [183.9874],\n",
      "        [180.8337],\n",
      "        [196.9502],\n",
      "        [140.5312]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9807783961296082 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958238635212183\n",
      "Epoch: 515 \n",
      " Hypothesis: tensor([[152.3509],\n",
      "        [183.9874],\n",
      "        [180.8337],\n",
      "        [196.9502],\n",
      "        [140.5312]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9807499647140503 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009582525119185448\n",
      "Epoch: 516 \n",
      " Hypothesis: tensor([[152.3509],\n",
      "        [183.9874],\n",
      "        [180.8337],\n",
      "        [196.9502],\n",
      "        [140.5312]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9807125329971313 \n",
      " W: tensor([[0.7286],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009582663886249065\n",
      "Epoch: 517 \n",
      " Hypothesis: tensor([[152.3508],\n",
      "        [183.9874],\n",
      "        [180.8337],\n",
      "        [196.9502],\n",
      "        [140.5313]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9806693196296692 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009582802653312683\n",
      "Epoch: 518 \n",
      " Hypothesis: tensor([[152.3508],\n",
      "        [183.9875],\n",
      "        [180.8337],\n",
      "        [196.9502],\n",
      "        [140.5313]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9806426763534546 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.0095829414203763\n",
      "Epoch: 519 \n",
      " Hypothesis: tensor([[152.3508],\n",
      "        [183.9875],\n",
      "        [180.8336],\n",
      "        [196.9502],\n",
      "        [140.5313]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9805921316146851 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009583080187439919\n",
      "Epoch: 520 \n",
      " Hypothesis: tensor([[152.3507],\n",
      "        [183.9875],\n",
      "        [180.8336],\n",
      "        [196.9501],\n",
      "        [140.5314]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9805580377578735 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009583218954503536\n",
      "Epoch: 521 \n",
      " Hypothesis: tensor([[152.3507],\n",
      "        [183.9875],\n",
      "        [180.8336],\n",
      "        [196.9502],\n",
      "        [140.5314]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9805353283882141 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009583357721567154\n",
      "Epoch: 522 \n",
      " Hypothesis: tensor([[152.3507],\n",
      "        [183.9875],\n",
      "        [180.8336],\n",
      "        [196.9501],\n",
      "        [140.5314]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.980484127998352 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009583496488630772\n",
      "Epoch: 523 \n",
      " Hypothesis: tensor([[152.3506],\n",
      "        [183.9876],\n",
      "        [180.8336],\n",
      "        [196.9501],\n",
      "        [140.5314]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9804534912109375 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958363525569439\n",
      "Epoch: 524 \n",
      " Hypothesis: tensor([[152.3506],\n",
      "        [183.9876],\n",
      "        [180.8336],\n",
      "        [196.9501],\n",
      "        [140.5315]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9804134368896484 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009583774022758007\n",
      "Epoch: 525 \n",
      " Hypothesis: tensor([[152.3506],\n",
      "        [183.9876],\n",
      "        [180.8336],\n",
      "        [196.9501],\n",
      "        [140.5315]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9803760647773743 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009583912789821625\n",
      "Epoch: 526 \n",
      " Hypothesis: tensor([[152.3506],\n",
      "        [183.9876],\n",
      "        [180.8336],\n",
      "        [196.9501],\n",
      "        [140.5316]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9803375005722046 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009584051556885242\n",
      "Epoch: 527 \n",
      " Hypothesis: tensor([[152.3505],\n",
      "        [183.9876],\n",
      "        [180.8336],\n",
      "        [196.9501],\n",
      "        [140.5316]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9803032875061035 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6011],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958419032394886\n",
      "Epoch: 528 \n",
      " Hypothesis: tensor([[152.3505],\n",
      "        [183.9877],\n",
      "        [180.8336],\n",
      "        [196.9501],\n",
      "        [140.5316]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9802669286727905 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009584329091012478\n",
      "Epoch: 529 \n",
      " Hypothesis: tensor([[152.3505],\n",
      "        [183.9877],\n",
      "        [180.8336],\n",
      "        [196.9501],\n",
      "        [140.5316]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.980233371257782 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009584467858076096\n",
      "Epoch: 530 \n",
      " Hypothesis: tensor([[152.3504],\n",
      "        [183.9877],\n",
      "        [180.8336],\n",
      "        [196.9501],\n",
      "        [140.5317]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.980198860168457 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009584606625139713\n",
      "Epoch: 531 \n",
      " Hypothesis: tensor([[152.3504],\n",
      "        [183.9877],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5317]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9801516532897949 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009584745392203331\n",
      "Epoch: 532 \n",
      " Hypothesis: tensor([[152.3504],\n",
      "        [183.9878],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5317]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9801192283630371 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009584884159266949\n",
      "Epoch: 533 \n",
      " Hypothesis: tensor([[152.3503],\n",
      "        [183.9878],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5317]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9800871014595032 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009585022926330566\n",
      "Epoch: 534 \n",
      " Hypothesis: tensor([[152.3503],\n",
      "        [183.9878],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5318]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9800583720207214 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009585161693394184\n",
      "Epoch: 535 \n",
      " Hypothesis: tensor([[152.3503],\n",
      "        [183.9878],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5318]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9800078272819519 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009585300460457802\n",
      "Epoch: 536 \n",
      " Hypothesis: tensor([[152.3502],\n",
      "        [183.9878],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5318]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9799736738204956 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958543922752142\n",
      "Epoch: 537 \n",
      " Hypothesis: tensor([[152.3502],\n",
      "        [183.9879],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5319]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9799333810806274 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009585577994585037\n",
      "Epoch: 538 \n",
      " Hypothesis: tensor([[152.3502],\n",
      "        [183.9879],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5319]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9798930883407593 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009585716761648655\n",
      "Epoch: 539 \n",
      " Hypothesis: tensor([[152.3501],\n",
      "        [183.9879],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5319]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9798663854598999 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009585855528712273\n",
      "Epoch: 540 \n",
      " Hypothesis: tensor([[152.3501],\n",
      "        [183.9879],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5320]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9798322916030884 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958599429577589\n",
      "Epoch: 541 \n",
      " Hypothesis: tensor([[152.3501],\n",
      "        [183.9879],\n",
      "        [180.8335],\n",
      "        [196.9500],\n",
      "        [140.5320]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9797868728637695 \n",
      " W: tensor([[0.7287],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009586133062839508\n",
      "Epoch: 542 \n",
      " Hypothesis: tensor([[152.3501],\n",
      "        [183.9880],\n",
      "        [180.8334],\n",
      "        [196.9500],\n",
      "        [140.5320]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9797533750534058 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009586271829903126\n",
      "Epoch: 543 \n",
      " Hypothesis: tensor([[152.3500],\n",
      "        [183.9880],\n",
      "        [180.8334],\n",
      "        [196.9500],\n",
      "        [140.5320]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9797250032424927 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009586410596966743\n",
      "Epoch: 544 \n",
      " Hypothesis: tensor([[152.3500],\n",
      "        [183.9880],\n",
      "        [180.8334],\n",
      "        [196.9500],\n",
      "        [140.5321]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9796883463859558 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009586549364030361\n",
      "Epoch: 545 \n",
      " Hypothesis: tensor([[152.3500],\n",
      "        [183.9880],\n",
      "        [180.8334],\n",
      "        [196.9499],\n",
      "        [140.5321]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9796493649482727 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009586688131093979\n",
      "Epoch: 546 \n",
      " Hypothesis: tensor([[152.3499],\n",
      "        [183.9881],\n",
      "        [180.8334],\n",
      "        [196.9499],\n",
      "        [140.5321]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9796057939529419 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009586826898157597\n",
      "Epoch: 547 \n",
      " Hypothesis: tensor([[152.3499],\n",
      "        [183.9881],\n",
      "        [180.8334],\n",
      "        [196.9499],\n",
      "        [140.5322]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9795585870742798 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009586965665221214\n",
      "Epoch: 548 \n",
      " Hypothesis: tensor([[152.3499],\n",
      "        [183.9881],\n",
      "        [180.8334],\n",
      "        [196.9499],\n",
      "        [140.5322]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9795252084732056 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009587104432284832\n",
      "Epoch: 549 \n",
      " Hypothesis: tensor([[152.3498],\n",
      "        [183.9881],\n",
      "        [180.8334],\n",
      "        [196.9499],\n",
      "        [140.5322]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9794937968254089 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958724319934845\n",
      "Epoch: 550 \n",
      " Hypothesis: tensor([[152.3498],\n",
      "        [183.9881],\n",
      "        [180.8334],\n",
      "        [196.9499],\n",
      "        [140.5322]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9794603586196899 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6010],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009587381966412067\n",
      "Epoch: 551 \n",
      " Hypothesis: tensor([[152.3498],\n",
      "        [183.9881],\n",
      "        [180.8333],\n",
      "        [196.9499],\n",
      "        [140.5323]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9794210195541382 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009587520733475685\n",
      "Epoch: 552 \n",
      " Hypothesis: tensor([[152.3497],\n",
      "        [183.9882],\n",
      "        [180.8333],\n",
      "        [196.9499],\n",
      "        [140.5323]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9793927073478699 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009587659500539303\n",
      "Epoch: 553 \n",
      " Hypothesis: tensor([[152.3497],\n",
      "        [183.9882],\n",
      "        [180.8333],\n",
      "        [196.9499],\n",
      "        [140.5323]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9793530702590942 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958779826760292\n",
      "Epoch: 554 \n",
      " Hypothesis: tensor([[152.3497],\n",
      "        [183.9882],\n",
      "        [180.8333],\n",
      "        [196.9498],\n",
      "        [140.5324]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9793127775192261 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009587937034666538\n",
      "Epoch: 555 \n",
      " Hypothesis: tensor([[152.3496],\n",
      "        [183.9882],\n",
      "        [180.8333],\n",
      "        [196.9498],\n",
      "        [140.5324]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9792734384536743 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009588075801730156\n",
      "Epoch: 556 \n",
      " Hypothesis: tensor([[152.3496],\n",
      "        [183.9883],\n",
      "        [180.8333],\n",
      "        [196.9498],\n",
      "        [140.5324]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9792451858520508 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009588214568793774\n",
      "Epoch: 557 \n",
      " Hypothesis: tensor([[152.3496],\n",
      "        [183.9883],\n",
      "        [180.8333],\n",
      "        [196.9498],\n",
      "        [140.5324]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9792090654373169 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009588353335857391\n",
      "Epoch: 558 \n",
      " Hypothesis: tensor([[152.3495],\n",
      "        [183.9883],\n",
      "        [180.8333],\n",
      "        [196.9498],\n",
      "        [140.5325]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.979169487953186 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009588492102921009\n",
      "Epoch: 559 \n",
      " Hypothesis: tensor([[152.3495],\n",
      "        [183.9883],\n",
      "        [180.8333],\n",
      "        [196.9498],\n",
      "        [140.5325]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9791299104690552 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009588630869984627\n",
      "Epoch: 560 \n",
      " Hypothesis: tensor([[152.3495],\n",
      "        [183.9883],\n",
      "        [180.8333],\n",
      "        [196.9498],\n",
      "        [140.5325]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9790924787521362 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009588769637048244\n",
      "Epoch: 561 \n",
      " Hypothesis: tensor([[152.3495],\n",
      "        [183.9883],\n",
      "        [180.8333],\n",
      "        [196.9498],\n",
      "        [140.5326]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9790626764297485 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009588908404111862\n",
      "Epoch: 562 \n",
      " Hypothesis: tensor([[152.3494],\n",
      "        [183.9884],\n",
      "        [180.8332],\n",
      "        [196.9498],\n",
      "        [140.5326]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9790230989456177 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958904717117548\n",
      "Epoch: 563 \n",
      " Hypothesis: tensor([[152.3494],\n",
      "        [183.9884],\n",
      "        [180.8332],\n",
      "        [196.9498],\n",
      "        [140.5326]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9789775609970093 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009589185938239098\n",
      "Epoch: 564 \n",
      " Hypothesis: tensor([[152.3494],\n",
      "        [183.9884],\n",
      "        [180.8332],\n",
      "        [196.9497],\n",
      "        [140.5327]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9789434671401978 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009589324705302715\n",
      "Epoch: 565 \n",
      " Hypothesis: tensor([[152.3493],\n",
      "        [183.9884],\n",
      "        [180.8332],\n",
      "        [196.9497],\n",
      "        [140.5327]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9789100885391235 \n",
      " W: tensor([[0.7288],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009589463472366333\n",
      "Epoch: 566 \n",
      " Hypothesis: tensor([[152.3493],\n",
      "        [183.9885],\n",
      "        [180.8332],\n",
      "        [196.9497],\n",
      "        [140.5327]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9788733720779419 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00958960223942995\n",
      "Epoch: 567 \n",
      " Hypothesis: tensor([[152.3493],\n",
      "        [183.9885],\n",
      "        [180.8332],\n",
      "        [196.9497],\n",
      "        [140.5327]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9788471460342407 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009589741006493568\n",
      "Epoch: 568 \n",
      " Hypothesis: tensor([[152.3492],\n",
      "        [183.9885],\n",
      "        [180.8332],\n",
      "        [196.9497],\n",
      "        [140.5328]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9788017272949219 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009589879773557186\n",
      "Epoch: 569 \n",
      " Hypothesis: tensor([[152.3492],\n",
      "        [183.9885],\n",
      "        [180.8332],\n",
      "        [196.9497],\n",
      "        [140.5328]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9787665605545044 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009590018540620804\n",
      "Epoch: 570 \n",
      " Hypothesis: tensor([[152.3492],\n",
      "        [183.9885],\n",
      "        [180.8332],\n",
      "        [196.9497],\n",
      "        [140.5328]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9787211418151855 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009590157307684422\n",
      "Epoch: 571 \n",
      " Hypothesis: tensor([[152.3492],\n",
      "        [183.9886],\n",
      "        [180.8332],\n",
      "        [196.9497],\n",
      "        [140.5329]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9786876440048218 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00959029607474804\n",
      "Epoch: 572 \n",
      " Hypothesis: tensor([[152.3491],\n",
      "        [183.9886],\n",
      "        [180.8331],\n",
      "        [196.9497],\n",
      "        [140.5329]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9786434173583984 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6009],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009590434841811657\n",
      "Epoch: 573 \n",
      " Hypothesis: tensor([[152.3491],\n",
      "        [183.9886],\n",
      "        [180.8331],\n",
      "        [196.9497],\n",
      "        [140.5329]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9786132574081421 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009590573608875275\n",
      "Epoch: 574 \n",
      " Hypothesis: tensor([[152.3491],\n",
      "        [183.9886],\n",
      "        [180.8331],\n",
      "        [196.9497],\n",
      "        [140.5330]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9785758256912231 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009590712375938892\n",
      "Epoch: 575 \n",
      " Hypothesis: tensor([[152.3490],\n",
      "        [183.9886],\n",
      "        [180.8331],\n",
      "        [196.9496],\n",
      "        [140.5330]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9785432815551758 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00959085114300251\n",
      "Epoch: 576 \n",
      " Hypothesis: tensor([[152.3490],\n",
      "        [183.9887],\n",
      "        [180.8331],\n",
      "        [196.9497],\n",
      "        [140.5330]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9785208702087402 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009590989910066128\n",
      "Epoch: 577 \n",
      " Hypothesis: tensor([[152.3490],\n",
      "        [183.9887],\n",
      "        [180.8331],\n",
      "        [196.9496],\n",
      "        [140.5330]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9784646034240723 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009591128677129745\n",
      "Epoch: 578 \n",
      " Hypothesis: tensor([[152.3489],\n",
      "        [183.9887],\n",
      "        [180.8331],\n",
      "        [196.9496],\n",
      "        [140.5331]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.978436291217804 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009591267444193363\n",
      "Epoch: 579 \n",
      " Hypothesis: tensor([[152.3489],\n",
      "        [183.9887],\n",
      "        [180.8331],\n",
      "        [196.9496],\n",
      "        [140.5331]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9783962368965149 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009591406211256981\n",
      "Epoch: 580 \n",
      " Hypothesis: tensor([[152.3489],\n",
      "        [183.9888],\n",
      "        [180.8331],\n",
      "        [196.9496],\n",
      "        [140.5331]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9783517122268677 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009591544978320599\n",
      "Epoch: 581 \n",
      " Hypothesis: tensor([[152.3488],\n",
      "        [183.9888],\n",
      "        [180.8331],\n",
      "        [196.9496],\n",
      "        [140.5332]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9783174395561218 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009591683745384216\n",
      "Epoch: 582 \n",
      " Hypothesis: tensor([[152.3488],\n",
      "        [183.9888],\n",
      "        [180.8331],\n",
      "        [196.9496],\n",
      "        [140.5332]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9782891273498535 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009591822512447834\n",
      "Epoch: 583 \n",
      " Hypothesis: tensor([[152.3488],\n",
      "        [183.9888],\n",
      "        [180.8330],\n",
      "        [196.9496],\n",
      "        [140.5332]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9782365560531616 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009591961279511452\n",
      "Epoch: 584 \n",
      " Hypothesis: tensor([[152.3487],\n",
      "        [183.9888],\n",
      "        [180.8330],\n",
      "        [196.9496],\n",
      "        [140.5332]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9782193303108215 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00959210004657507\n",
      "Epoch: 585 \n",
      " Hypothesis: tensor([[152.3487],\n",
      "        [183.9888],\n",
      "        [180.8330],\n",
      "        [196.9496],\n",
      "        [140.5333]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9781858325004578 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009592238813638687\n",
      "Epoch: 586 \n",
      " Hypothesis: tensor([[152.3487],\n",
      "        [183.9889],\n",
      "        [180.8330],\n",
      "        [196.9495],\n",
      "        [140.5333]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9781398773193359 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009592377580702305\n",
      "Epoch: 587 \n",
      " Hypothesis: tensor([[152.3486],\n",
      "        [183.9889],\n",
      "        [180.8330],\n",
      "        [196.9495],\n",
      "        [140.5333]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9781001806259155 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009592516347765923\n",
      "Epoch: 588 \n",
      " Hypothesis: tensor([[152.3486],\n",
      "        [183.9889],\n",
      "        [180.8330],\n",
      "        [196.9495],\n",
      "        [140.5334]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9780701398849487 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00959265511482954\n",
      "Epoch: 589 \n",
      " Hypothesis: tensor([[152.3486],\n",
      "        [183.9889],\n",
      "        [180.8330],\n",
      "        [196.9495],\n",
      "        [140.5334]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9780247807502747 \n",
      " W: tensor([[0.7289],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009592793881893158\n",
      "Epoch: 590 \n",
      " Hypothesis: tensor([[152.3486],\n",
      "        [183.9890],\n",
      "        [180.8330],\n",
      "        [196.9495],\n",
      "        [140.5334]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9780023694038391 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009592932648956776\n",
      "Epoch: 591 \n",
      " Hypothesis: tensor([[152.3485],\n",
      "        [183.9890],\n",
      "        [180.8330],\n",
      "        [196.9495],\n",
      "        [140.5334]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9779599905014038 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009593071416020393\n",
      "Epoch: 592 \n",
      " Hypothesis: tensor([[152.3485],\n",
      "        [183.9890],\n",
      "        [180.8329],\n",
      "        [196.9495],\n",
      "        [140.5335]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9779171943664551 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009593210183084011\n",
      "Epoch: 593 \n",
      " Hypothesis: tensor([[152.3484],\n",
      "        [183.9890],\n",
      "        [180.8329],\n",
      "        [196.9495],\n",
      "        [140.5335]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9778783917427063 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009593348950147629\n",
      "Epoch: 594 \n",
      " Hypothesis: tensor([[152.3484],\n",
      "        [183.9890],\n",
      "        [180.8329],\n",
      "        [196.9495],\n",
      "        [140.5335]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9778438806533813 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009593487717211246\n",
      "Epoch: 595 \n",
      " Hypothesis: tensor([[152.3484],\n",
      "        [183.9891],\n",
      "        [180.8329],\n",
      "        [196.9495],\n",
      "        [140.5336]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9778115153312683 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6008],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009593626484274864\n",
      "Epoch: 596 \n",
      " Hypothesis: tensor([[152.3483],\n",
      "        [183.9891],\n",
      "        [180.8329],\n",
      "        [196.9494],\n",
      "        [140.5336]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9777661561965942 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009593765251338482\n",
      "Epoch: 597 \n",
      " Hypothesis: tensor([[152.3483],\n",
      "        [183.9891],\n",
      "        [180.8329],\n",
      "        [196.9494],\n",
      "        [140.5336]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9777399897575378 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.0095939040184021\n",
      "Epoch: 598 \n",
      " Hypothesis: tensor([[152.3483],\n",
      "        [183.9891],\n",
      "        [180.8329],\n",
      "        [196.9494],\n",
      "        [140.5336]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9776957631111145 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009594042785465717\n",
      "Epoch: 599 \n",
      " Hypothesis: tensor([[152.3483],\n",
      "        [183.9892],\n",
      "        [180.8329],\n",
      "        [196.9494],\n",
      "        [140.5337]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9776533246040344 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009594181552529335\n",
      "Epoch: 600 \n",
      " Hypothesis: tensor([[152.3482],\n",
      "        [183.9892],\n",
      "        [180.8329],\n",
      "        [196.9494],\n",
      "        [140.5337]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9776191711425781 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009594320319592953\n",
      "Epoch: 601 \n",
      " Hypothesis: tensor([[152.3482],\n",
      "        [183.9892],\n",
      "        [180.8329],\n",
      "        [196.9494],\n",
      "        [140.5337]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9775916337966919 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00959445908665657\n",
      "Epoch: 602 \n",
      " Hypothesis: tensor([[152.3482],\n",
      "        [183.9892],\n",
      "        [180.8329],\n",
      "        [196.9494],\n",
      "        [140.5338]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9775513410568237 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009594597853720188\n",
      "Epoch: 603 \n",
      " Hypothesis: tensor([[152.3481],\n",
      "        [183.9892],\n",
      "        [180.8328],\n",
      "        [196.9494],\n",
      "        [140.5338]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9775120615959167 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009594736620783806\n",
      "Epoch: 604 \n",
      " Hypothesis: tensor([[152.3481],\n",
      "        [183.9892],\n",
      "        [180.8328],\n",
      "        [196.9494],\n",
      "        [140.5338]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9774729013442993 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009594875387847424\n",
      "Epoch: 605 \n",
      " Hypothesis: tensor([[152.3481],\n",
      "        [183.9893],\n",
      "        [180.8328],\n",
      "        [196.9494],\n",
      "        [140.5339]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.97742760181427 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009595014154911041\n",
      "Epoch: 606 \n",
      " Hypothesis: tensor([[152.3480],\n",
      "        [183.9893],\n",
      "        [180.8328],\n",
      "        [196.9494],\n",
      "        [140.5339]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.977408230304718 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009595152921974659\n",
      "Epoch: 607 \n",
      " Hypothesis: tensor([[152.3480],\n",
      "        [183.9893],\n",
      "        [180.8328],\n",
      "        [196.9494],\n",
      "        [140.5339]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9773748517036438 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009595291689038277\n",
      "Epoch: 608 \n",
      " Hypothesis: tensor([[152.3480],\n",
      "        [183.9893],\n",
      "        [180.8328],\n",
      "        [196.9494],\n",
      "        [140.5339]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9773403406143188 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009595430456101894\n",
      "Epoch: 609 \n",
      " Hypothesis: tensor([[152.3479],\n",
      "        [183.9893],\n",
      "        [180.8328],\n",
      "        [196.9493],\n",
      "        [140.5340]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9773011207580566 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009595569223165512\n",
      "Epoch: 610 \n",
      " Hypothesis: tensor([[152.3479],\n",
      "        [183.9894],\n",
      "        [180.8328],\n",
      "        [196.9493],\n",
      "        [140.5340]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9772518873214722 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.00959570799022913\n",
      "Epoch: 611 \n",
      " Hypothesis: tensor([[152.3479],\n",
      "        [183.9894],\n",
      "        [180.8328],\n",
      "        [196.9493],\n",
      "        [140.5340]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9772306680679321 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009595846757292747\n",
      "Epoch: 612 \n",
      " Hypothesis: tensor([[152.3479],\n",
      "        [183.9894],\n",
      "        [180.8327],\n",
      "        [196.9493],\n",
      "        [140.5340]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9771914482116699 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009595985524356365\n",
      "Epoch: 613 \n",
      " Hypothesis: tensor([[152.3478],\n",
      "        [183.9894],\n",
      "        [180.8327],\n",
      "        [196.9493],\n",
      "        [140.5341]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9771480560302734 \n",
      " W: tensor([[0.7290],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.009596124291419983\n",
      "Epoch: 614 \n",
      " Hypothesis: tensor([[152.3478],\n",
      "        [183.9895],\n",
      "        [180.8327],\n",
      "        [196.9493],\n",
      "        [140.5341]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9771037101745605 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6007],\n",
      "        [0.6808]], requires_grad=True) \n",
      " b: 0.0095962630584836\n",
      "Epoch: 615 \n",
      " Hypothesis: tensor([[152.3477],\n",
      "        [183.9895],\n",
      "        [180.8327],\n",
      "        [196.9493],\n",
      "        [140.5341]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9770764112472534 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6007],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009596401825547218\n",
      "Epoch: 616 \n",
      " Hypothesis: tensor([[152.3477],\n",
      "        [183.9895],\n",
      "        [180.8327],\n",
      "        [196.9493],\n",
      "        [140.5342]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9770560264587402 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6007],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009596540592610836\n",
      "Epoch: 617 \n",
      " Hypothesis: tensor([[152.3477],\n",
      "        [183.9895],\n",
      "        [180.8327],\n",
      "        [196.9493],\n",
      "        [140.5342]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9770107269287109 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6007],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009596679359674454\n",
      "Epoch: 618 \n",
      " Hypothesis: tensor([[152.3477],\n",
      "        [183.9895],\n",
      "        [180.8327],\n",
      "        [196.9492],\n",
      "        [140.5342]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9769614934921265 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6007],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009596818126738071\n",
      "Epoch: 619 \n",
      " Hypothesis: tensor([[152.3476],\n",
      "        [183.9896],\n",
      "        [180.8327],\n",
      "        [196.9492],\n",
      "        [140.5343]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9769312739372253 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00959695689380169\n",
      "Epoch: 620 \n",
      " Hypothesis: tensor([[152.3476],\n",
      "        [183.9896],\n",
      "        [180.8327],\n",
      "        [196.9492],\n",
      "        [140.5343]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9768928289413452 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009597095660865307\n",
      "Epoch: 621 \n",
      " Hypothesis: tensor([[152.3476],\n",
      "        [183.9896],\n",
      "        [180.8327],\n",
      "        [196.9492],\n",
      "        [140.5343]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9768633842468262 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009597234427928925\n",
      "Epoch: 622 \n",
      " Hypothesis: tensor([[152.3475],\n",
      "        [183.9896],\n",
      "        [180.8327],\n",
      "        [196.9492],\n",
      "        [140.5343]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.976830005645752 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009597373194992542\n",
      "Epoch: 623 \n",
      " Hypothesis: tensor([[152.3475],\n",
      "        [183.9896],\n",
      "        [180.8326],\n",
      "        [196.9492],\n",
      "        [140.5344]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9767829179763794 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00959751196205616\n",
      "Epoch: 624 \n",
      " Hypothesis: tensor([[152.3475],\n",
      "        [183.9897],\n",
      "        [180.8326],\n",
      "        [196.9492],\n",
      "        [140.5344]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9767505526542664 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009597650729119778\n",
      "Epoch: 625 \n",
      " Hypothesis: tensor([[152.3475],\n",
      "        [183.9897],\n",
      "        [180.8326],\n",
      "        [196.9492],\n",
      "        [140.5344]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9767175912857056 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009597789496183395\n",
      "Epoch: 626 \n",
      " Hypothesis: tensor([[152.3474],\n",
      "        [183.9897],\n",
      "        [180.8326],\n",
      "        [196.9492],\n",
      "        [140.5345]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9766827821731567 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009597928263247013\n",
      "Epoch: 627 \n",
      " Hypothesis: tensor([[152.3474],\n",
      "        [183.9897],\n",
      "        [180.8326],\n",
      "        [196.9492],\n",
      "        [140.5345]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9766327142715454 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00959806703031063\n",
      "Epoch: 628 \n",
      " Hypothesis: tensor([[152.3474],\n",
      "        [183.9897],\n",
      "        [180.8326],\n",
      "        [196.9492],\n",
      "        [140.5345]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9766043424606323 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009598205797374249\n",
      "Epoch: 629 \n",
      " Hypothesis: tensor([[152.3473],\n",
      "        [183.9898],\n",
      "        [180.8326],\n",
      "        [196.9492],\n",
      "        [140.5345]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9765709638595581 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009598344564437866\n",
      "Epoch: 630 \n",
      " Hypothesis: tensor([[152.3473],\n",
      "        [183.9898],\n",
      "        [180.8326],\n",
      "        [196.9491],\n",
      "        [140.5346]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9765318036079407 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009598483331501484\n",
      "Epoch: 631 \n",
      " Hypothesis: tensor([[152.3473],\n",
      "        [183.9898],\n",
      "        [180.8326],\n",
      "        [196.9491],\n",
      "        [140.5346]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9764944314956665 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009598622098565102\n",
      "Epoch: 632 \n",
      " Hypothesis: tensor([[152.3472],\n",
      "        [183.9898],\n",
      "        [180.8326],\n",
      "        [196.9491],\n",
      "        [140.5347]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9764512777328491 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00959876086562872\n",
      "Epoch: 633 \n",
      " Hypothesis: tensor([[152.3472],\n",
      "        [183.9899],\n",
      "        [180.8326],\n",
      "        [196.9491],\n",
      "        [140.5347]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9764121174812317 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009598899632692337\n",
      "Epoch: 634 \n",
      " Hypothesis: tensor([[152.3472],\n",
      "        [183.9899],\n",
      "        [180.8326],\n",
      "        [196.9491],\n",
      "        [140.5347]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9763776659965515 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009599038399755955\n",
      "Epoch: 635 \n",
      " Hypothesis: tensor([[152.3472],\n",
      "        [183.9899],\n",
      "        [180.8326],\n",
      "        [196.9491],\n",
      "        [140.5347]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9763584136962891 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009599177166819572\n",
      "Epoch: 636 \n",
      " Hypothesis: tensor([[152.3471],\n",
      "        [183.9899],\n",
      "        [180.8325],\n",
      "        [196.9491],\n",
      "        [140.5348]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9763191938400269 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00959931593388319\n",
      "Epoch: 637 \n",
      " Hypothesis: tensor([[152.3471],\n",
      "        [183.9899],\n",
      "        [180.8325],\n",
      "        [196.9491],\n",
      "        [140.5348]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9762789607048035 \n",
      " W: tensor([[0.7291],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009599454700946808\n",
      "Epoch: 638 \n",
      " Hypothesis: tensor([[152.3471],\n",
      "        [183.9900],\n",
      "        [180.8325],\n",
      "        [196.9491],\n",
      "        [140.5348]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9762393832206726 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009599593468010426\n",
      "Epoch: 639 \n",
      " Hypothesis: tensor([[152.3470],\n",
      "        [183.9900],\n",
      "        [180.8325],\n",
      "        [196.9491],\n",
      "        [140.5349]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9762060046195984 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009599732235074043\n",
      "Epoch: 640 \n",
      " Hypothesis: tensor([[152.3470],\n",
      "        [183.9900],\n",
      "        [180.8325],\n",
      "        [196.9491],\n",
      "        [140.5349]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9761776924133301 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009599871002137661\n",
      "Epoch: 641 \n",
      " Hypothesis: tensor([[152.3470],\n",
      "        [183.9900],\n",
      "        [180.8325],\n",
      "        [196.9491],\n",
      "        [140.5349]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9761323928833008 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6006],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009600009769201279\n",
      "Epoch: 642 \n",
      " Hypothesis: tensor([[152.3469],\n",
      "        [183.9901],\n",
      "        [180.8325],\n",
      "        [196.9491],\n",
      "        [140.5349]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9760932922363281 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009600148536264896\n",
      "Epoch: 643 \n",
      " Hypothesis: tensor([[152.3469],\n",
      "        [183.9901],\n",
      "        [180.8325],\n",
      "        [196.9491],\n",
      "        [140.5350]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9760677218437195 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009600287303328514\n",
      "Epoch: 644 \n",
      " Hypothesis: tensor([[152.3469],\n",
      "        [183.9901],\n",
      "        [180.8325],\n",
      "        [196.9491],\n",
      "        [140.5350]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9760344624519348 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009600426070392132\n",
      "Epoch: 645 \n",
      " Hypothesis: tensor([[152.3468],\n",
      "        [183.9901],\n",
      "        [180.8325],\n",
      "        [196.9490],\n",
      "        [140.5350]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9759852290153503 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00960056483745575\n",
      "Epoch: 646 \n",
      " Hypothesis: tensor([[152.3468],\n",
      "        [183.9901],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5351]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9759460687637329 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009600703604519367\n",
      "Epoch: 647 \n",
      " Hypothesis: tensor([[152.3468],\n",
      "        [183.9902],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5351]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9759178161621094 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009600842371582985\n",
      "Epoch: 648 \n",
      " Hypothesis: tensor([[152.3468],\n",
      "        [183.9902],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5351]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9758783578872681 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009600981138646603\n",
      "Epoch: 649 \n",
      " Hypothesis: tensor([[152.3467],\n",
      "        [183.9902],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5351]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9758527874946594 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00960111990571022\n",
      "Epoch: 650 \n",
      " Hypothesis: tensor([[152.3467],\n",
      "        [183.9902],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5352]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9758136868476868 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009601258672773838\n",
      "Epoch: 651 \n",
      " Hypothesis: tensor([[152.3467],\n",
      "        [183.9902],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5352]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.97577965259552 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009601397439837456\n",
      "Epoch: 652 \n",
      " Hypothesis: tensor([[152.3466],\n",
      "        [183.9903],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5352]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9757342338562012 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009601536206901073\n",
      "Epoch: 653 \n",
      " Hypothesis: tensor([[152.3466],\n",
      "        [183.9903],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5353]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9756951332092285 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009601674973964691\n",
      "Epoch: 654 \n",
      " Hypothesis: tensor([[152.3466],\n",
      "        [183.9903],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5353]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9756606221199036 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009601813741028309\n",
      "Epoch: 655 \n",
      " Hypothesis: tensor([[152.3466],\n",
      "        [183.9903],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5353]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9756344556808472 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009601952508091927\n",
      "Epoch: 656 \n",
      " Hypothesis: tensor([[152.3465],\n",
      "        [183.9904],\n",
      "        [180.8324],\n",
      "        [196.9490],\n",
      "        [140.5354]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9755989909172058 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009602091275155544\n",
      "Epoch: 657 \n",
      " Hypothesis: tensor([[152.3465],\n",
      "        [183.9904],\n",
      "        [180.8324],\n",
      "        [196.9489],\n",
      "        [140.5354]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9755598902702332 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009602230042219162\n",
      "Epoch: 658 \n",
      " Hypothesis: tensor([[152.3465],\n",
      "        [183.9904],\n",
      "        [180.8324],\n",
      "        [196.9489],\n",
      "        [140.5354]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9755217432975769 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00960236880928278\n",
      "Epoch: 659 \n",
      " Hypothesis: tensor([[152.3464],\n",
      "        [183.9904],\n",
      "        [180.8324],\n",
      "        [196.9489],\n",
      "        [140.5354]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9754794836044312 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009602507576346397\n",
      "Epoch: 660 \n",
      " Hypothesis: tensor([[152.3464],\n",
      "        [183.9905],\n",
      "        [180.8323],\n",
      "        [196.9489],\n",
      "        [140.5355]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9754341244697571 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009602646343410015\n",
      "Epoch: 661 \n",
      " Hypothesis: tensor([[152.3464],\n",
      "        [183.9905],\n",
      "        [180.8323],\n",
      "        [196.9489],\n",
      "        [140.5355]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9754091501235962 \n",
      " W: tensor([[0.7292],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009602785110473633\n",
      "Epoch: 662 \n",
      " Hypothesis: tensor([[152.3463],\n",
      "        [183.9905],\n",
      "        [180.8323],\n",
      "        [196.9489],\n",
      "        [140.5355]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9753695726394653 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00960292387753725\n",
      "Epoch: 663 \n",
      " Hypothesis: tensor([[152.3463],\n",
      "        [183.9905],\n",
      "        [180.8323],\n",
      "        [196.9489],\n",
      "        [140.5356]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9753341674804688 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009603062644600868\n",
      "Epoch: 664 \n",
      " Hypothesis: tensor([[152.3463],\n",
      "        [183.9905],\n",
      "        [180.8323],\n",
      "        [196.9489],\n",
      "        [140.5356]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9753021001815796 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6005],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009603201411664486\n",
      "Epoch: 665 \n",
      " Hypothesis: tensor([[152.3463],\n",
      "        [183.9906],\n",
      "        [180.8323],\n",
      "        [196.9489],\n",
      "        [140.5356]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9752618670463562 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009603340178728104\n",
      "Epoch: 666 \n",
      " Hypothesis: tensor([[152.3462],\n",
      "        [183.9906],\n",
      "        [180.8323],\n",
      "        [196.9489],\n",
      "        [140.5357]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9752265214920044 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009603478945791721\n",
      "Epoch: 667 \n",
      " Hypothesis: tensor([[152.3462],\n",
      "        [183.9906],\n",
      "        [180.8323],\n",
      "        [196.9489],\n",
      "        [140.5357]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9751941561698914 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009603617712855339\n",
      "Epoch: 668 \n",
      " Hypothesis: tensor([[152.3462],\n",
      "        [183.9906],\n",
      "        [180.8323],\n",
      "        [196.9489],\n",
      "        [140.5357]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9751550555229187 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009603756479918957\n",
      "Epoch: 669 \n",
      " Hypothesis: tensor([[152.3461],\n",
      "        [183.9906],\n",
      "        [180.8323],\n",
      "        [196.9488],\n",
      "        [140.5358]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9751148223876953 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009603895246982574\n",
      "Epoch: 670 \n",
      " Hypothesis: tensor([[152.3461],\n",
      "        [183.9907],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5358]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9750814437866211 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009604034014046192\n",
      "Epoch: 671 \n",
      " Hypothesis: tensor([[152.3461],\n",
      "        [183.9907],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5358]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9750481843948364 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00960417278110981\n",
      "Epoch: 672 \n",
      " Hypothesis: tensor([[152.3460],\n",
      "        [183.9907],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5358]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9750205874443054 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009604311548173428\n",
      "Epoch: 673 \n",
      " Hypothesis: tensor([[152.3460],\n",
      "        [183.9907],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5359]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9749835133552551 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009604450315237045\n",
      "Epoch: 674 \n",
      " Hypothesis: tensor([[152.3460],\n",
      "        [183.9908],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5359]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9749293327331543 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009604589082300663\n",
      "Epoch: 675 \n",
      " Hypothesis: tensor([[152.3459],\n",
      "        [183.9908],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5359]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9748952984809875 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00960472784936428\n",
      "Epoch: 676 \n",
      " Hypothesis: tensor([[152.3459],\n",
      "        [183.9908],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5359]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9748705625534058 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009604866616427898\n",
      "Epoch: 677 \n",
      " Hypothesis: tensor([[152.3459],\n",
      "        [183.9908],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5360]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9748385548591614 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009605005383491516\n",
      "Epoch: 678 \n",
      " Hypothesis: tensor([[152.3459],\n",
      "        [183.9908],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5360]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9747994542121887 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009605144150555134\n",
      "Epoch: 679 \n",
      " Hypothesis: tensor([[152.3458],\n",
      "        [183.9909],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5360]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9747629165649414 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009605282917618752\n",
      "Epoch: 680 \n",
      " Hypothesis: tensor([[152.3458],\n",
      "        [183.9909],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5361]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9747227430343628 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00960542168468237\n",
      "Epoch: 681 \n",
      " Hypothesis: tensor([[152.3458],\n",
      "        [183.9909],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5361]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9746814966201782 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009605560451745987\n",
      "Epoch: 682 \n",
      " Hypothesis: tensor([[152.3457],\n",
      "        [183.9909],\n",
      "        [180.8322],\n",
      "        [196.9488],\n",
      "        [140.5361]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9746481776237488 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009605699218809605\n",
      "Epoch: 683 \n",
      " Hypothesis: tensor([[152.3457],\n",
      "        [183.9910],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5362]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9746038317680359 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009605837985873222\n",
      "Epoch: 684 \n",
      " Hypothesis: tensor([[152.3457],\n",
      "        [183.9910],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5362]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.974578857421875 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00960597675293684\n",
      "Epoch: 685 \n",
      " Hypothesis: tensor([[152.3457],\n",
      "        [183.9910],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5362]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9745445251464844 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009606115520000458\n",
      "Epoch: 686 \n",
      " Hypothesis: tensor([[152.3456],\n",
      "        [183.9910],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5363]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9745111465454102 \n",
      " W: tensor([[0.7293],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009606254287064075\n",
      "Epoch: 687 \n",
      " Hypothesis: tensor([[152.3456],\n",
      "        [183.9910],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5363]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9744709134101868 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6004],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009606393054127693\n",
      "Epoch: 688 \n",
      " Hypothesis: tensor([[152.3456],\n",
      "        [183.9911],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5363]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9744318723678589 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009606531821191311\n",
      "Epoch: 689 \n",
      " Hypothesis: tensor([[152.3455],\n",
      "        [183.9911],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5363]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9743974804878235 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009606670588254929\n",
      "Epoch: 690 \n",
      " Hypothesis: tensor([[152.3455],\n",
      "        [183.9911],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5364]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9743641018867493 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009606809355318546\n",
      "Epoch: 691 \n",
      " Hypothesis: tensor([[152.3455],\n",
      "        [183.9911],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5364]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9743250012397766 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009606948122382164\n",
      "Epoch: 692 \n",
      " Hypothesis: tensor([[152.3454],\n",
      "        [183.9911],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5364]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.974284827709198 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009607086889445782\n",
      "Epoch: 693 \n",
      " Hypothesis: tensor([[152.3454],\n",
      "        [183.9912],\n",
      "        [180.8321],\n",
      "        [196.9487],\n",
      "        [140.5365]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9742504358291626 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.0096072256565094\n",
      "Epoch: 694 \n",
      " Hypothesis: tensor([[152.3454],\n",
      "        [183.9912],\n",
      "        [180.8320],\n",
      "        [196.9487],\n",
      "        [140.5365]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9742113351821899 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009607364423573017\n",
      "Epoch: 695 \n",
      " Hypothesis: tensor([[152.3454],\n",
      "        [183.9912],\n",
      "        [180.8320],\n",
      "        [196.9486],\n",
      "        [140.5365]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9741743206977844 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009607503190636635\n",
      "Epoch: 696 \n",
      " Hypothesis: tensor([[152.3453],\n",
      "        [183.9912],\n",
      "        [180.8320],\n",
      "        [196.9486],\n",
      "        [140.5366]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9741377830505371 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009607641957700253\n",
      "Epoch: 697 \n",
      " Hypothesis: tensor([[152.3453],\n",
      "        [183.9913],\n",
      "        [180.8320],\n",
      "        [196.9486],\n",
      "        [140.5366]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9741095304489136 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00960778072476387\n",
      "Epoch: 698 \n",
      " Hypothesis: tensor([[152.3453],\n",
      "        [183.9913],\n",
      "        [180.8320],\n",
      "        [196.9486],\n",
      "        [140.5366]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9740644693374634 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009607919491827488\n",
      "Epoch: 699 \n",
      " Hypothesis: tensor([[152.3452],\n",
      "        [183.9913],\n",
      "        [180.8320],\n",
      "        [196.9486],\n",
      "        [140.5367]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9740303158760071 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009608058258891106\n",
      "Epoch: 700 \n",
      " Hypothesis: tensor([[152.3452],\n",
      "        [183.9913],\n",
      "        [180.8320],\n",
      "        [196.9486],\n",
      "        [140.5367]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9740018844604492 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009608197025954723\n",
      "Epoch: 701 \n",
      " Hypothesis: tensor([[152.3452],\n",
      "        [183.9913],\n",
      "        [180.8320],\n",
      "        [196.9486],\n",
      "        [140.5367]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9739646911621094 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009608335793018341\n",
      "Epoch: 702 \n",
      " Hypothesis: tensor([[152.3452],\n",
      "        [183.9914],\n",
      "        [180.8320],\n",
      "        [196.9486],\n",
      "        [140.5367]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9739197492599487 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009608474560081959\n",
      "Epoch: 703 \n",
      " Hypothesis: tensor([[152.3451],\n",
      "        [183.9914],\n",
      "        [180.8320],\n",
      "        [196.9486],\n",
      "        [140.5368]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9738892316818237 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009608613327145576\n",
      "Epoch: 704 \n",
      " Hypothesis: tensor([[152.3451],\n",
      "        [183.9914],\n",
      "        [180.8320],\n",
      "        [196.9486],\n",
      "        [140.5368]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9738559722900391 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009608752094209194\n",
      "Epoch: 705 \n",
      " Hypothesis: tensor([[152.3451],\n",
      "        [183.9914],\n",
      "        [180.8319],\n",
      "        [196.9486],\n",
      "        [140.5368]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9738165140151978 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009608890861272812\n",
      "Epoch: 706 \n",
      " Hypothesis: tensor([[152.3450],\n",
      "        [183.9915],\n",
      "        [180.8319],\n",
      "        [196.9486],\n",
      "        [140.5369]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9737793207168579 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00960902962833643\n",
      "Epoch: 707 \n",
      " Hypothesis: tensor([[152.3450],\n",
      "        [183.9915],\n",
      "        [180.8319],\n",
      "        [196.9485],\n",
      "        [140.5369]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.973748505115509 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009609168395400047\n",
      "Epoch: 708 \n",
      " Hypothesis: tensor([[152.3450],\n",
      "        [183.9915],\n",
      "        [180.8319],\n",
      "        [196.9485],\n",
      "        [140.5369]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.973708987236023 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009609307162463665\n",
      "Epoch: 709 \n",
      " Hypothesis: tensor([[152.3449],\n",
      "        [183.9915],\n",
      "        [180.8319],\n",
      "        [196.9485],\n",
      "        [140.5369]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9736749529838562 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009609445929527283\n",
      "Epoch: 710 \n",
      " Hypothesis: tensor([[152.3449],\n",
      "        [183.9915],\n",
      "        [180.8319],\n",
      "        [196.9485],\n",
      "        [140.5370]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9736416935920715 \n",
      " W: tensor([[0.7294],\n",
      "        [0.6003],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.0096095846965909\n",
      "Epoch: 711 \n",
      " Hypothesis: tensor([[152.3449],\n",
      "        [183.9915],\n",
      "        [180.8319],\n",
      "        [196.9485],\n",
      "        [140.5370]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9736076593399048 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009609723463654518\n",
      "Epoch: 712 \n",
      " Hypothesis: tensor([[152.3448],\n",
      "        [183.9916],\n",
      "        [180.8319],\n",
      "        [196.9485],\n",
      "        [140.5370]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9735620617866516 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009609862230718136\n",
      "Epoch: 713 \n",
      " Hypothesis: tensor([[152.3448],\n",
      "        [183.9916],\n",
      "        [180.8319],\n",
      "        [196.9485],\n",
      "        [140.5371]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9735287427902222 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009610000997781754\n",
      "Epoch: 714 \n",
      " Hypothesis: tensor([[152.3448],\n",
      "        [183.9916],\n",
      "        [180.8319],\n",
      "        [196.9485],\n",
      "        [140.5371]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9734886288642883 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009610139764845371\n",
      "Epoch: 715 \n",
      " Hypothesis: tensor([[152.3448],\n",
      "        [183.9917],\n",
      "        [180.8319],\n",
      "        [196.9485],\n",
      "        [140.5371]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9734495282173157 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009610278531908989\n",
      "Epoch: 716 \n",
      " Hypothesis: tensor([[152.3447],\n",
      "        [183.9917],\n",
      "        [180.8318],\n",
      "        [196.9485],\n",
      "        [140.5371]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9734252095222473 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009610417298972607\n",
      "Epoch: 717 \n",
      " Hypothesis: tensor([[152.3447],\n",
      "        [183.9917],\n",
      "        [180.8318],\n",
      "        [196.9485],\n",
      "        [140.5372]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9733969569206238 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009610556066036224\n",
      "Epoch: 718 \n",
      " Hypothesis: tensor([[152.3447],\n",
      "        [183.9917],\n",
      "        [180.8318],\n",
      "        [196.9485],\n",
      "        [140.5372]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9733574986457825 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009610694833099842\n",
      "Epoch: 719 \n",
      " Hypothesis: tensor([[152.3446],\n",
      "        [183.9917],\n",
      "        [180.8318],\n",
      "        [196.9485],\n",
      "        [140.5372]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9733234643936157 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961083360016346\n",
      "Epoch: 720 \n",
      " Hypothesis: tensor([[152.3446],\n",
      "        [183.9918],\n",
      "        [180.8318],\n",
      "        [196.9485],\n",
      "        [140.5373]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9732692837715149 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009610972367227077\n",
      "Epoch: 721 \n",
      " Hypothesis: tensor([[152.3446],\n",
      "        [183.9918],\n",
      "        [180.8318],\n",
      "        [196.9485],\n",
      "        [140.5373]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9732500314712524 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009611111134290695\n",
      "Epoch: 722 \n",
      " Hypothesis: tensor([[152.3445],\n",
      "        [183.9918],\n",
      "        [180.8318],\n",
      "        [196.9485],\n",
      "        [140.5373]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9732217788696289 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009611249901354313\n",
      "Epoch: 723 \n",
      " Hypothesis: tensor([[152.3445],\n",
      "        [183.9918],\n",
      "        [180.8318],\n",
      "        [196.9484],\n",
      "        [140.5374]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9731707572937012 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961138866841793\n",
      "Epoch: 724 \n",
      " Hypothesis: tensor([[152.3445],\n",
      "        [183.9919],\n",
      "        [180.8318],\n",
      "        [196.9484],\n",
      "        [140.5374]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9731312990188599 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009611527435481548\n",
      "Epoch: 725 \n",
      " Hypothesis: tensor([[152.3445],\n",
      "        [183.9919],\n",
      "        [180.8318],\n",
      "        [196.9484],\n",
      "        [140.5374]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9730973243713379 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009611666202545166\n",
      "Epoch: 726 \n",
      " Hypothesis: tensor([[152.3444],\n",
      "        [183.9919],\n",
      "        [180.8318],\n",
      "        [196.9484],\n",
      "        [140.5374]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.973064124584198 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009611804969608784\n",
      "Epoch: 727 \n",
      " Hypothesis: tensor([[152.3444],\n",
      "        [183.9919],\n",
      "        [180.8318],\n",
      "        [196.9484],\n",
      "        [140.5375]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9730240106582642 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009611943736672401\n",
      "Epoch: 728 \n",
      " Hypothesis: tensor([[152.3444],\n",
      "        [183.9919],\n",
      "        [180.8318],\n",
      "        [196.9484],\n",
      "        [140.5375]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9729906320571899 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961208250373602\n",
      "Epoch: 729 \n",
      " Hypothesis: tensor([[152.3443],\n",
      "        [183.9920],\n",
      "        [180.8318],\n",
      "        [196.9484],\n",
      "        [140.5375]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9729561805725098 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009612221270799637\n",
      "Epoch: 730 \n",
      " Hypothesis: tensor([[152.3443],\n",
      "        [183.9920],\n",
      "        [180.8317],\n",
      "        [196.9484],\n",
      "        [140.5376]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9729171991348267 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009612360037863255\n",
      "Epoch: 731 \n",
      " Hypothesis: tensor([[152.3443],\n",
      "        [183.9920],\n",
      "        [180.8317],\n",
      "        [196.9484],\n",
      "        [140.5376]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9728791117668152 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009612498804926872\n",
      "Epoch: 732 \n",
      " Hypothesis: tensor([[152.3443],\n",
      "        [183.9920],\n",
      "        [180.8317],\n",
      "        [196.9484],\n",
      "        [140.5376]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9728458523750305 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961263757199049\n",
      "Epoch: 733 \n",
      " Hypothesis: tensor([[152.3442],\n",
      "        [183.9920],\n",
      "        [180.8317],\n",
      "        [196.9484],\n",
      "        [140.5377]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9728155136108398 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6002],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009612776339054108\n",
      "Epoch: 734 \n",
      " Hypothesis: tensor([[152.3442],\n",
      "        [183.9921],\n",
      "        [180.8317],\n",
      "        [196.9483],\n",
      "        [140.5377]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9727646112442017 \n",
      " W: tensor([[0.7295],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009612915106117725\n",
      "Epoch: 735 \n",
      " Hypothesis: tensor([[152.3441],\n",
      "        [183.9921],\n",
      "        [180.8317],\n",
      "        [196.9483],\n",
      "        [140.5377]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9727250933647156 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009613053873181343\n",
      "Epoch: 736 \n",
      " Hypothesis: tensor([[152.3441],\n",
      "        [183.9921],\n",
      "        [180.8317],\n",
      "        [196.9483],\n",
      "        [140.5378]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9726969003677368 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961319264024496\n",
      "Epoch: 737 \n",
      " Hypothesis: tensor([[152.3441],\n",
      "        [183.9921],\n",
      "        [180.8317],\n",
      "        [196.9483],\n",
      "        [140.5378]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9726688265800476 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009613331407308578\n",
      "Epoch: 738 \n",
      " Hypothesis: tensor([[152.3441],\n",
      "        [183.9922],\n",
      "        [180.8317],\n",
      "        [196.9483],\n",
      "        [140.5378]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9726147651672363 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009613470174372196\n",
      "Epoch: 739 \n",
      " Hypothesis: tensor([[152.3440],\n",
      "        [183.9922],\n",
      "        [180.8317],\n",
      "        [196.9483],\n",
      "        [140.5378]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9725865125656128 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009613608941435814\n",
      "Epoch: 740 \n",
      " Hypothesis: tensor([[152.3440],\n",
      "        [183.9922],\n",
      "        [180.8316],\n",
      "        [196.9483],\n",
      "        [140.5379]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9725532531738281 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009613747708499432\n",
      "Epoch: 741 \n",
      " Hypothesis: tensor([[152.3440],\n",
      "        [183.9922],\n",
      "        [180.8316],\n",
      "        [196.9483],\n",
      "        [140.5379]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9725109934806824 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961388647556305\n",
      "Epoch: 742 \n",
      " Hypothesis: tensor([[152.3439],\n",
      "        [183.9922],\n",
      "        [180.8316],\n",
      "        [196.9483],\n",
      "        [140.5379]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9724715948104858 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009614025242626667\n",
      "Epoch: 743 \n",
      " Hypothesis: tensor([[152.3439],\n",
      "        [183.9923],\n",
      "        [180.8316],\n",
      "        [196.9483],\n",
      "        [140.5379]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9724486470222473 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009614164009690285\n",
      "Epoch: 744 \n",
      " Hypothesis: tensor([[152.3439],\n",
      "        [183.9923],\n",
      "        [180.8316],\n",
      "        [196.9483],\n",
      "        [140.5380]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9724203944206238 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009614302776753902\n",
      "Epoch: 745 \n",
      " Hypothesis: tensor([[152.3439],\n",
      "        [183.9923],\n",
      "        [180.8316],\n",
      "        [196.9483],\n",
      "        [140.5380]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9723809957504272 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961444154381752\n",
      "Epoch: 746 \n",
      " Hypothesis: tensor([[152.3438],\n",
      "        [183.9923],\n",
      "        [180.8316],\n",
      "        [196.9483],\n",
      "        [140.5380]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9723418951034546 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009614580310881138\n",
      "Epoch: 747 \n",
      " Hypothesis: tensor([[152.3438],\n",
      "        [183.9923],\n",
      "        [180.8316],\n",
      "        [196.9482],\n",
      "        [140.5381]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9723027944564819 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009614719077944756\n",
      "Epoch: 748 \n",
      " Hypothesis: tensor([[152.3438],\n",
      "        [183.9924],\n",
      "        [180.8316],\n",
      "        [196.9482],\n",
      "        [140.5381]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9722696542739868 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009614857845008373\n",
      "Epoch: 749 \n",
      " Hypothesis: tensor([[152.3437],\n",
      "        [183.9924],\n",
      "        [180.8316],\n",
      "        [196.9482],\n",
      "        [140.5381]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9722263216972351 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009614996612071991\n",
      "Epoch: 750 \n",
      " Hypothesis: tensor([[152.3437],\n",
      "        [183.9924],\n",
      "        [180.8316],\n",
      "        [196.9482],\n",
      "        [140.5381]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9721932411193848 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009615135379135609\n",
      "Epoch: 751 \n",
      " Hypothesis: tensor([[152.3437],\n",
      "        [183.9924],\n",
      "        [180.8316],\n",
      "        [196.9482],\n",
      "        [140.5382]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9721649885177612 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009615274146199226\n",
      "Epoch: 752 \n",
      " Hypothesis: tensor([[152.3436],\n",
      "        [183.9925],\n",
      "        [180.8316],\n",
      "        [196.9482],\n",
      "        [140.5382]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9721218347549438 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009615412913262844\n",
      "Epoch: 753 \n",
      " Hypothesis: tensor([[152.3436],\n",
      "        [183.9925],\n",
      "        [180.8315],\n",
      "        [196.9482],\n",
      "        [140.5383]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9720908999443054 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009615551680326462\n",
      "Epoch: 754 \n",
      " Hypothesis: tensor([[152.3436],\n",
      "        [183.9925],\n",
      "        [180.8315],\n",
      "        [196.9482],\n",
      "        [140.5383]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9720603227615356 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961569044739008\n",
      "Epoch: 755 \n",
      " Hypothesis: tensor([[152.3436],\n",
      "        [183.9925],\n",
      "        [180.8315],\n",
      "        [196.9482],\n",
      "        [140.5383]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9720093011856079 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009615829214453697\n",
      "Epoch: 756 \n",
      " Hypothesis: tensor([[152.3435],\n",
      "        [183.9925],\n",
      "        [180.8315],\n",
      "        [196.9482],\n",
      "        [140.5383]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9719831347465515 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6001],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009615967981517315\n",
      "Epoch: 757 \n",
      " Hypothesis: tensor([[152.3435],\n",
      "        [183.9926],\n",
      "        [180.8315],\n",
      "        [196.9482],\n",
      "        [140.5384]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9719449281692505 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009616106748580933\n",
      "Epoch: 758 \n",
      " Hypothesis: tensor([[152.3435],\n",
      "        [183.9926],\n",
      "        [180.8315],\n",
      "        [196.9482],\n",
      "        [140.5384]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.971916675567627 \n",
      " W: tensor([[0.7296],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961624551564455\n",
      "Epoch: 759 \n",
      " Hypothesis: tensor([[152.3434],\n",
      "        [183.9926],\n",
      "        [180.8315],\n",
      "        [196.9482],\n",
      "        [140.5384]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.971879780292511 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009616384282708168\n",
      "Epoch: 760 \n",
      " Hypothesis: tensor([[152.3434],\n",
      "        [183.9926],\n",
      "        [180.8315],\n",
      "        [196.9482],\n",
      "        [140.5385]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9718402624130249 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009616523049771786\n",
      "Epoch: 761 \n",
      " Hypothesis: tensor([[152.3434],\n",
      "        [183.9926],\n",
      "        [180.8315],\n",
      "        [196.9482],\n",
      "        [140.5385]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9718121290206909 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009616661816835403\n",
      "Epoch: 762 \n",
      " Hypothesis: tensor([[152.3434],\n",
      "        [183.9927],\n",
      "        [180.8315],\n",
      "        [196.9481],\n",
      "        [140.5385]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9717580676078796 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009616800583899021\n",
      "Epoch: 763 \n",
      " Hypothesis: tensor([[152.3433],\n",
      "        [183.9927],\n",
      "        [180.8315],\n",
      "        [196.9481],\n",
      "        [140.5385]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9717308878898621 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009616939350962639\n",
      "Epoch: 764 \n",
      " Hypothesis: tensor([[152.3433],\n",
      "        [183.9927],\n",
      "        [180.8315],\n",
      "        [196.9481],\n",
      "        [140.5386]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9716939926147461 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009617078118026257\n",
      "Epoch: 765 \n",
      " Hypothesis: tensor([[152.3433],\n",
      "        [183.9927],\n",
      "        [180.8315],\n",
      "        [196.9481],\n",
      "        [140.5386]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9716538190841675 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009617216885089874\n",
      "Epoch: 766 \n",
      " Hypothesis: tensor([[152.3432],\n",
      "        [183.9928],\n",
      "        [180.8314],\n",
      "        [196.9481],\n",
      "        [140.5386]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9716144800186157 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009617355652153492\n",
      "Epoch: 767 \n",
      " Hypothesis: tensor([[152.3432],\n",
      "        [183.9928],\n",
      "        [180.8314],\n",
      "        [196.9481],\n",
      "        [140.5387]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9715811610221863 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961749441921711\n",
      "Epoch: 768 \n",
      " Hypothesis: tensor([[152.3432],\n",
      "        [183.9928],\n",
      "        [180.8314],\n",
      "        [196.9481],\n",
      "        [140.5387]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9715530276298523 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009617633186280727\n",
      "Epoch: 769 \n",
      " Hypothesis: tensor([[152.3431],\n",
      "        [183.9928],\n",
      "        [180.8314],\n",
      "        [196.9481],\n",
      "        [140.5387]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9715078473091125 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009617771953344345\n",
      "Epoch: 770 \n",
      " Hypothesis: tensor([[152.3431],\n",
      "        [183.9928],\n",
      "        [180.8314],\n",
      "        [196.9481],\n",
      "        [140.5388]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9714688062667847 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009617910720407963\n",
      "Epoch: 771 \n",
      " Hypothesis: tensor([[152.3431],\n",
      "        [183.9929],\n",
      "        [180.8314],\n",
      "        [196.9481],\n",
      "        [140.5388]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9714406132698059 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961804948747158\n",
      "Epoch: 772 \n",
      " Hypothesis: tensor([[152.3430],\n",
      "        [183.9929],\n",
      "        [180.8314],\n",
      "        [196.9481],\n",
      "        [140.5388]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9714062809944153 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009618188254535198\n",
      "Epoch: 773 \n",
      " Hypothesis: tensor([[152.3430],\n",
      "        [183.9929],\n",
      "        [180.8314],\n",
      "        [196.9480],\n",
      "        [140.5388]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9713610410690308 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009618327021598816\n",
      "Epoch: 774 \n",
      " Hypothesis: tensor([[152.3430],\n",
      "        [183.9929],\n",
      "        [180.8314],\n",
      "        [196.9480],\n",
      "        [140.5389]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9713279008865356 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009618465788662434\n",
      "Epoch: 775 \n",
      " Hypothesis: tensor([[152.3430],\n",
      "        [183.9930],\n",
      "        [180.8314],\n",
      "        [196.9480],\n",
      "        [140.5389]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9712940454483032 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009618604555726051\n",
      "Epoch: 776 \n",
      " Hypothesis: tensor([[152.3429],\n",
      "        [183.9930],\n",
      "        [180.8314],\n",
      "        [196.9480],\n",
      "        [140.5389]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9712597131729126 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009618743322789669\n",
      "Epoch: 777 \n",
      " Hypothesis: tensor([[152.3429],\n",
      "        [183.9930],\n",
      "        [180.8313],\n",
      "        [196.9480],\n",
      "        [140.5390]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9712263345718384 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009618882089853287\n",
      "Epoch: 778 \n",
      " Hypothesis: tensor([[152.3429],\n",
      "        [183.9930],\n",
      "        [180.8313],\n",
      "        [196.9480],\n",
      "        [140.5390]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9712013006210327 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009619020856916904\n",
      "Epoch: 779 \n",
      " Hypothesis: tensor([[152.3428],\n",
      "        [183.9930],\n",
      "        [180.8313],\n",
      "        [196.9480],\n",
      "        [140.5390]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.971156120300293 \n",
      " W: tensor([[0.7297],\n",
      "        [0.6000],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009619159623980522\n",
      "Epoch: 780 \n",
      " Hypothesis: tensor([[152.3428],\n",
      "        [183.9931],\n",
      "        [180.8313],\n",
      "        [196.9480],\n",
      "        [140.5390]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9711160659790039 \n",
      " W: tensor([[0.7297],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00961929839104414\n",
      "Epoch: 781 \n",
      " Hypothesis: tensor([[152.3428],\n",
      "        [183.9931],\n",
      "        [180.8313],\n",
      "        [196.9480],\n",
      "        [140.5391]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9710797071456909 \n",
      " W: tensor([[0.7297],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009619436226785183\n",
      "Epoch: 782 \n",
      " Hypothesis: tensor([[152.3427],\n",
      "        [183.9931],\n",
      "        [180.8313],\n",
      "        [196.9480],\n",
      "        [140.5391]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9710348844528198 \n",
      " W: tensor([[0.7297],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.0096195749938488\n",
      "Epoch: 783 \n",
      " Hypothesis: tensor([[152.3427],\n",
      "        [183.9931],\n",
      "        [180.8313],\n",
      "        [196.9480],\n",
      "        [140.5392]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9710026979446411 \n",
      " W: tensor([[0.7297],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009619712829589844\n",
      "Epoch: 784 \n",
      " Hypothesis: tensor([[152.3427],\n",
      "        [183.9931],\n",
      "        [180.8313],\n",
      "        [196.9480],\n",
      "        [140.5392]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9709762334823608 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009619850665330887\n",
      "Epoch: 785 \n",
      " Hypothesis: tensor([[152.3427],\n",
      "        [183.9932],\n",
      "        [180.8313],\n",
      "        [196.9480],\n",
      "        [140.5392]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9709372520446777 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009619989432394505\n",
      "Epoch: 786 \n",
      " Hypothesis: tensor([[152.3426],\n",
      "        [183.9932],\n",
      "        [180.8313],\n",
      "        [196.9479],\n",
      "        [140.5392]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9708970785140991 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009620127268135548\n",
      "Epoch: 787 \n",
      " Hypothesis: tensor([[152.3426],\n",
      "        [183.9932],\n",
      "        [180.8313],\n",
      "        [196.9479],\n",
      "        [140.5393]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9708629846572876 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962026510387659\n",
      "Epoch: 788 \n",
      " Hypothesis: tensor([[152.3426],\n",
      "        [183.9932],\n",
      "        [180.8313],\n",
      "        [196.9479],\n",
      "        [140.5393]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9708296060562134 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009620402939617634\n",
      "Epoch: 789 \n",
      " Hypothesis: tensor([[152.3425],\n",
      "        [183.9933],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5393]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9707926511764526 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009620540775358677\n",
      "Epoch: 790 \n",
      " Hypothesis: tensor([[152.3425],\n",
      "        [183.9933],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5394]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9707567095756531 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962067861109972\n",
      "Epoch: 791 \n",
      " Hypothesis: tensor([[152.3425],\n",
      "        [183.9933],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5394]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9707151651382446 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009620816446840763\n",
      "Epoch: 792 \n",
      " Hypothesis: tensor([[152.3425],\n",
      "        [183.9933],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5394]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.970687210559845 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009620954282581806\n",
      "Epoch: 793 \n",
      " Hypothesis: tensor([[152.3424],\n",
      "        [183.9934],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5394]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9706470370292664 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962109211832285\n",
      "Epoch: 794 \n",
      " Hypothesis: tensor([[152.3424],\n",
      "        [183.9934],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5395]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9706028699874878 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009621229954063892\n",
      "Epoch: 795 \n",
      " Hypothesis: tensor([[152.3424],\n",
      "        [183.9934],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5395]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9705706834793091 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009621367789804935\n",
      "Epoch: 796 \n",
      " Hypothesis: tensor([[152.3423],\n",
      "        [183.9934],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5396]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9705316424369812 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009621505625545979\n",
      "Epoch: 797 \n",
      " Hypothesis: tensor([[152.3423],\n",
      "        [183.9935],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5396]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9704983830451965 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009621643461287022\n",
      "Epoch: 798 \n",
      " Hypothesis: tensor([[152.3423],\n",
      "        [183.9935],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5396]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9704672694206238 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009621781297028065\n",
      "Epoch: 799 \n",
      " Hypothesis: tensor([[152.3423],\n",
      "        [183.9935],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5396]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9704340100288391 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009621919132769108\n",
      "Epoch: 800 \n",
      " Hypothesis: tensor([[152.3422],\n",
      "        [183.9935],\n",
      "        [180.8312],\n",
      "        [196.9479],\n",
      "        [140.5397]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9703976511955261 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009622056968510151\n",
      "Epoch: 801 \n",
      " Hypothesis: tensor([[152.3422],\n",
      "        [183.9935],\n",
      "        [180.8312],\n",
      "        [196.9478],\n",
      "        [140.5397]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9703658223152161 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009622194804251194\n",
      "Epoch: 802 \n",
      " Hypothesis: tensor([[152.3422],\n",
      "        [183.9936],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5397]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9703207015991211 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5999],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009622332639992237\n",
      "Epoch: 803 \n",
      " Hypothesis: tensor([[152.3421],\n",
      "        [183.9936],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5397]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9702874422073364 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962247047573328\n",
      "Epoch: 804 \n",
      " Hypothesis: tensor([[152.3421],\n",
      "        [183.9936],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5398]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9702531695365906 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009622608311474323\n",
      "Epoch: 805 \n",
      " Hypothesis: tensor([[152.3421],\n",
      "        [183.9936],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5398]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9702141880989075 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009622746147215366\n",
      "Epoch: 806 \n",
      " Hypothesis: tensor([[152.3420],\n",
      "        [183.9936],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5398]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.970180332660675 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962288398295641\n",
      "Epoch: 807 \n",
      " Hypothesis: tensor([[152.3420],\n",
      "        [183.9937],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5399]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.970140814781189 \n",
      " W: tensor([[0.7298],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009623021818697453\n",
      "Epoch: 808 \n",
      " Hypothesis: tensor([[152.3420],\n",
      "        [183.9937],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5399]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9701007604598999 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009623159654438496\n",
      "Epoch: 809 \n",
      " Hypothesis: tensor([[152.3419],\n",
      "        [183.9937],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5399]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9700795412063599 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009623297490179539\n",
      "Epoch: 810 \n",
      " Hypothesis: tensor([[152.3419],\n",
      "        [183.9937],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5400]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9700394868850708 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009623435325920582\n",
      "Epoch: 811 \n",
      " Hypothesis: tensor([[152.3419],\n",
      "        [183.9938],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5400]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9699963331222534 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009623573161661625\n",
      "Epoch: 812 \n",
      " Hypothesis: tensor([[152.3419],\n",
      "        [183.9938],\n",
      "        [180.8311],\n",
      "        [196.9478],\n",
      "        [140.5400]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.969962477684021 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009623710997402668\n",
      "Epoch: 813 \n",
      " Hypothesis: tensor([[152.3418],\n",
      "        [183.9938],\n",
      "        [180.8311],\n",
      "        [196.9477],\n",
      "        [140.5401]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9699152112007141 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009623848833143711\n",
      "Epoch: 814 \n",
      " Hypothesis: tensor([[152.3418],\n",
      "        [183.9938],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5401]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9698820114135742 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009623986668884754\n",
      "Epoch: 815 \n",
      " Hypothesis: tensor([[152.3418],\n",
      "        [183.9938],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5401]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9698539972305298 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009624124504625797\n",
      "Epoch: 816 \n",
      " Hypothesis: tensor([[152.3417],\n",
      "        [183.9939],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5401]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9698145985603333 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962426234036684\n",
      "Epoch: 817 \n",
      " Hypothesis: tensor([[152.3417],\n",
      "        [183.9939],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5402]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.969786524772644 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009624400176107883\n",
      "Epoch: 818 \n",
      " Hypothesis: tensor([[152.3417],\n",
      "        [183.9939],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5402]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9697376489639282 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009624538011848927\n",
      "Epoch: 819 \n",
      " Hypothesis: tensor([[152.3417],\n",
      "        [183.9939],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5402]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9697036743164062 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962467584758997\n",
      "Epoch: 820 \n",
      " Hypothesis: tensor([[152.3416],\n",
      "        [183.9939],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5403]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9696646928787231 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009624813683331013\n",
      "Epoch: 821 \n",
      " Hypothesis: tensor([[152.3416],\n",
      "        [183.9940],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5403]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9696372151374817 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009624951519072056\n",
      "Epoch: 822 \n",
      " Hypothesis: tensor([[152.3416],\n",
      "        [183.9940],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5403]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9696040153503418 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009625089354813099\n",
      "Epoch: 823 \n",
      " Hypothesis: tensor([[152.3415],\n",
      "        [183.9940],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5403]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9695780873298645 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009625227190554142\n",
      "Epoch: 824 \n",
      " Hypothesis: tensor([[152.3415],\n",
      "        [183.9940],\n",
      "        [180.8310],\n",
      "        [196.9477],\n",
      "        [140.5404]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9695329666137695 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009625365026295185\n",
      "Epoch: 825 \n",
      " Hypothesis: tensor([[152.3415],\n",
      "        [183.9941],\n",
      "        [180.8310],\n",
      "        [196.9476],\n",
      "        [140.5404]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9694929122924805 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5998],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009625502862036228\n",
      "Epoch: 826 \n",
      " Hypothesis: tensor([[152.3414],\n",
      "        [183.9941],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5404]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9694596529006958 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009625640697777271\n",
      "Epoch: 827 \n",
      " Hypothesis: tensor([[152.3414],\n",
      "        [183.9941],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5405]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.96942538022995 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009625778533518314\n",
      "Epoch: 828 \n",
      " Hypothesis: tensor([[152.3414],\n",
      "        [183.9941],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5405]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9693863987922668 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009625916369259357\n",
      "Epoch: 829 \n",
      " Hypothesis: tensor([[152.3414],\n",
      "        [183.9941],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5405]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9693474769592285 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.0096260542050004\n",
      "Epoch: 830 \n",
      " Hypothesis: tensor([[152.3413],\n",
      "        [183.9942],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5406]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9693193435668945 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009626192040741444\n",
      "Epoch: 831 \n",
      " Hypothesis: tensor([[152.3413],\n",
      "        [183.9942],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5406]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9692889451980591 \n",
      " W: tensor([[0.7299],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009626329876482487\n",
      "Epoch: 832 \n",
      " Hypothesis: tensor([[152.3413],\n",
      "        [183.9942],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5406]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9692457318305969 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962646771222353\n",
      "Epoch: 833 \n",
      " Hypothesis: tensor([[152.3412],\n",
      "        [183.9942],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5406]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9692068099975586 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009626605547964573\n",
      "Epoch: 834 \n",
      " Hypothesis: tensor([[152.3412],\n",
      "        [183.9942],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5407]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9691766500473022 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009626743383705616\n",
      "Epoch: 835 \n",
      " Hypothesis: tensor([[152.3412],\n",
      "        [183.9943],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5407]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9691367149353027 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009626881219446659\n",
      "Epoch: 836 \n",
      " Hypothesis: tensor([[152.3411],\n",
      "        [183.9943],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5407]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9691092371940613 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009627019055187702\n",
      "Epoch: 837 \n",
      " Hypothesis: tensor([[152.3411],\n",
      "        [183.9943],\n",
      "        [180.8309],\n",
      "        [196.9476],\n",
      "        [140.5408]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9690602421760559 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009627156890928745\n",
      "Epoch: 838 \n",
      " Hypothesis: tensor([[152.3411],\n",
      "        [183.9943],\n",
      "        [180.8308],\n",
      "        [196.9476],\n",
      "        [140.5408]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9690214395523071 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009627294726669788\n",
      "Epoch: 839 \n",
      " Hypothesis: tensor([[152.3410],\n",
      "        [183.9944],\n",
      "        [180.8308],\n",
      "        [196.9476],\n",
      "        [140.5408]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9689909219741821 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009627432562410831\n",
      "Epoch: 840 \n",
      " Hypothesis: tensor([[152.3410],\n",
      "        [183.9944],\n",
      "        [180.8308],\n",
      "        [196.9475],\n",
      "        [140.5408]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9689569473266602 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009627570398151875\n",
      "Epoch: 841 \n",
      " Hypothesis: tensor([[152.3410],\n",
      "        [183.9944],\n",
      "        [180.8308],\n",
      "        [196.9475],\n",
      "        [140.5409]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9689180254936218 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009627708233892918\n",
      "Epoch: 842 \n",
      " Hypothesis: tensor([[152.3410],\n",
      "        [183.9944],\n",
      "        [180.8308],\n",
      "        [196.9475],\n",
      "        [140.5409]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9688838124275208 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962784606963396\n",
      "Epoch: 843 \n",
      " Hypothesis: tensor([[152.3409],\n",
      "        [183.9945],\n",
      "        [180.8308],\n",
      "        [196.9475],\n",
      "        [140.5409]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9688445329666138 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009627983905375004\n",
      "Epoch: 844 \n",
      " Hypothesis: tensor([[152.3409],\n",
      "        [183.9945],\n",
      "        [180.8308],\n",
      "        [196.9475],\n",
      "        [140.5410]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9688105583190918 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009628121741116047\n",
      "Epoch: 845 \n",
      " Hypothesis: tensor([[152.3409],\n",
      "        [183.9945],\n",
      "        [180.8308],\n",
      "        [196.9475],\n",
      "        [140.5410]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.968771755695343 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962825957685709\n",
      "Epoch: 846 \n",
      " Hypothesis: tensor([[152.3408],\n",
      "        [183.9945],\n",
      "        [180.8308],\n",
      "        [196.9475],\n",
      "        [140.5410]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9687384366989136 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009628397412598133\n",
      "Epoch: 847 \n",
      " Hypothesis: tensor([[152.3408],\n",
      "        [183.9945],\n",
      "        [180.8308],\n",
      "        [196.9475],\n",
      "        [140.5411]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9687005281448364 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009628535248339176\n",
      "Epoch: 848 \n",
      " Hypothesis: tensor([[152.3408],\n",
      "        [183.9946],\n",
      "        [180.8307],\n",
      "        [196.9475],\n",
      "        [140.5411]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9686554074287415 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5997],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00962867308408022\n",
      "Epoch: 849 \n",
      " Hypothesis: tensor([[152.3408],\n",
      "        [183.9946],\n",
      "        [180.8307],\n",
      "        [196.9475],\n",
      "        [140.5411]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9686222076416016 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009628810919821262\n",
      "Epoch: 850 \n",
      " Hypothesis: tensor([[152.3407],\n",
      "        [183.9946],\n",
      "        [180.8307],\n",
      "        [196.9475],\n",
      "        [140.5412]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9685878753662109 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009628948755562305\n",
      "Epoch: 851 \n",
      " Hypothesis: tensor([[152.3407],\n",
      "        [183.9946],\n",
      "        [180.8307],\n",
      "        [196.9474],\n",
      "        [140.5412]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.968558132648468 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009629086591303349\n",
      "Epoch: 852 \n",
      " Hypothesis: tensor([[152.3407],\n",
      "        [183.9946],\n",
      "        [180.8307],\n",
      "        [196.9474],\n",
      "        [140.5412]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.968513011932373 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009629224427044392\n",
      "Epoch: 853 \n",
      " Hypothesis: tensor([[152.3406],\n",
      "        [183.9947],\n",
      "        [180.8307],\n",
      "        [196.9474],\n",
      "        [140.5412]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9684808850288391 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009629362262785435\n",
      "Epoch: 854 \n",
      " Hypothesis: tensor([[152.3406],\n",
      "        [183.9947],\n",
      "        [180.8307],\n",
      "        [196.9474],\n",
      "        [140.5413]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9684627652168274 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009629500098526478\n",
      "Epoch: 855 \n",
      " Hypothesis: tensor([[152.3406],\n",
      "        [183.9947],\n",
      "        [180.8307],\n",
      "        [196.9474],\n",
      "        [140.5413]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9684224128723145 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009629637934267521\n",
      "Epoch: 856 \n",
      " Hypothesis: tensor([[152.3405],\n",
      "        [183.9947],\n",
      "        [180.8307],\n",
      "        [196.9474],\n",
      "        [140.5413]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9683796167373657 \n",
      " W: tensor([[0.7300],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009629775770008564\n",
      "Epoch: 857 \n",
      " Hypothesis: tensor([[152.3405],\n",
      "        [183.9948],\n",
      "        [180.8307],\n",
      "        [196.9474],\n",
      "        [140.5414]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9683406949043274 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009629913605749607\n",
      "Epoch: 858 \n",
      " Hypothesis: tensor([[152.3405],\n",
      "        [183.9948],\n",
      "        [180.8307],\n",
      "        [196.9474],\n",
      "        [140.5414]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9682992696762085 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963005144149065\n",
      "Epoch: 859 \n",
      " Hypothesis: tensor([[152.3405],\n",
      "        [183.9948],\n",
      "        [180.8307],\n",
      "        [196.9474],\n",
      "        [140.5414]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9682732820510864 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009630189277231693\n",
      "Epoch: 860 \n",
      " Hypothesis: tensor([[152.3404],\n",
      "        [183.9948],\n",
      "        [180.8307],\n",
      "        [196.9474],\n",
      "        [140.5414]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9682432413101196 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009630327112972736\n",
      "Epoch: 861 \n",
      " Hypothesis: tensor([[152.3404],\n",
      "        [183.9948],\n",
      "        [180.8306],\n",
      "        [196.9474],\n",
      "        [140.5415]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9682042002677917 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963046494871378\n",
      "Epoch: 862 \n",
      " Hypothesis: tensor([[152.3404],\n",
      "        [183.9949],\n",
      "        [180.8306],\n",
      "        [196.9474],\n",
      "        [140.5415]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9681699872016907 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009630602784454823\n",
      "Epoch: 863 \n",
      " Hypothesis: tensor([[152.3403],\n",
      "        [183.9949],\n",
      "        [180.8306],\n",
      "        [196.9474],\n",
      "        [140.5415]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9681327939033508 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009630740620195866\n",
      "Epoch: 864 \n",
      " Hypothesis: tensor([[152.3403],\n",
      "        [183.9949],\n",
      "        [180.8306],\n",
      "        [196.9474],\n",
      "        [140.5416]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.968098521232605 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009630878455936909\n",
      "Epoch: 865 \n",
      " Hypothesis: tensor([[152.3403],\n",
      "        [183.9949],\n",
      "        [180.8306],\n",
      "        [196.9474],\n",
      "        [140.5416]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9680637121200562 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009631016291677952\n",
      "Epoch: 866 \n",
      " Hypothesis: tensor([[152.3402],\n",
      "        [183.9949],\n",
      "        [180.8306],\n",
      "        [196.9473],\n",
      "        [140.5416]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9680148363113403 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009631154127418995\n",
      "Epoch: 867 \n",
      " Hypothesis: tensor([[152.3402],\n",
      "        [183.9950],\n",
      "        [180.8306],\n",
      "        [196.9473],\n",
      "        [140.5417]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9679847955703735 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009631291963160038\n",
      "Epoch: 868 \n",
      " Hypothesis: tensor([[152.3402],\n",
      "        [183.9950],\n",
      "        [180.8306],\n",
      "        [196.9473],\n",
      "        [140.5417]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9679485559463501 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009631429798901081\n",
      "Epoch: 869 \n",
      " Hypothesis: tensor([[152.3402],\n",
      "        [183.9950],\n",
      "        [180.8306],\n",
      "        [196.9473],\n",
      "        [140.5417]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9679136276245117 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009631567634642124\n",
      "Epoch: 870 \n",
      " Hypothesis: tensor([[152.3401],\n",
      "        [183.9950],\n",
      "        [180.8306],\n",
      "        [196.9473],\n",
      "        [140.5417]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9678794145584106 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009631705470383167\n",
      "Epoch: 871 \n",
      " Hypothesis: tensor([[152.3401],\n",
      "        [183.9951],\n",
      "        [180.8306],\n",
      "        [196.9473],\n",
      "        [140.5418]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9678441882133484 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5996],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963184330612421\n",
      "Epoch: 872 \n",
      " Hypothesis: tensor([[152.3401],\n",
      "        [183.9951],\n",
      "        [180.8306],\n",
      "        [196.9473],\n",
      "        [140.5418]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.967807412147522 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009631981141865253\n",
      "Epoch: 873 \n",
      " Hypothesis: tensor([[152.3400],\n",
      "        [183.9951],\n",
      "        [180.8306],\n",
      "        [196.9473],\n",
      "        [140.5418]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9677673578262329 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009632118977606297\n",
      "Epoch: 874 \n",
      " Hypothesis: tensor([[152.3400],\n",
      "        [183.9951],\n",
      "        [180.8305],\n",
      "        [196.9473],\n",
      "        [140.5419]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9677260518074036 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963225681334734\n",
      "Epoch: 875 \n",
      " Hypothesis: tensor([[152.3400],\n",
      "        [183.9951],\n",
      "        [180.8305],\n",
      "        [196.9473],\n",
      "        [140.5419]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9676922559738159 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009632394649088383\n",
      "Epoch: 876 \n",
      " Hypothesis: tensor([[152.3400],\n",
      "        [183.9952],\n",
      "        [180.8305],\n",
      "        [196.9473],\n",
      "        [140.5419]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9676641225814819 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009632532484829426\n",
      "Epoch: 877 \n",
      " Hypothesis: tensor([[152.3399],\n",
      "        [183.9952],\n",
      "        [180.8305],\n",
      "        [196.9473],\n",
      "        [140.5419]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9676249623298645 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009632670320570469\n",
      "Epoch: 878 \n",
      " Hypothesis: tensor([[152.3399],\n",
      "        [183.9952],\n",
      "        [180.8305],\n",
      "        [196.9473],\n",
      "        [140.5420]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9675917625427246 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009632808156311512\n",
      "Epoch: 879 \n",
      " Hypothesis: tensor([[152.3399],\n",
      "        [183.9952],\n",
      "        [180.8305],\n",
      "        [196.9472],\n",
      "        [140.5420]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9675517082214355 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009632945992052555\n",
      "Epoch: 880 \n",
      " Hypothesis: tensor([[152.3398],\n",
      "        [183.9953],\n",
      "        [180.8305],\n",
      "        [196.9472],\n",
      "        [140.5421]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9675089716911316 \n",
      " W: tensor([[0.7301],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009633083827793598\n",
      "Epoch: 881 \n",
      " Hypothesis: tensor([[152.3398],\n",
      "        [183.9953],\n",
      "        [180.8305],\n",
      "        [196.9472],\n",
      "        [140.5421]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9674785733222961 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009633221663534641\n",
      "Epoch: 882 \n",
      " Hypothesis: tensor([[152.3398],\n",
      "        [183.9953],\n",
      "        [180.8305],\n",
      "        [196.9472],\n",
      "        [140.5421]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9674455523490906 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009633359499275684\n",
      "Epoch: 883 \n",
      " Hypothesis: tensor([[152.3397],\n",
      "        [183.9953],\n",
      "        [180.8305],\n",
      "        [196.9472],\n",
      "        [140.5421]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9674116373062134 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009633497335016727\n",
      "Epoch: 884 \n",
      " Hypothesis: tensor([[152.3397],\n",
      "        [183.9953],\n",
      "        [180.8305],\n",
      "        [196.9472],\n",
      "        [140.5422]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9673716425895691 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963363517075777\n",
      "Epoch: 885 \n",
      " Hypothesis: tensor([[152.3397],\n",
      "        [183.9954],\n",
      "        [180.8304],\n",
      "        [196.9472],\n",
      "        [140.5422]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9673323631286621 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009633773006498814\n",
      "Epoch: 886 \n",
      " Hypothesis: tensor([[152.3396],\n",
      "        [183.9954],\n",
      "        [180.8304],\n",
      "        [196.9472],\n",
      "        [140.5422]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9672991633415222 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009633910842239857\n",
      "Epoch: 887 \n",
      " Hypothesis: tensor([[152.3396],\n",
      "        [183.9954],\n",
      "        [180.8304],\n",
      "        [196.9472],\n",
      "        [140.5423]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9672711491584778 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.0096340486779809\n",
      "Epoch: 888 \n",
      " Hypothesis: tensor([[152.3396],\n",
      "        [183.9954],\n",
      "        [180.8304],\n",
      "        [196.9472],\n",
      "        [140.5423]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9672310948371887 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009634186513721943\n",
      "Epoch: 889 \n",
      " Hypothesis: tensor([[152.3396],\n",
      "        [183.9955],\n",
      "        [180.8304],\n",
      "        [196.9472],\n",
      "        [140.5423]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9671922922134399 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009634324349462986\n",
      "Epoch: 890 \n",
      " Hypothesis: tensor([[152.3395],\n",
      "        [183.9955],\n",
      "        [180.8304],\n",
      "        [196.9472],\n",
      "        [140.5423]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9671590924263 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009634462185204029\n",
      "Epoch: 891 \n",
      " Hypothesis: tensor([[152.3395],\n",
      "        [183.9955],\n",
      "        [180.8304],\n",
      "        [196.9472],\n",
      "        [140.5424]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9671249389648438 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009634600020945072\n",
      "Epoch: 892 \n",
      " Hypothesis: tensor([[152.3395],\n",
      "        [183.9955],\n",
      "        [180.8304],\n",
      "        [196.9471],\n",
      "        [140.5424]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9670921564102173 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009634737856686115\n",
      "Epoch: 893 \n",
      " Hypothesis: tensor([[152.3394],\n",
      "        [183.9955],\n",
      "        [180.8304],\n",
      "        [196.9471],\n",
      "        [140.5424]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9670408964157104 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009634875692427158\n",
      "Epoch: 894 \n",
      " Hypothesis: tensor([[152.3394],\n",
      "        [183.9956],\n",
      "        [180.8304],\n",
      "        [196.9471],\n",
      "        [140.5425]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.967012882232666 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5995],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009635013528168201\n",
      "Epoch: 895 \n",
      " Hypothesis: tensor([[152.3394],\n",
      "        [183.9956],\n",
      "        [180.8304],\n",
      "        [196.9471],\n",
      "        [140.5425]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9669766426086426 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009635151363909245\n",
      "Epoch: 896 \n",
      " Hypothesis: tensor([[152.3393],\n",
      "        [183.9956],\n",
      "        [180.8304],\n",
      "        [196.9471],\n",
      "        [140.5425]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9669429063796997 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009635289199650288\n",
      "Epoch: 897 \n",
      " Hypothesis: tensor([[152.3393],\n",
      "        [183.9956],\n",
      "        [180.8303],\n",
      "        [196.9471],\n",
      "        [140.5425]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9669097065925598 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963542703539133\n",
      "Epoch: 898 \n",
      " Hypothesis: tensor([[152.3393],\n",
      "        [183.9956],\n",
      "        [180.8303],\n",
      "        [196.9471],\n",
      "        [140.5426]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9668728113174438 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009635564871132374\n",
      "Epoch: 899 \n",
      " Hypothesis: tensor([[152.3392],\n",
      "        [183.9957],\n",
      "        [180.8303],\n",
      "        [196.9471],\n",
      "        [140.5426]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9668328166007996 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009635702706873417\n",
      "Epoch: 900 \n",
      " Hypothesis: tensor([[152.3392],\n",
      "        [183.9957],\n",
      "        [180.8303],\n",
      "        [196.9471],\n",
      "        [140.5426]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9667976498603821 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963584054261446\n",
      "Epoch: 901 \n",
      " Hypothesis: tensor([[152.3392],\n",
      "        [183.9957],\n",
      "        [180.8303],\n",
      "        [196.9471],\n",
      "        [140.5427]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9667576551437378 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009635978378355503\n",
      "Epoch: 902 \n",
      " Hypothesis: tensor([[152.3392],\n",
      "        [183.9957],\n",
      "        [180.8303],\n",
      "        [196.9471],\n",
      "        [140.5427]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9667224884033203 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009636116214096546\n",
      "Epoch: 903 \n",
      " Hypothesis: tensor([[152.3391],\n",
      "        [183.9958],\n",
      "        [180.8303],\n",
      "        [196.9471],\n",
      "        [140.5427]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.966683566570282 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963625404983759\n",
      "Epoch: 904 \n",
      " Hypothesis: tensor([[152.3391],\n",
      "        [183.9958],\n",
      "        [180.8303],\n",
      "        [196.9471],\n",
      "        [140.5428]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9666644334793091 \n",
      " W: tensor([[0.7302],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009636391885578632\n",
      "Epoch: 905 \n",
      " Hypothesis: tensor([[152.3391],\n",
      "        [183.9958],\n",
      "        [180.8303],\n",
      "        [196.9470],\n",
      "        [140.5428]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9666147232055664 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009636529721319675\n",
      "Epoch: 906 \n",
      " Hypothesis: tensor([[152.3390],\n",
      "        [183.9958],\n",
      "        [180.8303],\n",
      "        [196.9470],\n",
      "        [140.5428]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.966578483581543 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009636667557060719\n",
      "Epoch: 907 \n",
      " Hypothesis: tensor([[152.3390],\n",
      "        [183.9958],\n",
      "        [180.8302],\n",
      "        [196.9470],\n",
      "        [140.5428]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9665542840957642 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009636805392801762\n",
      "Epoch: 908 \n",
      " Hypothesis: tensor([[152.3390],\n",
      "        [183.9958],\n",
      "        [180.8302],\n",
      "        [196.9470],\n",
      "        [140.5429]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9665095210075378 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009636943228542805\n",
      "Epoch: 909 \n",
      " Hypothesis: tensor([[152.3389],\n",
      "        [183.9959],\n",
      "        [180.8302],\n",
      "        [196.9470],\n",
      "        [140.5429]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9664725065231323 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009637081064283848\n",
      "Epoch: 910 \n",
      " Hypothesis: tensor([[152.3389],\n",
      "        [183.9959],\n",
      "        [180.8302],\n",
      "        [196.9470],\n",
      "        [140.5429]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9664353132247925 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009637218900024891\n",
      "Epoch: 911 \n",
      " Hypothesis: tensor([[152.3389],\n",
      "        [183.9959],\n",
      "        [180.8302],\n",
      "        [196.9470],\n",
      "        [140.5430]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9664033055305481 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009637356735765934\n",
      "Epoch: 912 \n",
      " Hypothesis: tensor([[152.3389],\n",
      "        [183.9959],\n",
      "        [180.8302],\n",
      "        [196.9470],\n",
      "        [140.5430]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9663634300231934 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009637494571506977\n",
      "Epoch: 913 \n",
      " Hypothesis: tensor([[152.3388],\n",
      "        [183.9960],\n",
      "        [180.8302],\n",
      "        [196.9470],\n",
      "        [140.5430]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.966330349445343 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963763240724802\n",
      "Epoch: 914 \n",
      " Hypothesis: tensor([[152.3388],\n",
      "        [183.9960],\n",
      "        [180.8302],\n",
      "        [196.9470],\n",
      "        [140.5430]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.966302216053009 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009637770242989063\n",
      "Epoch: 915 \n",
      " Hypothesis: tensor([[152.3388],\n",
      "        [183.9960],\n",
      "        [180.8302],\n",
      "        [196.9469],\n",
      "        [140.5431]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9662664532661438 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009637908078730106\n",
      "Epoch: 916 \n",
      " Hypothesis: tensor([[152.3387],\n",
      "        [183.9960],\n",
      "        [180.8302],\n",
      "        [196.9469],\n",
      "        [140.5431]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9662322998046875 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963804591447115\n",
      "Epoch: 917 \n",
      " Hypothesis: tensor([[152.3387],\n",
      "        [183.9960],\n",
      "        [180.8301],\n",
      "        [196.9469],\n",
      "        [140.5431]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9661882519721985 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5994],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009638183750212193\n",
      "Epoch: 918 \n",
      " Hypothesis: tensor([[152.3387],\n",
      "        [183.9960],\n",
      "        [180.8301],\n",
      "        [196.9469],\n",
      "        [140.5432]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9661544561386108 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009638321585953236\n",
      "Epoch: 919 \n",
      " Hypothesis: tensor([[152.3386],\n",
      "        [183.9961],\n",
      "        [180.8301],\n",
      "        [196.9469],\n",
      "        [140.5432]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9661203622817993 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009638459421694279\n",
      "Epoch: 920 \n",
      " Hypothesis: tensor([[152.3386],\n",
      "        [183.9961],\n",
      "        [180.8301],\n",
      "        [196.9469],\n",
      "        [140.5432]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9660763740539551 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009638597257435322\n",
      "Epoch: 921 \n",
      " Hypothesis: tensor([[152.3386],\n",
      "        [183.9961],\n",
      "        [180.8301],\n",
      "        [196.9469],\n",
      "        [140.5432]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9660404324531555 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009638735093176365\n",
      "Epoch: 922 \n",
      " Hypothesis: tensor([[152.3385],\n",
      "        [183.9961],\n",
      "        [180.8301],\n",
      "        [196.9469],\n",
      "        [140.5433]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9660011529922485 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009638872928917408\n",
      "Epoch: 923 \n",
      " Hypothesis: tensor([[152.3385],\n",
      "        [183.9962],\n",
      "        [180.8301],\n",
      "        [196.9469],\n",
      "        [140.5433]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9659751653671265 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009639010764658451\n",
      "Epoch: 924 \n",
      " Hypothesis: tensor([[152.3385],\n",
      "        [183.9962],\n",
      "        [180.8301],\n",
      "        [196.9469],\n",
      "        [140.5433]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9659431576728821 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009639148600399494\n",
      "Epoch: 925 \n",
      " Hypothesis: tensor([[152.3384],\n",
      "        [183.9962],\n",
      "        [180.8301],\n",
      "        [196.9468],\n",
      "        [140.5434]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9659093022346497 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009639286436140537\n",
      "Epoch: 926 \n",
      " Hypothesis: tensor([[152.3384],\n",
      "        [183.9962],\n",
      "        [180.8301],\n",
      "        [196.9468],\n",
      "        [140.5434]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9658700823783875 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963942427188158\n",
      "Epoch: 927 \n",
      " Hypothesis: tensor([[152.3384],\n",
      "        [183.9962],\n",
      "        [180.8300],\n",
      "        [196.9468],\n",
      "        [140.5434]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.96583092212677 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009639562107622623\n",
      "Epoch: 928 \n",
      " Hypothesis: tensor([[152.3383],\n",
      "        [183.9963],\n",
      "        [180.8300],\n",
      "        [196.9468],\n",
      "        [140.5434]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9657913446426392 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009639699943363667\n",
      "Epoch: 929 \n",
      " Hypothesis: tensor([[152.3383],\n",
      "        [183.9963],\n",
      "        [180.8300],\n",
      "        [196.9468],\n",
      "        [140.5435]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9657602310180664 \n",
      " W: tensor([[0.7303],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00963983777910471\n",
      "Epoch: 930 \n",
      " Hypothesis: tensor([[152.3383],\n",
      "        [183.9963],\n",
      "        [180.8300],\n",
      "        [196.9468],\n",
      "        [140.5435]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9657189249992371 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009639975614845753\n",
      "Epoch: 931 \n",
      " Hypothesis: tensor([[152.3383],\n",
      "        [183.9963],\n",
      "        [180.8300],\n",
      "        [196.9468],\n",
      "        [140.5435]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9656940698623657 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009640113450586796\n",
      "Epoch: 932 \n",
      " Hypothesis: tensor([[152.3382],\n",
      "        [183.9964],\n",
      "        [180.8300],\n",
      "        [196.9468],\n",
      "        [140.5435]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9656511545181274 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009640251286327839\n",
      "Epoch: 933 \n",
      " Hypothesis: tensor([[152.3382],\n",
      "        [183.9964],\n",
      "        [180.8300],\n",
      "        [196.9468],\n",
      "        [140.5436]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9656121134757996 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009640389122068882\n",
      "Epoch: 934 \n",
      " Hypothesis: tensor([[152.3382],\n",
      "        [183.9964],\n",
      "        [180.8300],\n",
      "        [196.9468],\n",
      "        [140.5436]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9655793309211731 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009640526957809925\n",
      "Epoch: 935 \n",
      " Hypothesis: tensor([[152.3382],\n",
      "        [183.9964],\n",
      "        [180.8300],\n",
      "        [196.9467],\n",
      "        [140.5436]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9655503034591675 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009640664793550968\n",
      "Epoch: 936 \n",
      " Hypothesis: tensor([[152.3381],\n",
      "        [183.9964],\n",
      "        [180.8300],\n",
      "        [196.9467],\n",
      "        [140.5437]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.96551114320755 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009640802629292011\n",
      "Epoch: 937 \n",
      " Hypothesis: tensor([[152.3381],\n",
      "        [183.9964],\n",
      "        [180.8300],\n",
      "        [196.9467],\n",
      "        [140.5437]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9654759168624878 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009640940465033054\n",
      "Epoch: 938 \n",
      " Hypothesis: tensor([[152.3380],\n",
      "        [183.9965],\n",
      "        [180.8299],\n",
      "        [196.9467],\n",
      "        [140.5437]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.965437114238739 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009641078300774097\n",
      "Epoch: 939 \n",
      " Hypothesis: tensor([[152.3380],\n",
      "        [183.9965],\n",
      "        [180.8299],\n",
      "        [196.9467],\n",
      "        [140.5437]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.965406060218811 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5993],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00964121613651514\n",
      "Epoch: 940 \n",
      " Hypothesis: tensor([[152.3380],\n",
      "        [183.9965],\n",
      "        [180.8299],\n",
      "        [196.9467],\n",
      "        [140.5438]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9653733968734741 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009641353972256184\n",
      "Epoch: 941 \n",
      " Hypothesis: tensor([[152.3380],\n",
      "        [183.9965],\n",
      "        [180.8299],\n",
      "        [196.9467],\n",
      "        [140.5438]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9653183817863464 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009641491807997227\n",
      "Epoch: 942 \n",
      " Hypothesis: tensor([[152.3379],\n",
      "        [183.9966],\n",
      "        [180.8299],\n",
      "        [196.9467],\n",
      "        [140.5439]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9652853012084961 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00964162964373827\n",
      "Epoch: 943 \n",
      " Hypothesis: tensor([[152.3379],\n",
      "        [183.9966],\n",
      "        [180.8299],\n",
      "        [196.9467],\n",
      "        [140.5439]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9652611017227173 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009641767479479313\n",
      "Epoch: 944 \n",
      " Hypothesis: tensor([[152.3379],\n",
      "        [183.9966],\n",
      "        [180.8299],\n",
      "        [196.9467],\n",
      "        [140.5439]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9652279615402222 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009641905315220356\n",
      "Epoch: 945 \n",
      " Hypothesis: tensor([[152.3378],\n",
      "        [183.9966],\n",
      "        [180.8299],\n",
      "        [196.9467],\n",
      "        [140.5439]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9651772379875183 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009642043150961399\n",
      "Epoch: 946 \n",
      " Hypothesis: tensor([[152.3378],\n",
      "        [183.9966],\n",
      "        [180.8299],\n",
      "        [196.9467],\n",
      "        [140.5440]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9651492238044739 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009642180986702442\n",
      "Epoch: 947 \n",
      " Hypothesis: tensor([[152.3378],\n",
      "        [183.9966],\n",
      "        [180.8299],\n",
      "        [196.9466],\n",
      "        [140.5440]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9651154279708862 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009642318822443485\n",
      "Epoch: 948 \n",
      " Hypothesis: tensor([[152.3377],\n",
      "        [183.9967],\n",
      "        [180.8298],\n",
      "        [196.9466],\n",
      "        [140.5440]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9650704264640808 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009642456658184528\n",
      "Epoch: 949 \n",
      " Hypothesis: tensor([[152.3377],\n",
      "        [183.9967],\n",
      "        [180.8298],\n",
      "        [196.9466],\n",
      "        [140.5441]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9650362730026245 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009642594493925571\n",
      "Epoch: 950 \n",
      " Hypothesis: tensor([[152.3377],\n",
      "        [183.9967],\n",
      "        [180.8298],\n",
      "        [196.9466],\n",
      "        [140.5441]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9650182723999023 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009642732329666615\n",
      "Epoch: 951 \n",
      " Hypothesis: tensor([[152.3376],\n",
      "        [183.9967],\n",
      "        [180.8298],\n",
      "        [196.9466],\n",
      "        [140.5441]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9649850726127625 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009642870165407658\n",
      "Epoch: 952 \n",
      " Hypothesis: tensor([[152.3376],\n",
      "        [183.9968],\n",
      "        [180.8298],\n",
      "        [196.9466],\n",
      "        [140.5441]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9649340510368347 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.0096430080011487\n",
      "Epoch: 953 \n",
      " Hypothesis: tensor([[152.3376],\n",
      "        [183.9968],\n",
      "        [180.8298],\n",
      "        [196.9466],\n",
      "        [140.5442]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9649063348770142 \n",
      " W: tensor([[0.7304],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009643145836889744\n",
      "Epoch: 954 \n",
      " Hypothesis: tensor([[152.3376],\n",
      "        [183.9968],\n",
      "        [180.8298],\n",
      "        [196.9466],\n",
      "        [140.5442]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9648613929748535 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009643283672630787\n",
      "Epoch: 955 \n",
      " Hypothesis: tensor([[152.3375],\n",
      "        [183.9968],\n",
      "        [180.8298],\n",
      "        [196.9466],\n",
      "        [140.5442]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9648276567459106 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00964342150837183\n",
      "Epoch: 956 \n",
      " Hypothesis: tensor([[152.3375],\n",
      "        [183.9968],\n",
      "        [180.8298],\n",
      "        [196.9466],\n",
      "        [140.5443]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9647944569587708 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009643559344112873\n",
      "Epoch: 957 \n",
      " Hypothesis: tensor([[152.3375],\n",
      "        [183.9969],\n",
      "        [180.8298],\n",
      "        [196.9466],\n",
      "        [140.5443]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.964760422706604 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009643697179853916\n",
      "Epoch: 958 \n",
      " Hypothesis: tensor([[152.3374],\n",
      "        [183.9969],\n",
      "        [180.8298],\n",
      "        [196.9465],\n",
      "        [140.5443]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9647157788276672 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00964383501559496\n",
      "Epoch: 959 \n",
      " Hypothesis: tensor([[152.3374],\n",
      "        [183.9969],\n",
      "        [180.8297],\n",
      "        [196.9465],\n",
      "        [140.5443]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9646915197372437 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009643972851336002\n",
      "Epoch: 960 \n",
      " Hypothesis: tensor([[152.3374],\n",
      "        [183.9969],\n",
      "        [180.8297],\n",
      "        [196.9465],\n",
      "        [140.5444]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9646574854850769 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009644110687077045\n",
      "Epoch: 961 \n",
      " Hypothesis: tensor([[152.3373],\n",
      "        [183.9969],\n",
      "        [180.8297],\n",
      "        [196.9465],\n",
      "        [140.5444]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9646114110946655 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009644248522818089\n",
      "Epoch: 962 \n",
      " Hypothesis: tensor([[152.3373],\n",
      "        [183.9969],\n",
      "        [180.8297],\n",
      "        [196.9465],\n",
      "        [140.5444]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9645708203315735 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5992],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009644386358559132\n",
      "Epoch: 963 \n",
      " Hypothesis: tensor([[152.3373],\n",
      "        [183.9970],\n",
      "        [180.8297],\n",
      "        [196.9465],\n",
      "        [140.5444]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9645456075668335 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009644524194300175\n",
      "Epoch: 964 \n",
      " Hypothesis: tensor([[152.3372],\n",
      "        [183.9970],\n",
      "        [180.8297],\n",
      "        [196.9465],\n",
      "        [140.5445]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9645142555236816 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009644662030041218\n",
      "Epoch: 965 \n",
      " Hypothesis: tensor([[152.3372],\n",
      "        [183.9970],\n",
      "        [180.8297],\n",
      "        [196.9465],\n",
      "        [140.5445]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9644657969474792 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009644799865782261\n",
      "Epoch: 966 \n",
      " Hypothesis: tensor([[152.3372],\n",
      "        [183.9970],\n",
      "        [180.8297],\n",
      "        [196.9465],\n",
      "        [140.5445]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9644327163696289 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009644937701523304\n",
      "Epoch: 967 \n",
      " Hypothesis: tensor([[152.3371],\n",
      "        [183.9971],\n",
      "        [180.8297],\n",
      "        [196.9465],\n",
      "        [140.5446]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9643963575363159 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009645075537264347\n",
      "Epoch: 968 \n",
      " Hypothesis: tensor([[152.3371],\n",
      "        [183.9971],\n",
      "        [180.8297],\n",
      "        [196.9465],\n",
      "        [140.5446]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9643646478652954 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00964521337300539\n",
      "Epoch: 969 \n",
      " Hypothesis: tensor([[152.3371],\n",
      "        [183.9971],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5446]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9643257856369019 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009645351208746433\n",
      "Epoch: 970 \n",
      " Hypothesis: tensor([[152.3371],\n",
      "        [183.9971],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5446]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9642899632453918 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009645489044487476\n",
      "Epoch: 971 \n",
      " Hypothesis: tensor([[152.3370],\n",
      "        [183.9971],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5447]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9642511606216431 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00964562688022852\n",
      "Epoch: 972 \n",
      " Hypothesis: tensor([[152.3370],\n",
      "        [183.9972],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5447]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9642170071601868 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009645764715969563\n",
      "Epoch: 973 \n",
      " Hypothesis: tensor([[152.3370],\n",
      "        [183.9972],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5447]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9641839265823364 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009645902551710606\n",
      "Epoch: 974 \n",
      " Hypothesis: tensor([[152.3369],\n",
      "        [183.9972],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5448]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9641467928886414 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009646040387451649\n",
      "Epoch: 975 \n",
      " Hypothesis: tensor([[152.3369],\n",
      "        [183.9972],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5448]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9641109704971313 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009646178223192692\n",
      "Epoch: 976 \n",
      " Hypothesis: tensor([[152.3369],\n",
      "        [183.9972],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5448]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9640809893608093 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009646316058933735\n",
      "Epoch: 977 \n",
      " Hypothesis: tensor([[152.3368],\n",
      "        [183.9973],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5448]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.964042067527771 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009646453894674778\n",
      "Epoch: 978 \n",
      " Hypothesis: tensor([[152.3368],\n",
      "        [183.9973],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5449]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9640023112297058 \n",
      " W: tensor([[0.7305],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009646591730415821\n",
      "Epoch: 979 \n",
      " Hypothesis: tensor([[152.3368],\n",
      "        [183.9973],\n",
      "        [180.8296],\n",
      "        [196.9464],\n",
      "        [140.5449]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9639742970466614 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009646729566156864\n",
      "Epoch: 980 \n",
      " Hypothesis: tensor([[152.3367],\n",
      "        [183.9973],\n",
      "        [180.8295],\n",
      "        [196.9464],\n",
      "        [140.5449]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.963941216468811 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009646867401897907\n",
      "Epoch: 981 \n",
      " Hypothesis: tensor([[152.3367],\n",
      "        [183.9973],\n",
      "        [180.8295],\n",
      "        [196.9463],\n",
      "        [140.5450]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9638962745666504 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00964700523763895\n",
      "Epoch: 982 \n",
      " Hypothesis: tensor([[152.3367],\n",
      "        [183.9974],\n",
      "        [180.8295],\n",
      "        [196.9463],\n",
      "        [140.5450]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.963868260383606 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009647143073379993\n",
      "Epoch: 983 \n",
      " Hypothesis: tensor([[152.3367],\n",
      "        [183.9974],\n",
      "        [180.8295],\n",
      "        [196.9463],\n",
      "        [140.5450]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9638321995735168 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009647280909121037\n",
      "Epoch: 984 \n",
      " Hypothesis: tensor([[152.3366],\n",
      "        [183.9974],\n",
      "        [180.8295],\n",
      "        [196.9463],\n",
      "        [140.5450]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9637933969497681 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00964741874486208\n",
      "Epoch: 985 \n",
      " Hypothesis: tensor([[152.3366],\n",
      "        [183.9974],\n",
      "        [180.8295],\n",
      "        [196.9463],\n",
      "        [140.5451]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9637596011161804 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5991],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009647556580603123\n",
      "Epoch: 986 \n",
      " Hypothesis: tensor([[152.3365],\n",
      "        [183.9975],\n",
      "        [180.8295],\n",
      "        [196.9463],\n",
      "        [140.5451]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9637095332145691 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009647694416344166\n",
      "Epoch: 987 \n",
      " Hypothesis: tensor([[152.3365],\n",
      "        [183.9975],\n",
      "        [180.8295],\n",
      "        [196.9463],\n",
      "        [140.5452]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9636834859848022 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009647832252085209\n",
      "Epoch: 988 \n",
      " Hypothesis: tensor([[152.3365],\n",
      "        [183.9975],\n",
      "        [180.8295],\n",
      "        [196.9463],\n",
      "        [140.5452]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.963653564453125 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009647970087826252\n",
      "Epoch: 989 \n",
      " Hypothesis: tensor([[152.3365],\n",
      "        [183.9975],\n",
      "        [180.8295],\n",
      "        [196.9463],\n",
      "        [140.5452]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9636128544807434 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009648107923567295\n",
      "Epoch: 990 \n",
      " Hypothesis: tensor([[152.3364],\n",
      "        [183.9975],\n",
      "        [180.8294],\n",
      "        [196.9463],\n",
      "        [140.5452]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.963575541973114 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009648245759308338\n",
      "Epoch: 991 \n",
      " Hypothesis: tensor([[152.3364],\n",
      "        [183.9976],\n",
      "        [180.8294],\n",
      "        [196.9463],\n",
      "        [140.5453]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9635394811630249 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009648383595049381\n",
      "Epoch: 992 \n",
      " Hypothesis: tensor([[152.3364],\n",
      "        [183.9976],\n",
      "        [180.8294],\n",
      "        [196.9462],\n",
      "        [140.5453]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9635037183761597 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009648521430790424\n",
      "Epoch: 993 \n",
      " Hypothesis: tensor([[152.3363],\n",
      "        [183.9976],\n",
      "        [180.8294],\n",
      "        [196.9462],\n",
      "        [140.5453]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9634727239608765 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009648659266531467\n",
      "Epoch: 994 \n",
      " Hypothesis: tensor([[152.3363],\n",
      "        [183.9976],\n",
      "        [180.8294],\n",
      "        [196.9462],\n",
      "        [140.5453]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9634278416633606 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00964879710227251\n",
      "Epoch: 995 \n",
      " Hypothesis: tensor([[152.3363],\n",
      "        [183.9976],\n",
      "        [180.8294],\n",
      "        [196.9462],\n",
      "        [140.5454]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9633981585502625 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009648934938013554\n",
      "Epoch: 996 \n",
      " Hypothesis: tensor([[152.3362],\n",
      "        [183.9977],\n",
      "        [180.8294],\n",
      "        [196.9462],\n",
      "        [140.5454]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.963358998298645 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009649072773754597\n",
      "Epoch: 997 \n",
      " Hypothesis: tensor([[152.3362],\n",
      "        [183.9977],\n",
      "        [180.8294],\n",
      "        [196.9462],\n",
      "        [140.5454]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9633191227912903 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00964921060949564\n",
      "Epoch: 998 \n",
      " Hypothesis: tensor([[152.3362],\n",
      "        [183.9977],\n",
      "        [180.8294],\n",
      "        [196.9462],\n",
      "        [140.5455]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9632971882820129 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009649348445236683\n",
      "Epoch: 999 \n",
      " Hypothesis: tensor([[152.3362],\n",
      "        [183.9977],\n",
      "        [180.8294],\n",
      "        [196.9462],\n",
      "        [140.5455]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9632543325424194 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009649486280977726\n",
      "Epoch: 1000 \n",
      " Hypothesis: tensor([[152.3361],\n",
      "        [183.9977],\n",
      "        [180.8293],\n",
      "        [196.9462],\n",
      "        [140.5455]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9632221460342407 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009649624116718769\n",
      "Epoch: 1001 \n",
      " Hypothesis: tensor([[152.3361],\n",
      "        [183.9977],\n",
      "        [180.8293],\n",
      "        [196.9462],\n",
      "        [140.5455]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9631952047348022 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009649761952459812\n",
      "Epoch: 1002 \n",
      " Hypothesis: tensor([[152.3361],\n",
      "        [183.9978],\n",
      "        [180.8293],\n",
      "        [196.9462],\n",
      "        [140.5456]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9631552696228027 \n",
      " W: tensor([[0.7306],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009649899788200855\n",
      "Epoch: 1003 \n",
      " Hypothesis: tensor([[152.3360],\n",
      "        [183.9978],\n",
      "        [180.8293],\n",
      "        [196.9461],\n",
      "        [140.5456]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9631103277206421 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009650037623941898\n",
      "Epoch: 1004 \n",
      " Hypothesis: tensor([[152.3360],\n",
      "        [183.9978],\n",
      "        [180.8293],\n",
      "        [196.9461],\n",
      "        [140.5456]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9630843997001648 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009650175459682941\n",
      "Epoch: 1005 \n",
      " Hypothesis: tensor([[152.3360],\n",
      "        [183.9978],\n",
      "        [180.8293],\n",
      "        [196.9461],\n",
      "        [140.5457]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9630377888679504 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009650313295423985\n",
      "Epoch: 1006 \n",
      " Hypothesis: tensor([[152.3359],\n",
      "        [183.9978],\n",
      "        [180.8293],\n",
      "        [196.9461],\n",
      "        [140.5457]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9630047678947449 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009650451131165028\n",
      "Epoch: 1007 \n",
      " Hypothesis: tensor([[152.3359],\n",
      "        [183.9979],\n",
      "        [180.8293],\n",
      "        [196.9461],\n",
      "        [140.5457]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9629707336425781 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00965058896690607\n",
      "Epoch: 1008 \n",
      " Hypothesis: tensor([[152.3359],\n",
      "        [183.9979],\n",
      "        [180.8293],\n",
      "        [196.9461],\n",
      "        [140.5457]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9629260897636414 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5990],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009650726802647114\n",
      "Epoch: 1009 \n",
      " Hypothesis: tensor([[152.3358],\n",
      "        [183.9979],\n",
      "        [180.8293],\n",
      "        [196.9461],\n",
      "        [140.5458]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9628957509994507 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009650864638388157\n",
      "Epoch: 1010 \n",
      " Hypothesis: tensor([[152.3358],\n",
      "        [183.9979],\n",
      "        [180.8293],\n",
      "        [196.9461],\n",
      "        [140.5458]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9628589749336243 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.0096510024741292\n",
      "Epoch: 1011 \n",
      " Hypothesis: tensor([[152.3358],\n",
      "        [183.9980],\n",
      "        [180.8292],\n",
      "        [196.9461],\n",
      "        [140.5458]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9628239870071411 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009651140309870243\n",
      "Epoch: 1012 \n",
      " Hypothesis: tensor([[152.3358],\n",
      "        [183.9980],\n",
      "        [180.8292],\n",
      "        [196.9460],\n",
      "        [140.5459]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9627841114997864 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009651278145611286\n",
      "Epoch: 1013 \n",
      " Hypothesis: tensor([[152.3357],\n",
      "        [183.9980],\n",
      "        [180.8292],\n",
      "        [196.9461],\n",
      "        [140.5459]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9627490043640137 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00965141598135233\n",
      "Epoch: 1014 \n",
      " Hypothesis: tensor([[152.3357],\n",
      "        [183.9980],\n",
      "        [180.8292],\n",
      "        [196.9460],\n",
      "        [140.5459]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9627081751823425 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009651553817093372\n",
      "Epoch: 1015 \n",
      " Hypothesis: tensor([[152.3357],\n",
      "        [183.9980],\n",
      "        [180.8292],\n",
      "        [196.9460],\n",
      "        [140.5459]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9626854062080383 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009651691652834415\n",
      "Epoch: 1016 \n",
      " Hypothesis: tensor([[152.3356],\n",
      "        [183.9981],\n",
      "        [180.8292],\n",
      "        [196.9460],\n",
      "        [140.5460]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9626394510269165 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009651829488575459\n",
      "Epoch: 1017 \n",
      " Hypothesis: tensor([[152.3356],\n",
      "        [183.9981],\n",
      "        [180.8292],\n",
      "        [196.9460],\n",
      "        [140.5460]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9626056551933289 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009651967324316502\n",
      "Epoch: 1018 \n",
      " Hypothesis: tensor([[152.3356],\n",
      "        [183.9981],\n",
      "        [180.8292],\n",
      "        [196.9460],\n",
      "        [140.5460]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9625717401504517 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009652105160057545\n",
      "Epoch: 1019 \n",
      " Hypothesis: tensor([[152.3355],\n",
      "        [183.9981],\n",
      "        [180.8292],\n",
      "        [196.9460],\n",
      "        [140.5461]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9625387191772461 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009652242995798588\n",
      "Epoch: 1020 \n",
      " Hypothesis: tensor([[152.3355],\n",
      "        [183.9981],\n",
      "        [180.8291],\n",
      "        [196.9460],\n",
      "        [140.5461]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9625127911567688 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009652380831539631\n",
      "Epoch: 1021 \n",
      " Hypothesis: tensor([[152.3355],\n",
      "        [183.9982],\n",
      "        [180.8291],\n",
      "        [196.9460],\n",
      "        [140.5461]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9624727964401245 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009652518667280674\n",
      "Epoch: 1022 \n",
      " Hypothesis: tensor([[152.3354],\n",
      "        [183.9982],\n",
      "        [180.8291],\n",
      "        [196.9460],\n",
      "        [140.5461]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9624358415603638 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009652656503021717\n",
      "Epoch: 1023 \n",
      " Hypothesis: tensor([[152.3354],\n",
      "        [183.9982],\n",
      "        [180.8291],\n",
      "        [196.9459],\n",
      "        [140.5462]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9623980522155762 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00965279433876276\n",
      "Epoch: 1024 \n",
      " Hypothesis: tensor([[152.3354],\n",
      "        [183.9982],\n",
      "        [180.8291],\n",
      "        [196.9459],\n",
      "        [140.5462]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.962361216545105 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009652932174503803\n",
      "Epoch: 1025 \n",
      " Hypothesis: tensor([[152.3354],\n",
      "        [183.9982],\n",
      "        [180.8291],\n",
      "        [196.9459],\n",
      "        [140.5462]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9623241424560547 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009653070010244846\n",
      "Epoch: 1026 \n",
      " Hypothesis: tensor([[152.3353],\n",
      "        [183.9982],\n",
      "        [180.8291],\n",
      "        [196.9459],\n",
      "        [140.5462]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9622925519943237 \n",
      " W: tensor([[0.7307],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.00965320784598589\n",
      "Epoch: 1027 \n",
      " Hypothesis: tensor([[152.3353],\n",
      "        [183.9983],\n",
      "        [180.8291],\n",
      "        [196.9459],\n",
      "        [140.5463]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9622584581375122 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009653345681726933\n",
      "Epoch: 1028 \n",
      " Hypothesis: tensor([[152.3353],\n",
      "        [183.9983],\n",
      "        [180.8291],\n",
      "        [196.9459],\n",
      "        [140.5463]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9622214436531067 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009653483517467976\n",
      "Epoch: 1029 \n",
      " Hypothesis: tensor([[152.3352],\n",
      "        [183.9983],\n",
      "        [180.8291],\n",
      "        [196.9459],\n",
      "        [140.5463]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9621806144714355 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009653621353209019\n",
      "Epoch: 1030 \n",
      " Hypothesis: tensor([[152.3352],\n",
      "        [183.9983],\n",
      "        [180.8291],\n",
      "        [196.9459],\n",
      "        [140.5464]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9621526598930359 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009653759188950062\n",
      "Epoch: 1031 \n",
      " Hypothesis: tensor([[152.3352],\n",
      "        [183.9984],\n",
      "        [180.8290],\n",
      "        [196.9459],\n",
      "        [140.5464]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.96210777759552 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5989],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009653897024691105\n",
      "Epoch: 1032 \n",
      " Hypothesis: tensor([[152.3351],\n",
      "        [183.9984],\n",
      "        [180.8290],\n",
      "        [196.9459],\n",
      "        [140.5464]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9620798230171204 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009654034860432148\n",
      "Epoch: 1033 \n",
      " Hypothesis: tensor([[152.3351],\n",
      "        [183.9984],\n",
      "        [180.8290],\n",
      "        [196.9459],\n",
      "        [140.5465]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9620468020439148 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6809]], requires_grad=True) \n",
      " b: 0.009654172696173191\n",
      "Epoch: 1034 \n",
      " Hypothesis: tensor([[152.3351],\n",
      "        [183.9984],\n",
      "        [180.8290],\n",
      "        [196.9459],\n",
      "        [140.5465]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.962009072303772 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009654310531914234\n",
      "Epoch: 1035 \n",
      " Hypothesis: tensor([[152.3350],\n",
      "        [183.9985],\n",
      "        [180.8290],\n",
      "        [196.9458],\n",
      "        [140.5465]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9619658589363098 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009654448367655277\n",
      "Epoch: 1036 \n",
      " Hypothesis: tensor([[152.3350],\n",
      "        [183.9985],\n",
      "        [180.8290],\n",
      "        [196.9458],\n",
      "        [140.5465]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9619321823120117 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00965458620339632\n",
      "Epoch: 1037 \n",
      " Hypothesis: tensor([[152.3350],\n",
      "        [183.9985],\n",
      "        [180.8290],\n",
      "        [196.9458],\n",
      "        [140.5466]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9618894457817078 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009654724039137363\n",
      "Epoch: 1038 \n",
      " Hypothesis: tensor([[152.3349],\n",
      "        [183.9985],\n",
      "        [180.8290],\n",
      "        [196.9458],\n",
      "        [140.5466]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.961856484413147 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009654861874878407\n",
      "Epoch: 1039 \n",
      " Hypothesis: tensor([[152.3349],\n",
      "        [183.9985],\n",
      "        [180.8290],\n",
      "        [196.9458],\n",
      "        [140.5466]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9618194699287415 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00965499971061945\n",
      "Epoch: 1040 \n",
      " Hypothesis: tensor([[152.3349],\n",
      "        [183.9986],\n",
      "        [180.8290],\n",
      "        [196.9458],\n",
      "        [140.5467]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9617868661880493 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009655137546360493\n",
      "Epoch: 1041 \n",
      " Hypothesis: tensor([[152.3349],\n",
      "        [183.9986],\n",
      "        [180.8289],\n",
      "        [196.9458],\n",
      "        [140.5467]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9617487192153931 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009655275382101536\n",
      "Epoch: 1042 \n",
      " Hypothesis: tensor([[152.3348],\n",
      "        [183.9986],\n",
      "        [180.8289],\n",
      "        [196.9458],\n",
      "        [140.5467]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9617208242416382 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009655413217842579\n",
      "Epoch: 1043 \n",
      " Hypothesis: tensor([[152.3348],\n",
      "        [183.9986],\n",
      "        [180.8289],\n",
      "        [196.9458],\n",
      "        [140.5468]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9616810083389282 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009655551053583622\n",
      "Epoch: 1044 \n",
      " Hypothesis: tensor([[152.3348],\n",
      "        [183.9986],\n",
      "        [180.8289],\n",
      "        [196.9458],\n",
      "        [140.5468]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9616449475288391 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009655688889324665\n",
      "Epoch: 1045 \n",
      " Hypothesis: tensor([[152.3347],\n",
      "        [183.9987],\n",
      "        [180.8289],\n",
      "        [196.9458],\n",
      "        [140.5468]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9616119265556335 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009655826725065708\n",
      "Epoch: 1046 \n",
      " Hypothesis: tensor([[152.3347],\n",
      "        [183.9987],\n",
      "        [180.8289],\n",
      "        [196.9458],\n",
      "        [140.5468]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9615839719772339 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009655964560806751\n",
      "Epoch: 1047 \n",
      " Hypothesis: tensor([[152.3347],\n",
      "        [183.9987],\n",
      "        [180.8289],\n",
      "        [196.9458],\n",
      "        [140.5469]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9615510702133179 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009656102396547794\n",
      "Epoch: 1048 \n",
      " Hypothesis: tensor([[152.3346],\n",
      "        [183.9987],\n",
      "        [180.8289],\n",
      "        [196.9457],\n",
      "        [140.5469]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9615007638931274 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009656240232288837\n",
      "Epoch: 1049 \n",
      " Hypothesis: tensor([[152.3346],\n",
      "        [183.9987],\n",
      "        [180.8289],\n",
      "        [196.9457],\n",
      "        [140.5469]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9614558219909668 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00965637806802988\n",
      "Epoch: 1050 \n",
      " Hypothesis: tensor([[152.3346],\n",
      "        [183.9987],\n",
      "        [180.8289],\n",
      "        [196.9457],\n",
      "        [140.5470]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9614279866218567 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009656515903770924\n",
      "Epoch: 1051 \n",
      " Hypothesis: tensor([[152.3345],\n",
      "        [183.9988],\n",
      "        [180.8288],\n",
      "        [196.9457],\n",
      "        [140.5470]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9613887667655945 \n",
      " W: tensor([[0.7308],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009656653739511967\n",
      "Epoch: 1052 \n",
      " Hypothesis: tensor([[152.3345],\n",
      "        [183.9988],\n",
      "        [180.8288],\n",
      "        [196.9457],\n",
      "        [140.5470]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9613608121871948 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00965679157525301\n",
      "Epoch: 1053 \n",
      " Hypothesis: tensor([[152.3345],\n",
      "        [183.9988],\n",
      "        [180.8288],\n",
      "        [196.9457],\n",
      "        [140.5470]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9613119959831238 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009656929410994053\n",
      "Epoch: 1054 \n",
      " Hypothesis: tensor([[152.3344],\n",
      "        [183.9988],\n",
      "        [180.8288],\n",
      "        [196.9457],\n",
      "        [140.5471]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.961290180683136 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5988],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009657067246735096\n",
      "Epoch: 1055 \n",
      " Hypothesis: tensor([[152.3344],\n",
      "        [183.9989],\n",
      "        [180.8288],\n",
      "        [196.9457],\n",
      "        [140.5471]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9612531661987305 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009657205082476139\n",
      "Epoch: 1056 \n",
      " Hypothesis: tensor([[152.3344],\n",
      "        [183.9989],\n",
      "        [180.8288],\n",
      "        [196.9457],\n",
      "        [140.5471]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9612191319465637 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009657342918217182\n",
      "Epoch: 1057 \n",
      " Hypothesis: tensor([[152.3344],\n",
      "        [183.9989],\n",
      "        [180.8288],\n",
      "        [196.9456],\n",
      "        [140.5471]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.961186408996582 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009657480753958225\n",
      "Epoch: 1058 \n",
      " Hypothesis: tensor([[152.3343],\n",
      "        [183.9989],\n",
      "        [180.8288],\n",
      "        [196.9456],\n",
      "        [140.5472]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9611466526985168 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009657618589699268\n",
      "Epoch: 1059 \n",
      " Hypothesis: tensor([[152.3343],\n",
      "        [183.9989],\n",
      "        [180.8288],\n",
      "        [196.9456],\n",
      "        [140.5472]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9611197710037231 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009657756425440311\n",
      "Epoch: 1060 \n",
      " Hypothesis: tensor([[152.3343],\n",
      "        [183.9990],\n",
      "        [180.8288],\n",
      "        [196.9456],\n",
      "        [140.5472]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9610750079154968 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009657894261181355\n",
      "Epoch: 1061 \n",
      " Hypothesis: tensor([[152.3342],\n",
      "        [183.9990],\n",
      "        [180.8288],\n",
      "        [196.9456],\n",
      "        [140.5473]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9610351324081421 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009658032096922398\n",
      "Epoch: 1062 \n",
      " Hypothesis: tensor([[152.3342],\n",
      "        [183.9990],\n",
      "        [180.8288],\n",
      "        [196.9456],\n",
      "        [140.5473]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9610010981559753 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00965816993266344\n",
      "Epoch: 1063 \n",
      " Hypothesis: tensor([[152.3342],\n",
      "        [183.9990],\n",
      "        [180.8287],\n",
      "        [196.9456],\n",
      "        [140.5473]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9609645009040833 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009658307768404484\n",
      "Epoch: 1064 \n",
      " Hypothesis: tensor([[152.3342],\n",
      "        [183.9990],\n",
      "        [180.8287],\n",
      "        [196.9456],\n",
      "        [140.5473]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9609324336051941 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009658445604145527\n",
      "Epoch: 1065 \n",
      " Hypothesis: tensor([[152.3341],\n",
      "        [183.9991],\n",
      "        [180.8287],\n",
      "        [196.9456],\n",
      "        [140.5474]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.960893452167511 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00965858343988657\n",
      "Epoch: 1066 \n",
      " Hypothesis: tensor([[152.3341],\n",
      "        [183.9991],\n",
      "        [180.8287],\n",
      "        [196.9456],\n",
      "        [140.5474]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9608576893806458 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009658721275627613\n",
      "Epoch: 1067 \n",
      " Hypothesis: tensor([[152.3341],\n",
      "        [183.9991],\n",
      "        [180.8287],\n",
      "        [196.9455],\n",
      "        [140.5474]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.96082603931427 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009658859111368656\n",
      "Epoch: 1068 \n",
      " Hypothesis: tensor([[152.3340],\n",
      "        [183.9991],\n",
      "        [180.8287],\n",
      "        [196.9455],\n",
      "        [140.5475]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9607928991317749 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.0096589969471097\n",
      "Epoch: 1069 \n",
      " Hypothesis: tensor([[152.3340],\n",
      "        [183.9991],\n",
      "        [180.8287],\n",
      "        [196.9455],\n",
      "        [140.5475]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9607569575309753 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009659134782850742\n",
      "Epoch: 1070 \n",
      " Hypothesis: tensor([[152.3340],\n",
      "        [183.9992],\n",
      "        [180.8287],\n",
      "        [196.9455],\n",
      "        [140.5475]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9607189297676086 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009659272618591785\n",
      "Epoch: 1071 \n",
      " Hypothesis: tensor([[152.3339],\n",
      "        [183.9992],\n",
      "        [180.8286],\n",
      "        [196.9455],\n",
      "        [140.5475]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9606832265853882 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009659410454332829\n",
      "Epoch: 1072 \n",
      " Hypothesis: tensor([[152.3339],\n",
      "        [183.9992],\n",
      "        [180.8286],\n",
      "        [196.9455],\n",
      "        [140.5476]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9606491923332214 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009659548290073872\n",
      "Epoch: 1073 \n",
      " Hypothesis: tensor([[152.3339],\n",
      "        [183.9992],\n",
      "        [180.8286],\n",
      "        [196.9455],\n",
      "        [140.5476]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9606125950813293 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009659686125814915\n",
      "Epoch: 1074 \n",
      " Hypothesis: tensor([[152.3338],\n",
      "        [183.9992],\n",
      "        [180.8286],\n",
      "        [196.9455],\n",
      "        [140.5476]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9605768322944641 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009659823961555958\n",
      "Epoch: 1075 \n",
      " Hypothesis: tensor([[152.3338],\n",
      "        [183.9993],\n",
      "        [180.8286],\n",
      "        [196.9455],\n",
      "        [140.5477]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9605427980422974 \n",
      " W: tensor([[0.7309],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009659961797297001\n",
      "Epoch: 1076 \n",
      " Hypothesis: tensor([[152.3338],\n",
      "        [183.9993],\n",
      "        [180.8286],\n",
      "        [196.9454],\n",
      "        [140.5477]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.960493266582489 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009660099633038044\n",
      "Epoch: 1077 \n",
      " Hypothesis: tensor([[152.3337],\n",
      "        [183.9993],\n",
      "        [180.8286],\n",
      "        [196.9454],\n",
      "        [140.5477]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9604591131210327 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5987],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009660237468779087\n",
      "Epoch: 1078 \n",
      " Hypothesis: tensor([[152.3337],\n",
      "        [183.9993],\n",
      "        [180.8286],\n",
      "        [196.9454],\n",
      "        [140.5477]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9604350924491882 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966037530452013\n",
      "Epoch: 1079 \n",
      " Hypothesis: tensor([[152.3337],\n",
      "        [183.9993],\n",
      "        [180.8286],\n",
      "        [196.9454],\n",
      "        [140.5478]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9604021310806274 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009660513140261173\n",
      "Epoch: 1080 \n",
      " Hypothesis: tensor([[152.3336],\n",
      "        [183.9994],\n",
      "        [180.8286],\n",
      "        [196.9454],\n",
      "        [140.5478]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.960362434387207 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009660650976002216\n",
      "Epoch: 1081 \n",
      " Hypothesis: tensor([[152.3336],\n",
      "        [183.9994],\n",
      "        [180.8286],\n",
      "        [196.9454],\n",
      "        [140.5478]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9603286981582642 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966078881174326\n",
      "Epoch: 1082 \n",
      " Hypothesis: tensor([[152.3336],\n",
      "        [183.9994],\n",
      "        [180.8286],\n",
      "        [196.9454],\n",
      "        [140.5479]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.96028071641922 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009660926647484303\n",
      "Epoch: 1083 \n",
      " Hypothesis: tensor([[152.3336],\n",
      "        [183.9994],\n",
      "        [180.8286],\n",
      "        [196.9454],\n",
      "        [140.5479]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9602617025375366 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009661064483225346\n",
      "Epoch: 1084 \n",
      " Hypothesis: tensor([[152.3335],\n",
      "        [183.9994],\n",
      "        [180.8285],\n",
      "        [196.9454],\n",
      "        [140.5479]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.960222065448761 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009661202318966389\n",
      "Epoch: 1085 \n",
      " Hypothesis: tensor([[152.3335],\n",
      "        [183.9995],\n",
      "        [180.8285],\n",
      "        [196.9454],\n",
      "        [140.5480]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9601770639419556 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009661340154707432\n",
      "Epoch: 1086 \n",
      " Hypothesis: tensor([[152.3335],\n",
      "        [183.9995],\n",
      "        [180.8285],\n",
      "        [196.9454],\n",
      "        [140.5480]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9601451754570007 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009661477990448475\n",
      "Epoch: 1087 \n",
      " Hypothesis: tensor([[152.3334],\n",
      "        [183.9995],\n",
      "        [180.8285],\n",
      "        [196.9454],\n",
      "        [140.5480]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.960113525390625 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009661615826189518\n",
      "Epoch: 1088 \n",
      " Hypothesis: tensor([[152.3334],\n",
      "        [183.9995],\n",
      "        [180.8285],\n",
      "        [196.9454],\n",
      "        [140.5480]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.960079550743103 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009661753661930561\n",
      "Epoch: 1089 \n",
      " Hypothesis: tensor([[152.3334],\n",
      "        [183.9996],\n",
      "        [180.8285],\n",
      "        [196.9454],\n",
      "        [140.5481]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9600425958633423 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009661891497671604\n",
      "Epoch: 1090 \n",
      " Hypothesis: tensor([[152.3333],\n",
      "        [183.9996],\n",
      "        [180.8285],\n",
      "        [196.9453],\n",
      "        [140.5481]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9599922299385071 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009662029333412647\n",
      "Epoch: 1091 \n",
      " Hypothesis: tensor([[152.3333],\n",
      "        [183.9996],\n",
      "        [180.8285],\n",
      "        [196.9453],\n",
      "        [140.5481]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9599682092666626 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966216716915369\n",
      "Epoch: 1092 \n",
      " Hypothesis: tensor([[152.3333],\n",
      "        [183.9996],\n",
      "        [180.8284],\n",
      "        [196.9453],\n",
      "        [140.5482]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9599311947822571 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009662305004894733\n",
      "Epoch: 1093 \n",
      " Hypothesis: tensor([[152.3332],\n",
      "        [183.9996],\n",
      "        [180.8284],\n",
      "        [196.9453],\n",
      "        [140.5482]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9598954319953918 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009662442840635777\n",
      "Epoch: 1094 \n",
      " Hypothesis: tensor([[152.3332],\n",
      "        [183.9996],\n",
      "        [180.8284],\n",
      "        [196.9453],\n",
      "        [140.5482]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9598624110221863 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966258067637682\n",
      "Epoch: 1095 \n",
      " Hypothesis: tensor([[152.3332],\n",
      "        [183.9997],\n",
      "        [180.8284],\n",
      "        [196.9453],\n",
      "        [140.5482]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9598196148872375 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009662718512117863\n",
      "Epoch: 1096 \n",
      " Hypothesis: tensor([[152.3331],\n",
      "        [183.9997],\n",
      "        [180.8284],\n",
      "        [196.9453],\n",
      "        [140.5483]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9597986340522766 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009662856347858906\n",
      "Epoch: 1097 \n",
      " Hypothesis: tensor([[152.3331],\n",
      "        [183.9997],\n",
      "        [180.8284],\n",
      "        [196.9453],\n",
      "        [140.5483]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.959765613079071 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009662994183599949\n",
      "Epoch: 1098 \n",
      " Hypothesis: tensor([[152.3331],\n",
      "        [183.9997],\n",
      "        [180.8284],\n",
      "        [196.9453],\n",
      "        [140.5483]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9597221612930298 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009663132019340992\n",
      "Epoch: 1099 \n",
      " Hypothesis: tensor([[152.3331],\n",
      "        [183.9998],\n",
      "        [180.8284],\n",
      "        [196.9453],\n",
      "        [140.5484]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9596871137619019 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009663269855082035\n",
      "Epoch: 1100 \n",
      " Hypothesis: tensor([[152.3330],\n",
      "        [183.9998],\n",
      "        [180.8284],\n",
      "        [196.9453],\n",
      "        [140.5484]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9596424102783203 \n",
      " W: tensor([[0.7310],\n",
      "        [0.5986],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009663407690823078\n",
      "Epoch: 1101 \n",
      " Hypothesis: tensor([[152.3330],\n",
      "        [183.9998],\n",
      "        [180.8284],\n",
      "        [196.9452],\n",
      "        [140.5484]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9596086740493774 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009663545526564121\n",
      "Epoch: 1102 \n",
      " Hypothesis: tensor([[152.3330],\n",
      "        [183.9998],\n",
      "        [180.8284],\n",
      "        [196.9452],\n",
      "        [140.5484]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9595756530761719 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009663683362305164\n",
      "Epoch: 1103 \n",
      " Hypothesis: tensor([[152.3329],\n",
      "        [183.9998],\n",
      "        [180.8284],\n",
      "        [196.9452],\n",
      "        [140.5485]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9595379829406738 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009663821198046207\n",
      "Epoch: 1104 \n",
      " Hypothesis: tensor([[152.3329],\n",
      "        [183.9999],\n",
      "        [180.8283],\n",
      "        [196.9452],\n",
      "        [140.5485]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9594972729682922 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966395903378725\n",
      "Epoch: 1105 \n",
      " Hypothesis: tensor([[152.3329],\n",
      "        [183.9999],\n",
      "        [180.8283],\n",
      "        [196.9452],\n",
      "        [140.5485]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9594731330871582 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009664096869528294\n",
      "Epoch: 1106 \n",
      " Hypothesis: tensor([[152.3328],\n",
      "        [183.9999],\n",
      "        [180.8283],\n",
      "        [196.9452],\n",
      "        [140.5486]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9594364166259766 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009664234705269337\n",
      "Epoch: 1107 \n",
      " Hypothesis: tensor([[152.3328],\n",
      "        [183.9999],\n",
      "        [180.8283],\n",
      "        [196.9452],\n",
      "        [140.5486]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.959400475025177 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966437254101038\n",
      "Epoch: 1108 \n",
      " Hypothesis: tensor([[152.3328],\n",
      "        [183.9999],\n",
      "        [180.8283],\n",
      "        [196.9452],\n",
      "        [140.5486]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9593618512153625 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009664510376751423\n",
      "Epoch: 1109 \n",
      " Hypothesis: tensor([[152.3327],\n",
      "        [184.0000],\n",
      "        [180.8283],\n",
      "        [196.9452],\n",
      "        [140.5486]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9593338966369629 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009664648212492466\n",
      "Epoch: 1110 \n",
      " Hypothesis: tensor([[152.3327],\n",
      "        [184.0000],\n",
      "        [180.8283],\n",
      "        [196.9452],\n",
      "        [140.5487]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9593009948730469 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009664786048233509\n",
      "Epoch: 1111 \n",
      " Hypothesis: tensor([[152.3327],\n",
      "        [184.0000],\n",
      "        [180.8283],\n",
      "        [196.9452],\n",
      "        [140.5487]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9592608213424683 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009664923883974552\n",
      "Epoch: 1112 \n",
      " Hypothesis: tensor([[152.3327],\n",
      "        [184.0000],\n",
      "        [180.8283],\n",
      "        [196.9451],\n",
      "        [140.5487]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9592313766479492 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009665061719715595\n",
      "Epoch: 1113 \n",
      " Hypothesis: tensor([[152.3326],\n",
      "        [184.0000],\n",
      "        [180.8282],\n",
      "        [196.9451],\n",
      "        [140.5488]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9591922760009766 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009665199555456638\n",
      "Epoch: 1114 \n",
      " Hypothesis: tensor([[152.3326],\n",
      "        [184.0001],\n",
      "        [180.8282],\n",
      "        [196.9451],\n",
      "        [140.5488]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9591478109359741 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009665337391197681\n",
      "Epoch: 1115 \n",
      " Hypothesis: tensor([[152.3326],\n",
      "        [184.0001],\n",
      "        [180.8282],\n",
      "        [196.9451],\n",
      "        [140.5488]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9591138958930969 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009665475226938725\n",
      "Epoch: 1116 \n",
      " Hypothesis: tensor([[152.3325],\n",
      "        [184.0001],\n",
      "        [180.8282],\n",
      "        [196.9451],\n",
      "        [140.5488]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9590812921524048 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009665613062679768\n",
      "Epoch: 1117 \n",
      " Hypothesis: tensor([[152.3325],\n",
      "        [184.0001],\n",
      "        [180.8282],\n",
      "        [196.9451],\n",
      "        [140.5489]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9590405225753784 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966575089842081\n",
      "Epoch: 1118 \n",
      " Hypothesis: tensor([[152.3325],\n",
      "        [184.0001],\n",
      "        [180.8282],\n",
      "        [196.9451],\n",
      "        [140.5489]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9590015411376953 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009665888734161854\n",
      "Epoch: 1119 \n",
      " Hypothesis: tensor([[152.3324],\n",
      "        [184.0002],\n",
      "        [180.8282],\n",
      "        [196.9451],\n",
      "        [140.5489]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9589695930480957 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009666026569902897\n",
      "Epoch: 1120 \n",
      " Hypothesis: tensor([[152.3324],\n",
      "        [184.0002],\n",
      "        [180.8282],\n",
      "        [196.9451],\n",
      "        [140.5490]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9589454531669617 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966616440564394\n",
      "Epoch: 1121 \n",
      " Hypothesis: tensor([[152.3324],\n",
      "        [184.0002],\n",
      "        [180.8282],\n",
      "        [196.9451],\n",
      "        [140.5490]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9589118957519531 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009666302241384983\n",
      "Epoch: 1122 \n",
      " Hypothesis: tensor([[152.3324],\n",
      "        [184.0002],\n",
      "        [180.8282],\n",
      "        [196.9451],\n",
      "        [140.5490]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9588632583618164 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009666440077126026\n",
      "Epoch: 1123 \n",
      " Hypothesis: tensor([[152.3323],\n",
      "        [184.0002],\n",
      "        [180.8281],\n",
      "        [196.9450],\n",
      "        [140.5490]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.958829402923584 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5985],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966657791286707\n",
      "Epoch: 1124 \n",
      " Hypothesis: tensor([[152.3323],\n",
      "        [184.0003],\n",
      "        [180.8281],\n",
      "        [196.9450],\n",
      "        [140.5491]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9587944149971008 \n",
      " W: tensor([[0.7311],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009666715748608112\n",
      "Epoch: 1125 \n",
      " Hypothesis: tensor([[152.3322],\n",
      "        [184.0003],\n",
      "        [180.8281],\n",
      "        [196.9450],\n",
      "        [140.5491]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9587498903274536 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009666853584349155\n",
      "Epoch: 1126 \n",
      " Hypothesis: tensor([[152.3322],\n",
      "        [184.0003],\n",
      "        [180.8281],\n",
      "        [196.9450],\n",
      "        [140.5491]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9587258100509644 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009666991420090199\n",
      "Epoch: 1127 \n",
      " Hypothesis: tensor([[152.3322],\n",
      "        [184.0003],\n",
      "        [180.8281],\n",
      "        [196.9450],\n",
      "        [140.5492]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9586830139160156 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009667129255831242\n",
      "Epoch: 1128 \n",
      " Hypothesis: tensor([[152.3322],\n",
      "        [184.0003],\n",
      "        [180.8281],\n",
      "        [196.9450],\n",
      "        [140.5492]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9586485028266907 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009667267091572285\n",
      "Epoch: 1129 \n",
      " Hypothesis: tensor([[152.3321],\n",
      "        [184.0004],\n",
      "        [180.8281],\n",
      "        [196.9450],\n",
      "        [140.5492]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9586086273193359 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009667404927313328\n",
      "Epoch: 1130 \n",
      " Hypothesis: tensor([[152.3321],\n",
      "        [184.0004],\n",
      "        [180.8281],\n",
      "        [196.9450],\n",
      "        [140.5493]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9585697054862976 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009667542763054371\n",
      "Epoch: 1131 \n",
      " Hypothesis: tensor([[152.3321],\n",
      "        [184.0004],\n",
      "        [180.8281],\n",
      "        [196.9450],\n",
      "        [140.5493]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9585506319999695 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009667680598795414\n",
      "Epoch: 1132 \n",
      " Hypothesis: tensor([[152.3320],\n",
      "        [184.0004],\n",
      "        [180.8281],\n",
      "        [196.9450],\n",
      "        [140.5493]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9585140347480774 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009667818434536457\n",
      "Epoch: 1133 \n",
      " Hypothesis: tensor([[152.3320],\n",
      "        [184.0005],\n",
      "        [180.8281],\n",
      "        [196.9449],\n",
      "        [140.5493]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9584654569625854 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.0096679562702775\n",
      "Epoch: 1134 \n",
      " Hypothesis: tensor([[152.3320],\n",
      "        [184.0005],\n",
      "        [180.8280],\n",
      "        [196.9449],\n",
      "        [140.5494]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9584342837333679 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009668094106018543\n",
      "Epoch: 1135 \n",
      " Hypothesis: tensor([[152.3319],\n",
      "        [184.0005],\n",
      "        [180.8280],\n",
      "        [196.9449],\n",
      "        [140.5494]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9584006071090698 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009668231941759586\n",
      "Epoch: 1136 \n",
      " Hypothesis: tensor([[152.3319],\n",
      "        [184.0005],\n",
      "        [180.8280],\n",
      "        [196.9449],\n",
      "        [140.5494]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9583608508110046 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966836977750063\n",
      "Epoch: 1137 \n",
      " Hypothesis: tensor([[152.3319],\n",
      "        [184.0005],\n",
      "        [180.8280],\n",
      "        [196.9449],\n",
      "        [140.5495]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9583218693733215 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009668507613241673\n",
      "Epoch: 1138 \n",
      " Hypothesis: tensor([[152.3318],\n",
      "        [184.0005],\n",
      "        [180.8280],\n",
      "        [196.9449],\n",
      "        [140.5495]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9582950472831726 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009668645448982716\n",
      "Epoch: 1139 \n",
      " Hypothesis: tensor([[152.3318],\n",
      "        [184.0006],\n",
      "        [180.8280],\n",
      "        [196.9449],\n",
      "        [140.5495]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.958255410194397 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009668783284723759\n",
      "Epoch: 1140 \n",
      " Hypothesis: tensor([[152.3318],\n",
      "        [184.0006],\n",
      "        [180.8280],\n",
      "        [196.9449],\n",
      "        [140.5495]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9582316279411316 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009668921120464802\n",
      "Epoch: 1141 \n",
      " Hypothesis: tensor([[152.3318],\n",
      "        [184.0006],\n",
      "        [180.8280],\n",
      "        [196.9449],\n",
      "        [140.5496]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9581868052482605 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009669058956205845\n",
      "Epoch: 1142 \n",
      " Hypothesis: tensor([[152.3317],\n",
      "        [184.0006],\n",
      "        [180.8280],\n",
      "        [196.9449],\n",
      "        [140.5496]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9581528902053833 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009669196791946888\n",
      "Epoch: 1143 \n",
      " Hypothesis: tensor([[152.3317],\n",
      "        [184.0006],\n",
      "        [180.8280],\n",
      "        [196.9449],\n",
      "        [140.5496]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9581203460693359 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009669334627687931\n",
      "Epoch: 1144 \n",
      " Hypothesis: tensor([[152.3317],\n",
      "        [184.0007],\n",
      "        [180.8279],\n",
      "        [196.9449],\n",
      "        [140.5497]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9580813646316528 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009669472463428974\n",
      "Epoch: 1145 \n",
      " Hypothesis: tensor([[152.3316],\n",
      "        [184.0007],\n",
      "        [180.8279],\n",
      "        [196.9449],\n",
      "        [140.5497]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.958053469657898 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009669610299170017\n",
      "Epoch: 1146 \n",
      " Hypothesis: tensor([[152.3316],\n",
      "        [184.0007],\n",
      "        [180.8279],\n",
      "        [196.9448],\n",
      "        [140.5497]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9580029249191284 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5984],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00966974813491106\n",
      "Epoch: 1147 \n",
      " Hypothesis: tensor([[152.3316],\n",
      "        [184.0007],\n",
      "        [180.8279],\n",
      "        [196.9448],\n",
      "        [140.5497]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9579700231552124 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009669885970652103\n",
      "Epoch: 1148 \n",
      " Hypothesis: tensor([[152.3315],\n",
      "        [184.0008],\n",
      "        [180.8279],\n",
      "        [196.9448],\n",
      "        [140.5498]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9579391479492188 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009670023806393147\n",
      "Epoch: 1149 \n",
      " Hypothesis: tensor([[152.3315],\n",
      "        [184.0008],\n",
      "        [180.8279],\n",
      "        [196.9448],\n",
      "        [140.5498]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9579005241394043 \n",
      " W: tensor([[0.7312],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967016164213419\n",
      "Epoch: 1150 \n",
      " Hypothesis: tensor([[152.3315],\n",
      "        [184.0008],\n",
      "        [180.8279],\n",
      "        [196.9448],\n",
      "        [140.5498]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9578726887702942 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009670299477875233\n",
      "Epoch: 1151 \n",
      " Hypothesis: tensor([[152.3315],\n",
      "        [184.0008],\n",
      "        [180.8279],\n",
      "        [196.9448],\n",
      "        [140.5499]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9578336477279663 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009670437313616276\n",
      "Epoch: 1152 \n",
      " Hypothesis: tensor([[152.3314],\n",
      "        [184.0008],\n",
      "        [180.8279],\n",
      "        [196.9448],\n",
      "        [140.5499]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9577892422676086 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009670575149357319\n",
      "Epoch: 1153 \n",
      " Hypothesis: tensor([[152.3314],\n",
      "        [184.0009],\n",
      "        [180.8279],\n",
      "        [196.9448],\n",
      "        [140.5499]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9577611088752747 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009670712985098362\n",
      "Epoch: 1154 \n",
      " Hypothesis: tensor([[152.3314],\n",
      "        [184.0009],\n",
      "        [180.8279],\n",
      "        [196.9448],\n",
      "        [140.5499]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9577285051345825 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009670850820839405\n",
      "Epoch: 1155 \n",
      " Hypothesis: tensor([[152.3313],\n",
      "        [184.0009],\n",
      "        [180.8279],\n",
      "        [196.9448],\n",
      "        [140.5500]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9576972723007202 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009670988656580448\n",
      "Epoch: 1156 \n",
      " Hypothesis: tensor([[152.3313],\n",
      "        [184.0009],\n",
      "        [180.8278],\n",
      "        [196.9447],\n",
      "        [140.5500]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9576589465141296 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009671126492321491\n",
      "Epoch: 1157 \n",
      " Hypothesis: tensor([[152.3313],\n",
      "        [184.0009],\n",
      "        [180.8278],\n",
      "        [196.9447],\n",
      "        [140.5500]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9576219320297241 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009671264328062534\n",
      "Epoch: 1158 \n",
      " Hypothesis: tensor([[152.3312],\n",
      "        [184.0010],\n",
      "        [180.8278],\n",
      "        [196.9447],\n",
      "        [140.5501]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9575833082199097 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009671402163803577\n",
      "Epoch: 1159 \n",
      " Hypothesis: tensor([[152.3312],\n",
      "        [184.0010],\n",
      "        [180.8278],\n",
      "        [196.9447],\n",
      "        [140.5501]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9575592875480652 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967153999954462\n",
      "Epoch: 1160 \n",
      " Hypothesis: tensor([[152.3312],\n",
      "        [184.0010],\n",
      "        [180.8278],\n",
      "        [196.9447],\n",
      "        [140.5501]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9575087428092957 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009671677835285664\n",
      "Epoch: 1161 \n",
      " Hypothesis: tensor([[152.3311],\n",
      "        [184.0010],\n",
      "        [180.8278],\n",
      "        [196.9447],\n",
      "        [140.5501]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9574748277664185 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009671815671026707\n",
      "Epoch: 1162 \n",
      " Hypothesis: tensor([[152.3311],\n",
      "        [184.0011],\n",
      "        [180.8278],\n",
      "        [196.9447],\n",
      "        [140.5502]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.957440972328186 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967195350676775\n",
      "Epoch: 1163 \n",
      " Hypothesis: tensor([[152.3311],\n",
      "        [184.0011],\n",
      "        [180.8278],\n",
      "        [196.9447],\n",
      "        [140.5502]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9574025273323059 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009672091342508793\n",
      "Epoch: 1164 \n",
      " Hypothesis: tensor([[152.3311],\n",
      "        [184.0011],\n",
      "        [180.8278],\n",
      "        [196.9447],\n",
      "        [140.5502]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.957363486289978 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009672229178249836\n",
      "Epoch: 1165 \n",
      " Hypothesis: tensor([[152.3310],\n",
      "        [184.0011],\n",
      "        [180.8278],\n",
      "        [196.9447],\n",
      "        [140.5503]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9573356509208679 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009672367013990879\n",
      "Epoch: 1166 \n",
      " Hypothesis: tensor([[152.3310],\n",
      "        [184.0011],\n",
      "        [180.8277],\n",
      "        [196.9447],\n",
      "        [140.5503]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9573028683662415 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009672504849731922\n",
      "Epoch: 1167 \n",
      " Hypothesis: tensor([[152.3310],\n",
      "        [184.0012],\n",
      "        [180.8277],\n",
      "        [196.9447],\n",
      "        [140.5503]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9572650790214539 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009672642685472965\n",
      "Epoch: 1168 \n",
      " Hypothesis: tensor([[152.3309],\n",
      "        [184.0012],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5504]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.957226574420929 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009672780521214008\n",
      "Epoch: 1169 \n",
      " Hypothesis: tensor([[152.3309],\n",
      "        [184.0012],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5504]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9571905136108398 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009672918356955051\n",
      "Epoch: 1170 \n",
      " Hypothesis: tensor([[152.3309],\n",
      "        [184.0012],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5504]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.957159698009491 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5983],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009673056192696095\n",
      "Epoch: 1171 \n",
      " Hypothesis: tensor([[152.3309],\n",
      "        [184.0012],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5504]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9571210145950317 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009673194028437138\n",
      "Epoch: 1172 \n",
      " Hypothesis: tensor([[152.3308],\n",
      "        [184.0013],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5505]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9570881724357605 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967333186417818\n",
      "Epoch: 1173 \n",
      " Hypothesis: tensor([[152.3308],\n",
      "        [184.0013],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5505]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9570573568344116 \n",
      " W: tensor([[0.7313],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009673469699919224\n",
      "Epoch: 1174 \n",
      " Hypothesis: tensor([[152.3308],\n",
      "        [184.0013],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5505]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.957018256187439 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009673607535660267\n",
      "Epoch: 1175 \n",
      " Hypothesis: tensor([[152.3307],\n",
      "        [184.0013],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5506]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9569905996322632 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967374537140131\n",
      "Epoch: 1176 \n",
      " Hypothesis: tensor([[152.3307],\n",
      "        [184.0014],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5506]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.956950843334198 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009673883207142353\n",
      "Epoch: 1177 \n",
      " Hypothesis: tensor([[152.3307],\n",
      "        [184.0014],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5506]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9569122195243835 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009674021042883396\n",
      "Epoch: 1178 \n",
      " Hypothesis: tensor([[152.3306],\n",
      "        [184.0014],\n",
      "        [180.8277],\n",
      "        [196.9446],\n",
      "        [140.5506]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9568843841552734 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967415887862444\n",
      "Epoch: 1179 \n",
      " Hypothesis: tensor([[152.3306],\n",
      "        [184.0014],\n",
      "        [180.8276],\n",
      "        [196.9446],\n",
      "        [140.5507]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9568454623222351 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009674296714365482\n",
      "Epoch: 1180 \n",
      " Hypothesis: tensor([[152.3306],\n",
      "        [184.0014],\n",
      "        [180.8276],\n",
      "        [196.9446],\n",
      "        [140.5507]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9568139314651489 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009674434550106525\n",
      "Epoch: 1181 \n",
      " Hypothesis: tensor([[152.3306],\n",
      "        [184.0015],\n",
      "        [180.8276],\n",
      "        [196.9445],\n",
      "        [140.5507]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9567670822143555 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009674572385847569\n",
      "Epoch: 1182 \n",
      " Hypothesis: tensor([[152.3305],\n",
      "        [184.0015],\n",
      "        [180.8276],\n",
      "        [196.9445],\n",
      "        [140.5508]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9567341804504395 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009674710221588612\n",
      "Epoch: 1183 \n",
      " Hypothesis: tensor([[152.3305],\n",
      "        [184.0015],\n",
      "        [180.8276],\n",
      "        [196.9445],\n",
      "        [140.5508]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.95671147108078 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009674848057329655\n",
      "Epoch: 1184 \n",
      " Hypothesis: tensor([[152.3305],\n",
      "        [184.0015],\n",
      "        [180.8276],\n",
      "        [196.9445],\n",
      "        [140.5508]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9566718339920044 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009674985893070698\n",
      "Epoch: 1185 \n",
      " Hypothesis: tensor([[152.3304],\n",
      "        [184.0015],\n",
      "        [180.8276],\n",
      "        [196.9445],\n",
      "        [140.5508]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9566389322280884 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009675123728811741\n",
      "Epoch: 1186 \n",
      " Hypothesis: tensor([[152.3304],\n",
      "        [184.0016],\n",
      "        [180.8276],\n",
      "        [196.9445],\n",
      "        [140.5509]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9565998911857605 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009675261564552784\n",
      "Epoch: 1187 \n",
      " Hypothesis: tensor([[152.3304],\n",
      "        [184.0016],\n",
      "        [180.8276],\n",
      "        [196.9445],\n",
      "        [140.5509]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.956566333770752 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009675399400293827\n",
      "Epoch: 1188 \n",
      " Hypothesis: tensor([[152.3304],\n",
      "        [184.0016],\n",
      "        [180.8276],\n",
      "        [196.9445],\n",
      "        [140.5509]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9565277099609375 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967553723603487\n",
      "Epoch: 1189 \n",
      " Hypothesis: tensor([[152.3303],\n",
      "        [184.0016],\n",
      "        [180.8276],\n",
      "        [196.9445],\n",
      "        [140.5510]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9564937353134155 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009675675071775913\n",
      "Epoch: 1190 \n",
      " Hypothesis: tensor([[152.3303],\n",
      "        [184.0016],\n",
      "        [180.8275],\n",
      "        [196.9445],\n",
      "        [140.5510]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9564609527587891 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009675812907516956\n",
      "Epoch: 1191 \n",
      " Hypothesis: tensor([[152.3303],\n",
      "        [184.0017],\n",
      "        [180.8275],\n",
      "        [196.9445],\n",
      "        [140.5510]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9564269781112671 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009675950743258\n",
      "Epoch: 1192 \n",
      " Hypothesis: tensor([[152.3302],\n",
      "        [184.0017],\n",
      "        [180.8275],\n",
      "        [196.9445],\n",
      "        [140.5511]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9563884735107422 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009676088578999043\n",
      "Epoch: 1193 \n",
      " Hypothesis: tensor([[152.3302],\n",
      "        [184.0017],\n",
      "        [180.8275],\n",
      "        [196.9445],\n",
      "        [140.5511]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9563497304916382 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5982],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009676226414740086\n",
      "Epoch: 1194 \n",
      " Hypothesis: tensor([[152.3302],\n",
      "        [184.0017],\n",
      "        [180.8275],\n",
      "        [196.9444],\n",
      "        [140.5511]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9563101530075073 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009676364250481129\n",
      "Epoch: 1195 \n",
      " Hypothesis: tensor([[152.3301],\n",
      "        [184.0018],\n",
      "        [180.8275],\n",
      "        [196.9444],\n",
      "        [140.5511]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9562804102897644 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009676502086222172\n",
      "Epoch: 1196 \n",
      " Hypothesis: tensor([[152.3301],\n",
      "        [184.0018],\n",
      "        [180.8275],\n",
      "        [196.9444],\n",
      "        [140.5512]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9562433958053589 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009676639921963215\n",
      "Epoch: 1197 \n",
      " Hypothesis: tensor([[152.3301],\n",
      "        [184.0018],\n",
      "        [180.8275],\n",
      "        [196.9444],\n",
      "        [140.5512]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9562047719955444 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009676777757704258\n",
      "Epoch: 1198 \n",
      " Hypothesis: tensor([[152.3300],\n",
      "        [184.0018],\n",
      "        [180.8275],\n",
      "        [196.9444],\n",
      "        [140.5512]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9561770558357239 \n",
      " W: tensor([[0.7314],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009676915593445301\n",
      "Epoch: 1199 \n",
      " Hypothesis: tensor([[152.3300],\n",
      "        [184.0018],\n",
      "        [180.8275],\n",
      "        [196.9444],\n",
      "        [140.5513]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9561411142349243 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009677053429186344\n",
      "Epoch: 1200 \n",
      " Hypothesis: tensor([[152.3300],\n",
      "        [184.0019],\n",
      "        [180.8275],\n",
      "        [196.9444],\n",
      "        [140.5513]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9561025500297546 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009677191264927387\n",
      "Epoch: 1201 \n",
      " Hypothesis: tensor([[152.3300],\n",
      "        [184.0019],\n",
      "        [180.8275],\n",
      "        [196.9444],\n",
      "        [140.5513]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9560686349868774 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967732910066843\n",
      "Epoch: 1202 \n",
      " Hypothesis: tensor([[152.3299],\n",
      "        [184.0019],\n",
      "        [180.8275],\n",
      "        [196.9444],\n",
      "        [140.5513]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9560427665710449 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009677466936409473\n",
      "Epoch: 1203 \n",
      " Hypothesis: tensor([[152.3299],\n",
      "        [184.0019],\n",
      "        [180.8274],\n",
      "        [196.9444],\n",
      "        [140.5514]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9560041427612305 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009677604772150517\n",
      "Epoch: 1204 \n",
      " Hypothesis: tensor([[152.3299],\n",
      "        [184.0020],\n",
      "        [180.8274],\n",
      "        [196.9444],\n",
      "        [140.5514]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9559594392776489 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967774260789156\n",
      "Epoch: 1205 \n",
      " Hypothesis: tensor([[152.3298],\n",
      "        [184.0020],\n",
      "        [180.8274],\n",
      "        [196.9444],\n",
      "        [140.5514]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9559255838394165 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009677880443632603\n",
      "Epoch: 1206 \n",
      " Hypothesis: tensor([[152.3298],\n",
      "        [184.0020],\n",
      "        [180.8274],\n",
      "        [196.9444],\n",
      "        [140.5515]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9558976888656616 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009678018279373646\n",
      "Epoch: 1207 \n",
      " Hypothesis: tensor([[152.3298],\n",
      "        [184.0020],\n",
      "        [180.8274],\n",
      "        [196.9444],\n",
      "        [140.5515]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9558737874031067 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009678156115114689\n",
      "Epoch: 1208 \n",
      " Hypothesis: tensor([[152.3298],\n",
      "        [184.0020],\n",
      "        [180.8274],\n",
      "        [196.9444],\n",
      "        [140.5515]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9558290243148804 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009678293950855732\n",
      "Epoch: 1209 \n",
      " Hypothesis: tensor([[152.3297],\n",
      "        [184.0021],\n",
      "        [180.8274],\n",
      "        [196.9443],\n",
      "        [140.5515]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9557954668998718 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009678431786596775\n",
      "Epoch: 1210 \n",
      " Hypothesis: tensor([[152.3297],\n",
      "        [184.0021],\n",
      "        [180.8274],\n",
      "        [196.9443],\n",
      "        [140.5516]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9557536840438843 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009678569622337818\n",
      "Epoch: 1211 \n",
      " Hypothesis: tensor([[152.3297],\n",
      "        [184.0021],\n",
      "        [180.8274],\n",
      "        [196.9443],\n",
      "        [140.5516]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9557287096977234 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009678707458078861\n",
      "Epoch: 1212 \n",
      " Hypothesis: tensor([[152.3297],\n",
      "        [184.0021],\n",
      "        [180.8274],\n",
      "        [196.9443],\n",
      "        [140.5516]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9556921720504761 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009678845293819904\n",
      "Epoch: 1213 \n",
      " Hypothesis: tensor([[152.3296],\n",
      "        [184.0022],\n",
      "        [180.8274],\n",
      "        [196.9443],\n",
      "        [140.5517]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9556524157524109 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009678983129560947\n",
      "Epoch: 1214 \n",
      " Hypothesis: tensor([[152.3296],\n",
      "        [184.0022],\n",
      "        [180.8273],\n",
      "        [196.9443],\n",
      "        [140.5517]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9556196928024292 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967912096530199\n",
      "Epoch: 1215 \n",
      " Hypothesis: tensor([[152.3296],\n",
      "        [184.0022],\n",
      "        [180.8273],\n",
      "        [196.9443],\n",
      "        [140.5517]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9555861353874207 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009679258801043034\n",
      "Epoch: 1216 \n",
      " Hypothesis: tensor([[152.3295],\n",
      "        [184.0022],\n",
      "        [180.8273],\n",
      "        [196.9443],\n",
      "        [140.5518]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9555414319038391 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5981],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009679396636784077\n",
      "Epoch: 1217 \n",
      " Hypothesis: tensor([[152.3295],\n",
      "        [184.0022],\n",
      "        [180.8273],\n",
      "        [196.9443],\n",
      "        [140.5518]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9555066227912903 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00967953447252512\n",
      "Epoch: 1218 \n",
      " Hypothesis: tensor([[152.3295],\n",
      "        [184.0023],\n",
      "        [180.8273],\n",
      "        [196.9443],\n",
      "        [140.5518]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9554697275161743 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009679672308266163\n",
      "Epoch: 1219 \n",
      " Hypothesis: tensor([[152.3294],\n",
      "        [184.0023],\n",
      "        [180.8273],\n",
      "        [196.9443],\n",
      "        [140.5518]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.955436110496521 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009679810144007206\n",
      "Epoch: 1220 \n",
      " Hypothesis: tensor([[152.3294],\n",
      "        [184.0023],\n",
      "        [180.8273],\n",
      "        [196.9442],\n",
      "        [140.5519]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9554025530815125 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009679947979748249\n",
      "Epoch: 1221 \n",
      " Hypothesis: tensor([[152.3294],\n",
      "        [184.0023],\n",
      "        [180.8273],\n",
      "        [196.9442],\n",
      "        [140.5519]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9553636312484741 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009680085815489292\n",
      "Epoch: 1222 \n",
      " Hypothesis: tensor([[152.3293],\n",
      "        [184.0023],\n",
      "        [180.8273],\n",
      "        [196.9442],\n",
      "        [140.5519]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9553307294845581 \n",
      " W: tensor([[0.7315],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009680223651230335\n",
      "Epoch: 1223 \n",
      " Hypothesis: tensor([[152.3293],\n",
      "        [184.0023],\n",
      "        [180.8273],\n",
      "        [196.9442],\n",
      "        [140.5520]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9553117752075195 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009680361486971378\n",
      "Epoch: 1224 \n",
      " Hypothesis: tensor([[152.3293],\n",
      "        [184.0024],\n",
      "        [180.8273],\n",
      "        [196.9442],\n",
      "        [140.5520]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9552723169326782 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009680499322712421\n",
      "Epoch: 1225 \n",
      " Hypothesis: tensor([[152.3293],\n",
      "        [184.0024],\n",
      "        [180.8273],\n",
      "        [196.9442],\n",
      "        [140.5520]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9552274942398071 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009680637158453465\n",
      "Epoch: 1226 \n",
      " Hypothesis: tensor([[152.3292],\n",
      "        [184.0024],\n",
      "        [180.8273],\n",
      "        [196.9442],\n",
      "        [140.5520]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9551939964294434 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009680774994194508\n",
      "Epoch: 1227 \n",
      " Hypothesis: tensor([[152.3292],\n",
      "        [184.0024],\n",
      "        [180.8272],\n",
      "        [196.9442],\n",
      "        [140.5521]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9551553726196289 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968091282993555\n",
      "Epoch: 1228 \n",
      " Hypothesis: tensor([[152.3292],\n",
      "        [184.0025],\n",
      "        [180.8272],\n",
      "        [196.9442],\n",
      "        [140.5521]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9551215171813965 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009681050665676594\n",
      "Epoch: 1229 \n",
      " Hypothesis: tensor([[152.3291],\n",
      "        [184.0025],\n",
      "        [180.8272],\n",
      "        [196.9442],\n",
      "        [140.5521]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9550836682319641 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009681188501417637\n",
      "Epoch: 1230 \n",
      " Hypothesis: tensor([[152.3291],\n",
      "        [184.0025],\n",
      "        [180.8272],\n",
      "        [196.9442],\n",
      "        [140.5522]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9550558924674988 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968132633715868\n",
      "Epoch: 1231 \n",
      " Hypothesis: tensor([[152.3291],\n",
      "        [184.0025],\n",
      "        [180.8272],\n",
      "        [196.9442],\n",
      "        [140.5522]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9550269842147827 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009681464172899723\n",
      "Epoch: 1232 \n",
      " Hypothesis: tensor([[152.3291],\n",
      "        [184.0025],\n",
      "        [180.8272],\n",
      "        [196.9442],\n",
      "        [140.5522]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9549795985221863 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009681602008640766\n",
      "Epoch: 1233 \n",
      " Hypothesis: tensor([[152.3290],\n",
      "        [184.0026],\n",
      "        [180.8272],\n",
      "        [196.9442],\n",
      "        [140.5523]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9549387097358704 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968173984438181\n",
      "Epoch: 1234 \n",
      " Hypothesis: tensor([[152.3290],\n",
      "        [184.0026],\n",
      "        [180.8272],\n",
      "        [196.9442],\n",
      "        [140.5523]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9549108743667603 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009681877680122852\n",
      "Epoch: 1235 \n",
      " Hypothesis: tensor([[152.3290],\n",
      "        [184.0026],\n",
      "        [180.8272],\n",
      "        [196.9442],\n",
      "        [140.5523]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9548841714859009 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009682015515863895\n",
      "Epoch: 1236 \n",
      " Hypothesis: tensor([[152.3289],\n",
      "        [184.0026],\n",
      "        [180.8272],\n",
      "        [196.9441],\n",
      "        [140.5524]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9548404812812805 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009682153351604939\n",
      "Epoch: 1237 \n",
      " Hypothesis: tensor([[152.3289],\n",
      "        [184.0027],\n",
      "        [180.8272],\n",
      "        [196.9441],\n",
      "        [140.5524]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9548085927963257 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009682291187345982\n",
      "Epoch: 1238 \n",
      " Hypothesis: tensor([[152.3289],\n",
      "        [184.0027],\n",
      "        [180.8272],\n",
      "        [196.9441],\n",
      "        [140.5524]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9547788500785828 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009682429023087025\n",
      "Epoch: 1239 \n",
      " Hypothesis: tensor([[152.3288],\n",
      "        [184.0027],\n",
      "        [180.8271],\n",
      "        [196.9441],\n",
      "        [140.5524]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9547284245491028 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5980],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009682566858828068\n",
      "Epoch: 1240 \n",
      " Hypothesis: tensor([[152.3288],\n",
      "        [184.0027],\n",
      "        [180.8271],\n",
      "        [196.9441],\n",
      "        [140.5525]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9546955823898315 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009682704694569111\n",
      "Epoch: 1241 \n",
      " Hypothesis: tensor([[152.3288],\n",
      "        [184.0028],\n",
      "        [180.8271],\n",
      "        [196.9441],\n",
      "        [140.5525]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9546617269515991 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009682842530310154\n",
      "Epoch: 1242 \n",
      " Hypothesis: tensor([[152.3288],\n",
      "        [184.0028],\n",
      "        [180.8271],\n",
      "        [196.9441],\n",
      "        [140.5525]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9546231031417847 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009682980366051197\n",
      "Epoch: 1243 \n",
      " Hypothesis: tensor([[152.3287],\n",
      "        [184.0028],\n",
      "        [180.8271],\n",
      "        [196.9441],\n",
      "        [140.5526]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9545855522155762 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968311820179224\n",
      "Epoch: 1244 \n",
      " Hypothesis: tensor([[152.3287],\n",
      "        [184.0028],\n",
      "        [180.8271],\n",
      "        [196.9441],\n",
      "        [140.5526]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9545527696609497 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009683256037533283\n",
      "Epoch: 1245 \n",
      " Hypothesis: tensor([[152.3287],\n",
      "        [184.0029],\n",
      "        [180.8271],\n",
      "        [196.9441],\n",
      "        [140.5526]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9545189142227173 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009683393873274326\n",
      "Epoch: 1246 \n",
      " Hypothesis: tensor([[152.3286],\n",
      "        [184.0029],\n",
      "        [180.8271],\n",
      "        [196.9440],\n",
      "        [140.5526]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.954489529132843 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968353170901537\n",
      "Epoch: 1247 \n",
      " Hypothesis: tensor([[152.3286],\n",
      "        [184.0029],\n",
      "        [180.8271],\n",
      "        [196.9440],\n",
      "        [140.5527]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9544485807418823 \n",
      " W: tensor([[0.7316],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009683669544756413\n",
      "Epoch: 1248 \n",
      " Hypothesis: tensor([[152.3286],\n",
      "        [184.0029],\n",
      "        [180.8271],\n",
      "        [196.9440],\n",
      "        [140.5527]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9544208645820618 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009683807380497456\n",
      "Epoch: 1249 \n",
      " Hypothesis: tensor([[152.3286],\n",
      "        [184.0029],\n",
      "        [180.8271],\n",
      "        [196.9440],\n",
      "        [140.5527]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9543842077255249 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009683945216238499\n",
      "Epoch: 1250 \n",
      " Hypothesis: tensor([[152.3285],\n",
      "        [184.0029],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5528]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9543396234512329 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009684083051979542\n",
      "Epoch: 1251 \n",
      " Hypothesis: tensor([[152.3285],\n",
      "        [184.0030],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5528]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9543207287788391 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009684220887720585\n",
      "Epoch: 1252 \n",
      " Hypothesis: tensor([[152.3285],\n",
      "        [184.0030],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5528]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9542818069458008 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009684358723461628\n",
      "Epoch: 1253 \n",
      " Hypothesis: tensor([[152.3284],\n",
      "        [184.0030],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5529]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9542413949966431 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009684496559202671\n",
      "Epoch: 1254 \n",
      " Hypothesis: tensor([[152.3284],\n",
      "        [184.0030],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5529]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9542005658149719 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009684634394943714\n",
      "Epoch: 1255 \n",
      " Hypothesis: tensor([[152.3284],\n",
      "        [184.0031],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5529]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9541757702827454 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009684772230684757\n",
      "Epoch: 1256 \n",
      " Hypothesis: tensor([[152.3283],\n",
      "        [184.0031],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5529]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9541430473327637 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.0096849100664258\n",
      "Epoch: 1257 \n",
      " Hypothesis: tensor([[152.3283],\n",
      "        [184.0031],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5530]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9541040658950806 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009685047902166843\n",
      "Epoch: 1258 \n",
      " Hypothesis: tensor([[152.3283],\n",
      "        [184.0031],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5530]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9540694952011108 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009685185737907887\n",
      "Epoch: 1259 \n",
      " Hypothesis: tensor([[152.3283],\n",
      "        [184.0031],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5530]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9540390968322754 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968532357364893\n",
      "Epoch: 1260 \n",
      " Hypothesis: tensor([[152.3282],\n",
      "        [184.0032],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5531]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9540001749992371 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009685461409389973\n",
      "Epoch: 1261 \n",
      " Hypothesis: tensor([[152.3282],\n",
      "        [184.0032],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5531]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9539673924446106 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009685599245131016\n",
      "Epoch: 1262 \n",
      " Hypothesis: tensor([[152.3282],\n",
      "        [184.0032],\n",
      "        [180.8270],\n",
      "        [196.9440],\n",
      "        [140.5531]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9539396166801453 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009685737080872059\n",
      "Epoch: 1263 \n",
      " Hypothesis: tensor([[152.3281],\n",
      "        [184.0032],\n",
      "        [180.8270],\n",
      "        [196.9439],\n",
      "        [140.5531]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9539000391960144 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5979],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009685874916613102\n",
      "Epoch: 1264 \n",
      " Hypothesis: tensor([[152.3281],\n",
      "        [184.0032],\n",
      "        [180.8269],\n",
      "        [196.9439],\n",
      "        [140.5532]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9538615345954895 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009686012752354145\n",
      "Epoch: 1265 \n",
      " Hypothesis: tensor([[152.3281],\n",
      "        [184.0033],\n",
      "        [180.8269],\n",
      "        [196.9439],\n",
      "        [140.5532]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9538168907165527 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009686150588095188\n",
      "Epoch: 1266 \n",
      " Hypothesis: tensor([[152.3280],\n",
      "        [184.0033],\n",
      "        [180.8269],\n",
      "        [196.9439],\n",
      "        [140.5532]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9537833333015442 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009686288423836231\n",
      "Epoch: 1267 \n",
      " Hypothesis: tensor([[152.3280],\n",
      "        [184.0033],\n",
      "        [180.8269],\n",
      "        [196.9439],\n",
      "        [140.5533]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9537494778633118 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009686426259577274\n",
      "Epoch: 1268 \n",
      " Hypothesis: tensor([[152.3280],\n",
      "        [184.0033],\n",
      "        [180.8269],\n",
      "        [196.9439],\n",
      "        [140.5533]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9537255167961121 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009686564095318317\n",
      "Epoch: 1269 \n",
      " Hypothesis: tensor([[152.3280],\n",
      "        [184.0033],\n",
      "        [180.8269],\n",
      "        [196.9439],\n",
      "        [140.5533]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9536870121955872 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968670193105936\n",
      "Epoch: 1270 \n",
      " Hypothesis: tensor([[152.3279],\n",
      "        [184.0034],\n",
      "        [180.8269],\n",
      "        [196.9439],\n",
      "        [140.5533]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9536531567573547 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009686839766800404\n",
      "Epoch: 1271 \n",
      " Hypothesis: tensor([[152.3279],\n",
      "        [184.0034],\n",
      "        [180.8269],\n",
      "        [196.9439],\n",
      "        [140.5534]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9536145925521851 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009686977602541447\n",
      "Epoch: 1272 \n",
      " Hypothesis: tensor([[152.3279],\n",
      "        [184.0034],\n",
      "        [180.8269],\n",
      "        [196.9439],\n",
      "        [140.5534]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9535757303237915 \n",
      " W: tensor([[0.7317],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968711543828249\n",
      "Epoch: 1273 \n",
      " Hypothesis: tensor([[152.3278],\n",
      "        [184.0034],\n",
      "        [180.8269],\n",
      "        [196.9439],\n",
      "        [140.5534]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9535479545593262 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009687253274023533\n",
      "Epoch: 1274 \n",
      " Hypothesis: tensor([[152.3278],\n",
      "        [184.0035],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5535]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9535033106803894 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009687391109764576\n",
      "Epoch: 1275 \n",
      " Hypothesis: tensor([[152.3278],\n",
      "        [184.0035],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5535]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9534699320793152 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009687528945505619\n",
      "Epoch: 1276 \n",
      " Hypothesis: tensor([[152.3277],\n",
      "        [184.0035],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5535]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9534371495246887 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009687666781246662\n",
      "Epoch: 1277 \n",
      " Hypothesis: tensor([[152.3277],\n",
      "        [184.0035],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5536]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9533917307853699 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009687804616987705\n",
      "Epoch: 1278 \n",
      " Hypothesis: tensor([[152.3277],\n",
      "        [184.0036],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5536]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9533548355102539 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009687942452728748\n",
      "Epoch: 1279 \n",
      " Hypothesis: tensor([[152.3277],\n",
      "        [184.0036],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5536]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.953342080116272 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009688080288469791\n",
      "Epoch: 1280 \n",
      " Hypothesis: tensor([[152.3276],\n",
      "        [184.0036],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5536]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9533031582832336 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009688218124210835\n",
      "Epoch: 1281 \n",
      " Hypothesis: tensor([[152.3276],\n",
      "        [184.0036],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5537]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9532696604728699 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009688355959951878\n",
      "Epoch: 1282 \n",
      " Hypothesis: tensor([[152.3276],\n",
      "        [184.0036],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5537]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9532249569892883 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968849379569292\n",
      "Epoch: 1283 \n",
      " Hypothesis: tensor([[152.3275],\n",
      "        [184.0036],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5537]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9531973004341125 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009688631631433964\n",
      "Epoch: 1284 \n",
      " Hypothesis: tensor([[152.3275],\n",
      "        [184.0037],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5538]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9531645774841309 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009688769467175007\n",
      "Epoch: 1285 \n",
      " Hypothesis: tensor([[152.3275],\n",
      "        [184.0037],\n",
      "        [180.8268],\n",
      "        [196.9438],\n",
      "        [140.5538]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.953125 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968890730291605\n",
      "Epoch: 1286 \n",
      " Hypothesis: tensor([[152.3275],\n",
      "        [184.0037],\n",
      "        [180.8267],\n",
      "        [196.9438],\n",
      "        [140.5538]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9530871510505676 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5978],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009689045138657093\n",
      "Epoch: 1287 \n",
      " Hypothesis: tensor([[152.3274],\n",
      "        [184.0037],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5538]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9530475735664368 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009689182974398136\n",
      "Epoch: 1288 \n",
      " Hypothesis: tensor([[152.3274],\n",
      "        [184.0038],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5539]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9530218243598938 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00968932081013918\n",
      "Epoch: 1289 \n",
      " Hypothesis: tensor([[152.3274],\n",
      "        [184.0038],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5539]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9529891014099121 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009689458645880222\n",
      "Epoch: 1290 \n",
      " Hypothesis: tensor([[152.3273],\n",
      "        [184.0038],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5539]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9529474377632141 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009689596481621265\n",
      "Epoch: 1291 \n",
      " Hypothesis: tensor([[152.3273],\n",
      "        [184.0038],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5540]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9529136419296265 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009689734317362309\n",
      "Epoch: 1292 \n",
      " Hypothesis: tensor([[152.3273],\n",
      "        [184.0038],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5540]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9528859853744507 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009689872153103352\n",
      "Epoch: 1293 \n",
      " Hypothesis: tensor([[152.3272],\n",
      "        [184.0039],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5540]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9528423547744751 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009690009988844395\n",
      "Epoch: 1294 \n",
      " Hypothesis: tensor([[152.3272],\n",
      "        [184.0039],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5541]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9528055191040039 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009690147824585438\n",
      "Epoch: 1295 \n",
      " Hypothesis: tensor([[152.3272],\n",
      "        [184.0039],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5541]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9527809023857117 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969028566032648\n",
      "Epoch: 1296 \n",
      " Hypothesis: tensor([[152.3272],\n",
      "        [184.0039],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5541]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9527362585067749 \n",
      " W: tensor([[0.7318],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009690423496067524\n",
      "Epoch: 1297 \n",
      " Hypothesis: tensor([[152.3271],\n",
      "        [184.0040],\n",
      "        [180.8267],\n",
      "        [196.9437],\n",
      "        [140.5541]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9527027010917664 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009690561331808567\n",
      "Epoch: 1298 \n",
      " Hypothesis: tensor([[152.3271],\n",
      "        [184.0040],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5542]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9526581764221191 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969069916754961\n",
      "Epoch: 1299 \n",
      " Hypothesis: tensor([[152.3271],\n",
      "        [184.0040],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5542]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9526365399360657 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009690837003290653\n",
      "Epoch: 1300 \n",
      " Hypothesis: tensor([[152.3270],\n",
      "        [184.0040],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5542]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9525977373123169 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009690974839031696\n",
      "Epoch: 1301 \n",
      " Hypothesis: tensor([[152.3270],\n",
      "        [184.0040],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5543]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.952558159828186 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969111267477274\n",
      "Epoch: 1302 \n",
      " Hypothesis: tensor([[152.3270],\n",
      "        [184.0041],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5543]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9525253176689148 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009691250510513783\n",
      "Epoch: 1303 \n",
      " Hypothesis: tensor([[152.3270],\n",
      "        [184.0041],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5543]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9525007009506226 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009691388346254826\n",
      "Epoch: 1304 \n",
      " Hypothesis: tensor([[152.3269],\n",
      "        [184.0041],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5543]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9524669647216797 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009691526181995869\n",
      "Epoch: 1305 \n",
      " Hypothesis: tensor([[152.3269],\n",
      "        [184.0041],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5544]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9524341821670532 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009691664017736912\n",
      "Epoch: 1306 \n",
      " Hypothesis: tensor([[152.3269],\n",
      "        [184.0041],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5544]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9523868560791016 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009691801853477955\n",
      "Epoch: 1307 \n",
      " Hypothesis: tensor([[152.3268],\n",
      "        [184.0042],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5544]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9523530006408691 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009691939689218998\n",
      "Epoch: 1308 \n",
      " Hypothesis: tensor([[152.3268],\n",
      "        [184.0042],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5545]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9523252248764038 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009692077524960041\n",
      "Epoch: 1309 \n",
      " Hypothesis: tensor([[152.3268],\n",
      "        [184.0042],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5545]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9522730112075806 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5977],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009692215360701084\n",
      "Epoch: 1310 \n",
      " Hypothesis: tensor([[152.3268],\n",
      "        [184.0042],\n",
      "        [180.8266],\n",
      "        [196.9436],\n",
      "        [140.5545]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9522472620010376 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009692353196442127\n",
      "Epoch: 1311 \n",
      " Hypothesis: tensor([[152.3267],\n",
      "        [184.0043],\n",
      "        [180.8265],\n",
      "        [196.9436],\n",
      "        [140.5546]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9522064328193665 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969249103218317\n",
      "Epoch: 1312 \n",
      " Hypothesis: tensor([[152.3267],\n",
      "        [184.0043],\n",
      "        [180.8265],\n",
      "        [196.9436],\n",
      "        [140.5546]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9521786570549011 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009692628867924213\n",
      "Epoch: 1313 \n",
      " Hypothesis: tensor([[152.3267],\n",
      "        [184.0043],\n",
      "        [180.8265],\n",
      "        [196.9435],\n",
      "        [140.5546]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9521422386169434 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009692766703665257\n",
      "Epoch: 1314 \n",
      " Hypothesis: tensor([[152.3266],\n",
      "        [184.0043],\n",
      "        [180.8265],\n",
      "        [196.9435],\n",
      "        [140.5546]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9521026611328125 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.0096929045394063\n",
      "Epoch: 1315 \n",
      " Hypothesis: tensor([[152.3266],\n",
      "        [184.0043],\n",
      "        [180.8265],\n",
      "        [196.9435],\n",
      "        [140.5547]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9520699381828308 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009693042375147343\n",
      "Epoch: 1316 \n",
      " Hypothesis: tensor([[152.3266],\n",
      "        [184.0044],\n",
      "        [180.8265],\n",
      "        [196.9435],\n",
      "        [140.5547]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9520310163497925 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009693180210888386\n",
      "Epoch: 1317 \n",
      " Hypothesis: tensor([[152.3266],\n",
      "        [184.0044],\n",
      "        [180.8265],\n",
      "        [196.9435],\n",
      "        [140.5547]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9519995450973511 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009693318046629429\n",
      "Epoch: 1318 \n",
      " Hypothesis: tensor([[152.3265],\n",
      "        [184.0044],\n",
      "        [180.8265],\n",
      "        [196.9435],\n",
      "        [140.5548]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9519588351249695 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009693455882370472\n",
      "Epoch: 1319 \n",
      " Hypothesis: tensor([[152.3265],\n",
      "        [184.0044],\n",
      "        [180.8265],\n",
      "        [196.9435],\n",
      "        [140.5548]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9519340395927429 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009693593718111515\n",
      "Epoch: 1320 \n",
      " Hypothesis: tensor([[152.3264],\n",
      "        [184.0044],\n",
      "        [180.8265],\n",
      "        [196.9435],\n",
      "        [140.5548]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9519013166427612 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009693731553852558\n",
      "Epoch: 1321 \n",
      " Hypothesis: tensor([[152.3264],\n",
      "        [184.0045],\n",
      "        [180.8264],\n",
      "        [196.9435],\n",
      "        [140.5548]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9518625140190125 \n",
      " W: tensor([[0.7319],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009693869389593601\n",
      "Epoch: 1322 \n",
      " Hypothesis: tensor([[152.3264],\n",
      "        [184.0045],\n",
      "        [180.8265],\n",
      "        [196.9435],\n",
      "        [140.5549]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.951828122138977 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009694007225334644\n",
      "Epoch: 1323 \n",
      " Hypothesis: tensor([[152.3264],\n",
      "        [184.0045],\n",
      "        [180.8264],\n",
      "        [196.9435],\n",
      "        [140.5549]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9517976641654968 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009694145061075687\n",
      "Epoch: 1324 \n",
      " Hypothesis: tensor([[152.3263],\n",
      "        [184.0045],\n",
      "        [180.8264],\n",
      "        [196.9435],\n",
      "        [140.5549]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9517587423324585 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969428289681673\n",
      "Epoch: 1325 \n",
      " Hypothesis: tensor([[152.3263],\n",
      "        [184.0045],\n",
      "        [180.8264],\n",
      "        [196.9435],\n",
      "        [140.5550]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9517310857772827 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009694420732557774\n",
      "Epoch: 1326 \n",
      " Hypothesis: tensor([[152.3263],\n",
      "        [184.0046],\n",
      "        [180.8264],\n",
      "        [196.9435],\n",
      "        [140.5550]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9516983032226562 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009694558568298817\n",
      "Epoch: 1327 \n",
      " Hypothesis: tensor([[152.3262],\n",
      "        [184.0046],\n",
      "        [180.8264],\n",
      "        [196.9435],\n",
      "        [140.5550]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9516676068305969 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969469640403986\n",
      "Epoch: 1328 \n",
      " Hypothesis: tensor([[152.3262],\n",
      "        [184.0046],\n",
      "        [180.8264],\n",
      "        [196.9434],\n",
      "        [140.5551]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9516203999519348 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009694834239780903\n",
      "Epoch: 1329 \n",
      " Hypothesis: tensor([[152.3262],\n",
      "        [184.0046],\n",
      "        [180.8264],\n",
      "        [196.9434],\n",
      "        [140.5551]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.951580822467804 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009694972075521946\n",
      "Epoch: 1330 \n",
      " Hypothesis: tensor([[152.3262],\n",
      "        [184.0047],\n",
      "        [180.8264],\n",
      "        [196.9434],\n",
      "        [140.5551]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9515423774719238 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009695109911262989\n",
      "Epoch: 1331 \n",
      " Hypothesis: tensor([[152.3261],\n",
      "        [184.0047],\n",
      "        [180.8264],\n",
      "        [196.9434],\n",
      "        [140.5551]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.951512336730957 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009695247747004032\n",
      "Epoch: 1332 \n",
      " Hypothesis: tensor([[152.3261],\n",
      "        [184.0047],\n",
      "        [180.8264],\n",
      "        [196.9434],\n",
      "        [140.5552]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9514846801757812 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009695385582745075\n",
      "Epoch: 1333 \n",
      " Hypothesis: tensor([[152.3261],\n",
      "        [184.0047],\n",
      "        [180.8264],\n",
      "        [196.9434],\n",
      "        [140.5552]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9514461755752563 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5976],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009695523418486118\n",
      "Epoch: 1334 \n",
      " Hypothesis: tensor([[152.3260],\n",
      "        [184.0047],\n",
      "        [180.8263],\n",
      "        [196.9434],\n",
      "        [140.5552]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.951407253742218 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009695661254227161\n",
      "Epoch: 1335 \n",
      " Hypothesis: tensor([[152.3260],\n",
      "        [184.0048],\n",
      "        [180.8263],\n",
      "        [196.9434],\n",
      "        [140.5553]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9513738751411438 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009695799089968204\n",
      "Epoch: 1336 \n",
      " Hypothesis: tensor([[152.3260],\n",
      "        [184.0048],\n",
      "        [180.8263],\n",
      "        [196.9434],\n",
      "        [140.5553]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9513400197029114 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009695936925709248\n",
      "Epoch: 1337 \n",
      " Hypothesis: tensor([[152.3259],\n",
      "        [184.0048],\n",
      "        [180.8263],\n",
      "        [196.9434],\n",
      "        [140.5553]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9513074159622192 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969607476145029\n",
      "Epoch: 1338 \n",
      " Hypothesis: tensor([[152.3259],\n",
      "        [184.0048],\n",
      "        [180.8263],\n",
      "        [196.9434],\n",
      "        [140.5553]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9512628316879272 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009696212597191334\n",
      "Epoch: 1339 \n",
      " Hypothesis: tensor([[152.3259],\n",
      "        [184.0049],\n",
      "        [180.8263],\n",
      "        [196.9433],\n",
      "        [140.5554]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9512293934822083 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009696350432932377\n",
      "Epoch: 1340 \n",
      " Hypothesis: tensor([[152.3259],\n",
      "        [184.0049],\n",
      "        [180.8263],\n",
      "        [196.9433],\n",
      "        [140.5554]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9512017369270325 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969648826867342\n",
      "Epoch: 1341 \n",
      " Hypothesis: tensor([[152.3258],\n",
      "        [184.0049],\n",
      "        [180.8263],\n",
      "        [196.9433],\n",
      "        [140.5554]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9511513710021973 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009696626104414463\n",
      "Epoch: 1342 \n",
      " Hypothesis: tensor([[152.3258],\n",
      "        [184.0049],\n",
      "        [180.8263],\n",
      "        [196.9433],\n",
      "        [140.5555]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9511145353317261 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009696763940155506\n",
      "Epoch: 1343 \n",
      " Hypothesis: tensor([[152.3258],\n",
      "        [184.0049],\n",
      "        [180.8263],\n",
      "        [196.9433],\n",
      "        [140.5555]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9511017799377441 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969690177589655\n",
      "Epoch: 1344 \n",
      " Hypothesis: tensor([[152.3257],\n",
      "        [184.0050],\n",
      "        [180.8263],\n",
      "        [196.9433],\n",
      "        [140.5555]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9510629773139954 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009697039611637592\n",
      "Epoch: 1345 \n",
      " Hypothesis: tensor([[152.3257],\n",
      "        [184.0050],\n",
      "        [180.8263],\n",
      "        [196.9433],\n",
      "        [140.5556]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9510223269462585 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009697177447378635\n",
      "Epoch: 1346 \n",
      " Hypothesis: tensor([[152.3257],\n",
      "        [184.0050],\n",
      "        [180.8263],\n",
      "        [196.9433],\n",
      "        [140.5556]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9509741067886353 \n",
      " W: tensor([[0.7320],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009697315283119678\n",
      "Epoch: 1347 \n",
      " Hypothesis: tensor([[152.3257],\n",
      "        [184.0050],\n",
      "        [180.8263],\n",
      "        [196.9433],\n",
      "        [140.5556]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9509552121162415 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009697453118860722\n",
      "Epoch: 1348 \n",
      " Hypothesis: tensor([[152.3256],\n",
      "        [184.0050],\n",
      "        [180.8262],\n",
      "        [196.9433],\n",
      "        [140.5556]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9509245753288269 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009697590954601765\n",
      "Epoch: 1349 \n",
      " Hypothesis: tensor([[152.3256],\n",
      "        [184.0051],\n",
      "        [180.8262],\n",
      "        [196.9433],\n",
      "        [140.5557]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9508851170539856 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009697728790342808\n",
      "Epoch: 1350 \n",
      " Hypothesis: tensor([[152.3256],\n",
      "        [184.0051],\n",
      "        [180.8262],\n",
      "        [196.9433],\n",
      "        [140.5557]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9508463144302368 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969786662608385\n",
      "Epoch: 1351 \n",
      " Hypothesis: tensor([[152.3255],\n",
      "        [184.0051],\n",
      "        [180.8262],\n",
      "        [196.9433],\n",
      "        [140.5557]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9508205652236938 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009698004461824894\n",
      "Epoch: 1352 \n",
      " Hypothesis: tensor([[152.3255],\n",
      "        [184.0051],\n",
      "        [180.8262],\n",
      "        [196.9433],\n",
      "        [140.5558]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9507800936698914 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009698142297565937\n",
      "Epoch: 1353 \n",
      " Hypothesis: tensor([[152.3255],\n",
      "        [184.0052],\n",
      "        [180.8262],\n",
      "        [196.9432],\n",
      "        [140.5558]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9507375955581665 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969828013330698\n",
      "Epoch: 1354 \n",
      " Hypothesis: tensor([[152.3254],\n",
      "        [184.0052],\n",
      "        [180.8262],\n",
      "        [196.9432],\n",
      "        [140.5558]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9507077932357788 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009698417969048023\n",
      "Epoch: 1355 \n",
      " Hypothesis: tensor([[152.3254],\n",
      "        [184.0052],\n",
      "        [180.8262],\n",
      "        [196.9432],\n",
      "        [140.5558]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9506751894950867 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009698555804789066\n",
      "Epoch: 1356 \n",
      " Hypothesis: tensor([[152.3254],\n",
      "        [184.0052],\n",
      "        [180.8262],\n",
      "        [196.9432],\n",
      "        [140.5559]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9506376385688782 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5975],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00969869364053011\n",
      "Epoch: 1357 \n",
      " Hypothesis: tensor([[152.3253],\n",
      "        [184.0052],\n",
      "        [180.8261],\n",
      "        [196.9432],\n",
      "        [140.5559]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9506009221076965 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009698831476271152\n",
      "Epoch: 1358 \n",
      " Hypothesis: tensor([[152.3253],\n",
      "        [184.0052],\n",
      "        [180.8261],\n",
      "        [196.9432],\n",
      "        [140.5559]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9505645036697388 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009698969312012196\n",
      "Epoch: 1359 \n",
      " Hypothesis: tensor([[152.3253],\n",
      "        [184.0053],\n",
      "        [180.8261],\n",
      "        [196.9432],\n",
      "        [140.5560]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9505345225334167 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009699107147753239\n",
      "Epoch: 1360 \n",
      " Hypothesis: tensor([[152.3253],\n",
      "        [184.0053],\n",
      "        [180.8261],\n",
      "        [196.9432],\n",
      "        [140.5560]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9505030512809753 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009699244983494282\n",
      "Epoch: 1361 \n",
      " Hypothesis: tensor([[152.3252],\n",
      "        [184.0053],\n",
      "        [180.8261],\n",
      "        [196.9432],\n",
      "        [140.5560]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.950463593006134 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009699382819235325\n",
      "Epoch: 1362 \n",
      " Hypothesis: tensor([[152.3252],\n",
      "        [184.0053],\n",
      "        [180.8261],\n",
      "        [196.9432],\n",
      "        [140.5560]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9504251480102539 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009699520654976368\n",
      "Epoch: 1363 \n",
      " Hypothesis: tensor([[152.3252],\n",
      "        [184.0054],\n",
      "        [180.8261],\n",
      "        [196.9431],\n",
      "        [140.5561]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.950386643409729 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009699658490717411\n",
      "Epoch: 1364 \n",
      " Hypothesis: tensor([[152.3251],\n",
      "        [184.0054],\n",
      "        [180.8261],\n",
      "        [196.9431],\n",
      "        [140.5561]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.950347900390625 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009699796326458454\n",
      "Epoch: 1365 \n",
      " Hypothesis: tensor([[152.3251],\n",
      "        [184.0054],\n",
      "        [180.8261],\n",
      "        [196.9431],\n",
      "        [140.5561]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9503251910209656 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009699934162199497\n",
      "Epoch: 1366 \n",
      " Hypothesis: tensor([[152.3251],\n",
      "        [184.0054],\n",
      "        [180.8261],\n",
      "        [196.9431],\n",
      "        [140.5562]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9502824544906616 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970007199794054\n",
      "Epoch: 1367 \n",
      " Hypothesis: tensor([[152.3250],\n",
      "        [184.0054],\n",
      "        [180.8261],\n",
      "        [196.9431],\n",
      "        [140.5562]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9502552151679993 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009700209833681583\n",
      "Epoch: 1368 \n",
      " Hypothesis: tensor([[152.3250],\n",
      "        [184.0055],\n",
      "        [180.8261],\n",
      "        [196.9431],\n",
      "        [140.5562]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.950223445892334 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009700347669422626\n",
      "Epoch: 1369 \n",
      " Hypothesis: tensor([[152.3250],\n",
      "        [184.0055],\n",
      "        [180.8260],\n",
      "        [196.9431],\n",
      "        [140.5562]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9501846432685852 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970048550516367\n",
      "Epoch: 1370 \n",
      " Hypothesis: tensor([[152.3250],\n",
      "        [184.0055],\n",
      "        [180.8260],\n",
      "        [196.9431],\n",
      "        [140.5563]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.950146496295929 \n",
      " W: tensor([[0.7321],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009700623340904713\n",
      "Epoch: 1371 \n",
      " Hypothesis: tensor([[152.3249],\n",
      "        [184.0055],\n",
      "        [180.8260],\n",
      "        [196.9431],\n",
      "        [140.5563]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9501076936721802 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009700761176645756\n",
      "Epoch: 1372 \n",
      " Hypothesis: tensor([[152.3249],\n",
      "        [184.0055],\n",
      "        [180.8260],\n",
      "        [196.9431],\n",
      "        [140.5563]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9500800371170044 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009700899012386799\n",
      "Epoch: 1373 \n",
      " Hypothesis: tensor([[152.3249],\n",
      "        [184.0056],\n",
      "        [180.8260],\n",
      "        [196.9431],\n",
      "        [140.5564]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9500386118888855 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009701036848127842\n",
      "Epoch: 1374 \n",
      " Hypothesis: tensor([[152.3248],\n",
      "        [184.0056],\n",
      "        [180.8260],\n",
      "        [196.9431],\n",
      "        [140.5564]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9500109553337097 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009701174683868885\n",
      "Epoch: 1375 \n",
      " Hypothesis: tensor([[152.3248],\n",
      "        [184.0056],\n",
      "        [180.8260],\n",
      "        [196.9431],\n",
      "        [140.5564]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9499694108963013 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009701312519609928\n",
      "Epoch: 1376 \n",
      " Hypothesis: tensor([[152.3248],\n",
      "        [184.0056],\n",
      "        [180.8260],\n",
      "        [196.9430],\n",
      "        [140.5565]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9499298930168152 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009701450355350971\n",
      "Epoch: 1377 \n",
      " Hypothesis: tensor([[152.3248],\n",
      "        [184.0056],\n",
      "        [180.8260],\n",
      "        [196.9430],\n",
      "        [140.5565]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9499003291130066 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009701588191092014\n",
      "Epoch: 1378 \n",
      " Hypothesis: tensor([[152.3247],\n",
      "        [184.0057],\n",
      "        [180.8260],\n",
      "        [196.9430],\n",
      "        [140.5565]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9498615264892578 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009701726026833057\n",
      "Epoch: 1379 \n",
      " Hypothesis: tensor([[152.3247],\n",
      "        [184.0057],\n",
      "        [180.8260],\n",
      "        [196.9430],\n",
      "        [140.5565]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.949833869934082 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5974],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.0097018638625741\n",
      "Epoch: 1380 \n",
      " Hypothesis: tensor([[152.3247],\n",
      "        [184.0057],\n",
      "        [180.8259],\n",
      "        [196.9430],\n",
      "        [140.5566]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9498031735420227 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009702001698315144\n",
      "Epoch: 1381 \n",
      " Hypothesis: tensor([[152.3246],\n",
      "        [184.0057],\n",
      "        [180.8259],\n",
      "        [196.9430],\n",
      "        [140.5566]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9497586488723755 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009702139534056187\n",
      "Epoch: 1382 \n",
      " Hypothesis: tensor([[152.3246],\n",
      "        [184.0058],\n",
      "        [180.8259],\n",
      "        [196.9430],\n",
      "        [140.5566]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9497171640396118 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970227736979723\n",
      "Epoch: 1383 \n",
      " Hypothesis: tensor([[152.3246],\n",
      "        [184.0058],\n",
      "        [180.8259],\n",
      "        [196.9430],\n",
      "        [140.5567]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9496914744377136 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009702415205538273\n",
      "Epoch: 1384 \n",
      " Hypothesis: tensor([[152.3245],\n",
      "        [184.0058],\n",
      "        [180.8259],\n",
      "        [196.9430],\n",
      "        [140.5567]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9496548771858215 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009702553041279316\n",
      "Epoch: 1385 \n",
      " Hypothesis: tensor([[152.3245],\n",
      "        [184.0058],\n",
      "        [180.8259],\n",
      "        [196.9430],\n",
      "        [140.5567]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9496272206306458 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009702690877020359\n",
      "Epoch: 1386 \n",
      " Hypothesis: tensor([[152.3245],\n",
      "        [184.0058],\n",
      "        [180.8259],\n",
      "        [196.9430],\n",
      "        [140.5567]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9495878219604492 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009702828712761402\n",
      "Epoch: 1387 \n",
      " Hypothesis: tensor([[152.3245],\n",
      "        [184.0059],\n",
      "        [180.8259],\n",
      "        [196.9430],\n",
      "        [140.5568]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9495570063591003 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009702966548502445\n",
      "Epoch: 1388 \n",
      " Hypothesis: tensor([[152.3244],\n",
      "        [184.0059],\n",
      "        [180.8259],\n",
      "        [196.9429],\n",
      "        [140.5568]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9495105743408203 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009703104384243488\n",
      "Epoch: 1389 \n",
      " Hypothesis: tensor([[152.3244],\n",
      "        [184.0059],\n",
      "        [180.8259],\n",
      "        [196.9429],\n",
      "        [140.5568]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.949485182762146 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009703242219984531\n",
      "Epoch: 1390 \n",
      " Hypothesis: tensor([[152.3244],\n",
      "        [184.0059],\n",
      "        [180.8259],\n",
      "        [196.9429],\n",
      "        [140.5569]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9494575262069702 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009703380055725574\n",
      "Epoch: 1391 \n",
      " Hypothesis: tensor([[152.3243],\n",
      "        [184.0059],\n",
      "        [180.8259],\n",
      "        [196.9429],\n",
      "        [140.5569]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9494168162345886 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009703517891466618\n",
      "Epoch: 1392 \n",
      " Hypothesis: tensor([[152.3243],\n",
      "        [184.0060],\n",
      "        [180.8258],\n",
      "        [196.9429],\n",
      "        [140.5569]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9493861198425293 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970365572720766\n",
      "Epoch: 1393 \n",
      " Hypothesis: tensor([[152.3243],\n",
      "        [184.0060],\n",
      "        [180.8258],\n",
      "        [196.9429],\n",
      "        [140.5569]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9493476748466492 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009703793562948704\n",
      "Epoch: 1394 \n",
      " Hypothesis: tensor([[152.3242],\n",
      "        [184.0060],\n",
      "        [180.8258],\n",
      "        [196.9429],\n",
      "        [140.5570]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9493004679679871 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009703931398689747\n",
      "Epoch: 1395 \n",
      " Hypothesis: tensor([[152.3242],\n",
      "        [184.0060],\n",
      "        [180.8258],\n",
      "        [196.9429],\n",
      "        [140.5570]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9492708444595337 \n",
      " W: tensor([[0.7322],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970406923443079\n",
      "Epoch: 1396 \n",
      " Hypothesis: tensor([[152.3242],\n",
      "        [184.0060],\n",
      "        [180.8258],\n",
      "        [196.9429],\n",
      "        [140.5570]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9492460489273071 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009704207070171833\n",
      "Epoch: 1397 \n",
      " Hypothesis: tensor([[152.3242],\n",
      "        [184.0061],\n",
      "        [180.8258],\n",
      "        [196.9429],\n",
      "        [140.5571]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9492064714431763 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009704344905912876\n",
      "Epoch: 1398 \n",
      " Hypothesis: tensor([[152.3241],\n",
      "        [184.0061],\n",
      "        [180.8258],\n",
      "        [196.9429],\n",
      "        [140.5571]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9491737484931946 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970448274165392\n",
      "Epoch: 1399 \n",
      " Hypothesis: tensor([[152.3241],\n",
      "        [184.0061],\n",
      "        [180.8258],\n",
      "        [196.9429],\n",
      "        [140.5571]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.949134349822998 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009704620577394962\n",
      "Epoch: 1400 \n",
      " Hypothesis: tensor([[152.3241],\n",
      "        [184.0061],\n",
      "        [180.8258],\n",
      "        [196.9428],\n",
      "        [140.5571]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9490987062454224 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009704758413136005\n",
      "Epoch: 1401 \n",
      " Hypothesis: tensor([[152.3240],\n",
      "        [184.0061],\n",
      "        [180.8258],\n",
      "        [196.9428],\n",
      "        [140.5572]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9490710496902466 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009704896248877048\n",
      "Epoch: 1402 \n",
      " Hypothesis: tensor([[152.3240],\n",
      "        [184.0062],\n",
      "        [180.8257],\n",
      "        [196.9428],\n",
      "        [140.5572]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9490383267402649 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009705034084618092\n",
      "Epoch: 1403 \n",
      " Hypothesis: tensor([[152.3240],\n",
      "        [184.0062],\n",
      "        [180.8257],\n",
      "        [196.9428],\n",
      "        [140.5572]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9489988088607788 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5973],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009705171920359135\n",
      "Epoch: 1404 \n",
      " Hypothesis: tensor([[152.3239],\n",
      "        [184.0062],\n",
      "        [180.8257],\n",
      "        [196.9428],\n",
      "        [140.5573]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9489604830741882 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009705309756100178\n",
      "Epoch: 1405 \n",
      " Hypothesis: tensor([[152.3239],\n",
      "        [184.0062],\n",
      "        [180.8257],\n",
      "        [196.9428],\n",
      "        [140.5573]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9489219784736633 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970544759184122\n",
      "Epoch: 1406 \n",
      " Hypothesis: tensor([[152.3239],\n",
      "        [184.0063],\n",
      "        [180.8257],\n",
      "        [196.9428],\n",
      "        [140.5573]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9488775134086609 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009705585427582264\n",
      "Epoch: 1407 \n",
      " Hypothesis: tensor([[152.3239],\n",
      "        [184.0063],\n",
      "        [180.8257],\n",
      "        [196.9428],\n",
      "        [140.5573]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9488638043403625 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009705723263323307\n",
      "Epoch: 1408 \n",
      " Hypothesis: tensor([[152.3238],\n",
      "        [184.0063],\n",
      "        [180.8257],\n",
      "        [196.9428],\n",
      "        [140.5574]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.948816180229187 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970586109906435\n",
      "Epoch: 1409 \n",
      " Hypothesis: tensor([[152.3238],\n",
      "        [184.0063],\n",
      "        [180.8257],\n",
      "        [196.9427],\n",
      "        [140.5574]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9487869143486023 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009705998934805393\n",
      "Epoch: 1410 \n",
      " Hypothesis: tensor([[152.3238],\n",
      "        [184.0063],\n",
      "        [180.8257],\n",
      "        [196.9427],\n",
      "        [140.5574]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9487393498420715 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009706136770546436\n",
      "Epoch: 1411 \n",
      " Hypothesis: tensor([[152.3237],\n",
      "        [184.0063],\n",
      "        [180.8257],\n",
      "        [196.9427],\n",
      "        [140.5575]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9487116932868958 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970627460628748\n",
      "Epoch: 1412 \n",
      " Hypothesis: tensor([[152.3237],\n",
      "        [184.0064],\n",
      "        [180.8257],\n",
      "        [196.9427],\n",
      "        [140.5575]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9486879110336304 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009706412442028522\n",
      "Epoch: 1413 \n",
      " Hypothesis: tensor([[152.3237],\n",
      "        [184.0064],\n",
      "        [180.8257],\n",
      "        [196.9427],\n",
      "        [140.5575]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9486484527587891 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009706550277769566\n",
      "Epoch: 1414 \n",
      " Hypothesis: tensor([[152.3236],\n",
      "        [184.0064],\n",
      "        [180.8256],\n",
      "        [196.9427],\n",
      "        [140.5575]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9486100077629089 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009706688113510609\n",
      "Epoch: 1415 \n",
      " Hypothesis: tensor([[152.3236],\n",
      "        [184.0064],\n",
      "        [180.8256],\n",
      "        [196.9427],\n",
      "        [140.5576]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9485763311386108 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009706825949251652\n",
      "Epoch: 1416 \n",
      " Hypothesis: tensor([[152.3236],\n",
      "        [184.0065],\n",
      "        [180.8256],\n",
      "        [196.9427],\n",
      "        [140.5576]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9485379457473755 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009706963784992695\n",
      "Epoch: 1417 \n",
      " Hypothesis: tensor([[152.3235],\n",
      "        [184.0065],\n",
      "        [180.8256],\n",
      "        [196.9427],\n",
      "        [140.5576]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9485052227973938 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009707101620733738\n",
      "Epoch: 1418 \n",
      " Hypothesis: tensor([[152.3235],\n",
      "        [184.0065],\n",
      "        [180.8256],\n",
      "        [196.9427],\n",
      "        [140.5576]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9484695196151733 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009707239456474781\n",
      "Epoch: 1419 \n",
      " Hypothesis: tensor([[152.3235],\n",
      "        [184.0065],\n",
      "        [180.8256],\n",
      "        [196.9427],\n",
      "        [140.5577]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9484419822692871 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009707377292215824\n",
      "Epoch: 1420 \n",
      " Hypothesis: tensor([[152.3235],\n",
      "        [184.0065],\n",
      "        [180.8256],\n",
      "        [196.9427],\n",
      "        [140.5577]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9484092593193054 \n",
      " W: tensor([[0.7323],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009707515127956867\n",
      "Epoch: 1421 \n",
      " Hypothesis: tensor([[152.3234],\n",
      "        [184.0066],\n",
      "        [180.8256],\n",
      "        [196.9427],\n",
      "        [140.5577]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9483698606491089 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970765296369791\n",
      "Epoch: 1422 \n",
      " Hypothesis: tensor([[152.3234],\n",
      "        [184.0066],\n",
      "        [180.8256],\n",
      "        [196.9427],\n",
      "        [140.5578]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9483254551887512 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009707790799438953\n",
      "Epoch: 1423 \n",
      " Hypothesis: tensor([[152.3234],\n",
      "        [184.0066],\n",
      "        [180.8256],\n",
      "        [196.9427],\n",
      "        [140.5578]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9482977986335754 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009707928635179996\n",
      "Epoch: 1424 \n",
      " Hypothesis: tensor([[152.3233],\n",
      "        [184.0066],\n",
      "        [180.8255],\n",
      "        [196.9427],\n",
      "        [140.5578]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9482600092887878 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970806647092104\n",
      "Epoch: 1425 \n",
      " Hypothesis: tensor([[152.3233],\n",
      "        [184.0067],\n",
      "        [180.8255],\n",
      "        [196.9426],\n",
      "        [140.5578]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9482237100601196 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009708204306662083\n",
      "Epoch: 1426 \n",
      " Hypothesis: tensor([[152.3233],\n",
      "        [184.0067],\n",
      "        [180.8255],\n",
      "        [196.9426],\n",
      "        [140.5579]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9481961131095886 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5972],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009708342142403126\n",
      "Epoch: 1427 \n",
      " Hypothesis: tensor([[152.3232],\n",
      "        [184.0067],\n",
      "        [180.8255],\n",
      "        [196.9426],\n",
      "        [140.5579]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9481489062309265 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009708479978144169\n",
      "Epoch: 1428 \n",
      " Hypothesis: tensor([[152.3232],\n",
      "        [184.0067],\n",
      "        [180.8255],\n",
      "        [196.9426],\n",
      "        [140.5580]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9481045007705688 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009708617813885212\n",
      "Epoch: 1429 \n",
      " Hypothesis: tensor([[152.3232],\n",
      "        [184.0067],\n",
      "        [180.8255],\n",
      "        [196.9426],\n",
      "        [140.5580]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9480817914009094 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009708755649626255\n",
      "Epoch: 1430 \n",
      " Hypothesis: tensor([[152.3232],\n",
      "        [184.0067],\n",
      "        [180.8255],\n",
      "        [196.9426],\n",
      "        [140.5580]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9480530023574829 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009708893485367298\n",
      "Epoch: 1431 \n",
      " Hypothesis: tensor([[152.3231],\n",
      "        [184.0068],\n",
      "        [180.8255],\n",
      "        [196.9426],\n",
      "        [140.5580]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9480233192443848 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009709031321108341\n",
      "Epoch: 1432 \n",
      " Hypothesis: tensor([[152.3231],\n",
      "        [184.0068],\n",
      "        [180.8255],\n",
      "        [196.9426],\n",
      "        [140.5580]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9479839205741882 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009709169156849384\n",
      "Epoch: 1433 \n",
      " Hypothesis: tensor([[152.3231],\n",
      "        [184.0068],\n",
      "        [180.8255],\n",
      "        [196.9426],\n",
      "        [140.5581]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.94794762134552 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009709306992590427\n",
      "Epoch: 1434 \n",
      " Hypothesis: tensor([[152.3230],\n",
      "        [184.0068],\n",
      "        [180.8255],\n",
      "        [196.9426],\n",
      "        [140.5581]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9479148983955383 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00970944482833147\n",
      "Epoch: 1435 \n",
      " Hypothesis: tensor([[152.3230],\n",
      "        [184.0069],\n",
      "        [180.8254],\n",
      "        [196.9426],\n",
      "        [140.5581]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9478791952133179 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009709582664072514\n",
      "Epoch: 1436 \n",
      " Hypothesis: tensor([[152.3230],\n",
      "        [184.0069],\n",
      "        [180.8254],\n",
      "        [196.9425],\n",
      "        [140.5582]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.947847843170166 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009709720499813557\n",
      "Epoch: 1437 \n",
      " Hypothesis: tensor([[152.3229],\n",
      "        [184.0069],\n",
      "        [180.8254],\n",
      "        [196.9425],\n",
      "        [140.5582]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9478121995925903 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.0097098583355546\n",
      "Epoch: 1438 \n",
      " Hypothesis: tensor([[152.3229],\n",
      "        [184.0069],\n",
      "        [180.8254],\n",
      "        [196.9425],\n",
      "        [140.5582]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.947769045829773 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009709996171295643\n",
      "Epoch: 1439 \n",
      " Hypothesis: tensor([[152.3229],\n",
      "        [184.0069],\n",
      "        [180.8254],\n",
      "        [196.9425],\n",
      "        [140.5583]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9477354288101196 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009710134007036686\n",
      "Epoch: 1440 \n",
      " Hypothesis: tensor([[152.3228],\n",
      "        [184.0070],\n",
      "        [180.8254],\n",
      "        [196.9425],\n",
      "        [140.5583]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9477017521858215 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009710271842777729\n",
      "Epoch: 1441 \n",
      " Hypothesis: tensor([[152.3228],\n",
      "        [184.0070],\n",
      "        [180.8254],\n",
      "        [196.9425],\n",
      "        [140.5583]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9476633071899414 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009710409678518772\n",
      "Epoch: 1442 \n",
      " Hypothesis: tensor([[152.3228],\n",
      "        [184.0070],\n",
      "        [180.8254],\n",
      "        [196.9425],\n",
      "        [140.5583]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9476249814033508 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009710547514259815\n",
      "Epoch: 1443 \n",
      " Hypothesis: tensor([[152.3228],\n",
      "        [184.0070],\n",
      "        [180.8254],\n",
      "        [196.9425],\n",
      "        [140.5584]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9475933313369751 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009710685350000858\n",
      "Epoch: 1444 \n",
      " Hypothesis: tensor([[152.3227],\n",
      "        [184.0070],\n",
      "        [180.8254],\n",
      "        [196.9425],\n",
      "        [140.5584]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9475674629211426 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009710823185741901\n",
      "Epoch: 1445 \n",
      " Hypothesis: tensor([[152.3227],\n",
      "        [184.0070],\n",
      "        [180.8253],\n",
      "        [196.9425],\n",
      "        [140.5584]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9475311040878296 \n",
      " W: tensor([[0.7324],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009710961021482944\n",
      "Epoch: 1446 \n",
      " Hypothesis: tensor([[152.3227],\n",
      "        [184.0071],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5585]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9474808573722839 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009711098857223988\n",
      "Epoch: 1447 \n",
      " Hypothesis: tensor([[152.3226],\n",
      "        [184.0071],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5585]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9474620819091797 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00971123669296503\n",
      "Epoch: 1448 \n",
      " Hypothesis: tensor([[152.3226],\n",
      "        [184.0071],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5585]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9474271535873413 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009711374528706074\n",
      "Epoch: 1449 \n",
      " Hypothesis: tensor([[152.3226],\n",
      "        [184.0071],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5585]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9473850131034851 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5971],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009711512364447117\n",
      "Epoch: 1450 \n",
      " Hypothesis: tensor([[152.3225],\n",
      "        [184.0072],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5586]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9473514556884766 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00971165020018816\n",
      "Epoch: 1451 \n",
      " Hypothesis: tensor([[152.3225],\n",
      "        [184.0072],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5586]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9473279118537903 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009711788035929203\n",
      "Epoch: 1452 \n",
      " Hypothesis: tensor([[152.3225],\n",
      "        [184.0072],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5586]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9472824335098267 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009711925871670246\n",
      "Epoch: 1453 \n",
      " Hypothesis: tensor([[152.3224],\n",
      "        [184.0072],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5587]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9472450017929077 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.00971206370741129\n",
      "Epoch: 1454 \n",
      " Hypothesis: tensor([[152.3224],\n",
      "        [184.0072],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5587]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9472193717956543 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009712201543152332\n",
      "Epoch: 1455 \n",
      " Hypothesis: tensor([[152.3224],\n",
      "        [184.0073],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5587]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9471787214279175 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009712339378893375\n",
      "Epoch: 1456 \n",
      " Hypothesis: tensor([[152.3224],\n",
      "        [184.0073],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5588]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9471500515937805 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009712477214634418\n",
      "Epoch: 1457 \n",
      " Hypothesis: tensor([[152.3223],\n",
      "        [184.0073],\n",
      "        [180.8253],\n",
      "        [196.9424],\n",
      "        [140.5588]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9471049308776855 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6810]], requires_grad=True) \n",
      " b: 0.009712615050375462\n",
      "Epoch: 1458 \n",
      " Hypothesis: tensor([[152.3223],\n",
      "        [184.0073],\n",
      "        [180.8252],\n",
      "        [196.9423],\n",
      "        [140.5588]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9470608830451965 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009712752886116505\n",
      "Epoch: 1459 \n",
      " Hypothesis: tensor([[152.3223],\n",
      "        [184.0073],\n",
      "        [180.8252],\n",
      "        [196.9423],\n",
      "        [140.5588]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9470369219779968 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009712890721857548\n",
      "Epoch: 1460 \n",
      " Hypothesis: tensor([[152.3223],\n",
      "        [184.0074],\n",
      "        [180.8252],\n",
      "        [196.9423],\n",
      "        [140.5589]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9470121264457703 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00971302855759859\n",
      "Epoch: 1461 \n",
      " Hypothesis: tensor([[152.3222],\n",
      "        [184.0074],\n",
      "        [180.8252],\n",
      "        [196.9423],\n",
      "        [140.5589]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9469707608222961 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009713166393339634\n",
      "Epoch: 1462 \n",
      " Hypothesis: tensor([[152.3222],\n",
      "        [184.0074],\n",
      "        [180.8252],\n",
      "        [196.9423],\n",
      "        [140.5589]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9469354748725891 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009713304229080677\n",
      "Epoch: 1463 \n",
      " Hypothesis: tensor([[152.3222],\n",
      "        [184.0074],\n",
      "        [180.8252],\n",
      "        [196.9423],\n",
      "        [140.5589]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9469016790390015 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00971344206482172\n",
      "Epoch: 1464 \n",
      " Hypothesis: tensor([[152.3221],\n",
      "        [184.0074],\n",
      "        [180.8252],\n",
      "        [196.9423],\n",
      "        [140.5590]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9468690752983093 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009713579900562763\n",
      "Epoch: 1465 \n",
      " Hypothesis: tensor([[152.3221],\n",
      "        [184.0075],\n",
      "        [180.8252],\n",
      "        [196.9423],\n",
      "        [140.5590]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9468414187431335 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009713717736303806\n",
      "Epoch: 1466 \n",
      " Hypothesis: tensor([[152.3221],\n",
      "        [184.0075],\n",
      "        [180.8251],\n",
      "        [196.9423],\n",
      "        [140.5590]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9467862844467163 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00971385557204485\n",
      "Epoch: 1467 \n",
      " Hypothesis: tensor([[152.3220],\n",
      "        [184.0075],\n",
      "        [180.8251],\n",
      "        [196.9423],\n",
      "        [140.5591]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9467586278915405 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009713993407785892\n",
      "Epoch: 1468 \n",
      " Hypothesis: tensor([[152.3220],\n",
      "        [184.0075],\n",
      "        [180.8251],\n",
      "        [196.9423],\n",
      "        [140.5591]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9467312097549438 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009714131243526936\n",
      "Epoch: 1469 \n",
      " Hypothesis: tensor([[152.3220],\n",
      "        [184.0076],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5591]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.946681022644043 \n",
      " W: tensor([[0.7325],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009714269079267979\n",
      "Epoch: 1470 \n",
      " Hypothesis: tensor([[152.3219],\n",
      "        [184.0076],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5592]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.946642279624939 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009714406915009022\n",
      "Epoch: 1471 \n",
      " Hypothesis: tensor([[152.3219],\n",
      "        [184.0076],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5592]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9466296434402466 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009714544750750065\n",
      "Epoch: 1472 \n",
      " Hypothesis: tensor([[152.3219],\n",
      "        [184.0076],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5592]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.94659024477005 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5970],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009714682586491108\n",
      "Epoch: 1473 \n",
      " Hypothesis: tensor([[152.3219],\n",
      "        [184.0076],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5592]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9465519189834595 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009714820422232151\n",
      "Epoch: 1474 \n",
      " Hypothesis: tensor([[152.3218],\n",
      "        [184.0077],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5593]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.946513295173645 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009714958257973194\n",
      "Epoch: 1475 \n",
      " Hypothesis: tensor([[152.3218],\n",
      "        [184.0077],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5593]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9464788436889648 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009715096093714237\n",
      "Epoch: 1476 \n",
      " Hypothesis: tensor([[152.3218],\n",
      "        [184.0077],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5593]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9464414715766907 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00971523392945528\n",
      "Epoch: 1477 \n",
      " Hypothesis: tensor([[152.3217],\n",
      "        [184.0077],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5594]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9464028477668762 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009715371765196323\n",
      "Epoch: 1478 \n",
      " Hypothesis: tensor([[152.3217],\n",
      "        [184.0077],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5594]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.946384072303772 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009715509600937366\n",
      "Epoch: 1479 \n",
      " Hypothesis: tensor([[152.3217],\n",
      "        [184.0078],\n",
      "        [180.8251],\n",
      "        [196.9422],\n",
      "        [140.5594]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9463504552841187 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00971564743667841\n",
      "Epoch: 1480 \n",
      " Hypothesis: tensor([[152.3216],\n",
      "        [184.0078],\n",
      "        [180.8250],\n",
      "        [196.9422],\n",
      "        [140.5594]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9463073015213013 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009715785272419453\n",
      "Epoch: 1481 \n",
      " Hypothesis: tensor([[152.3216],\n",
      "        [184.0078],\n",
      "        [180.8250],\n",
      "        [196.9422],\n",
      "        [140.5595]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.946273684501648 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009715923108160496\n",
      "Epoch: 1482 \n",
      " Hypothesis: tensor([[152.3216],\n",
      "        [184.0078],\n",
      "        [180.8250],\n",
      "        [196.9422],\n",
      "        [140.5595]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9462411999702454 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009716060943901539\n",
      "Epoch: 1483 \n",
      " Hypothesis: tensor([[152.3215],\n",
      "        [184.0078],\n",
      "        [180.8250],\n",
      "        [196.9421],\n",
      "        [140.5595]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9462018013000488 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009716198779642582\n",
      "Epoch: 1484 \n",
      " Hypothesis: tensor([[152.3215],\n",
      "        [184.0079],\n",
      "        [180.8250],\n",
      "        [196.9421],\n",
      "        [140.5596]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9461691975593567 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009716336615383625\n",
      "Epoch: 1485 \n",
      " Hypothesis: tensor([[152.3215],\n",
      "        [184.0079],\n",
      "        [180.8250],\n",
      "        [196.9421],\n",
      "        [140.5596]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9461297988891602 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009716474451124668\n",
      "Epoch: 1486 \n",
      " Hypothesis: tensor([[152.3215],\n",
      "        [184.0079],\n",
      "        [180.8250],\n",
      "        [196.9421],\n",
      "        [140.5596]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9460972547531128 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009716612286865711\n",
      "Epoch: 1487 \n",
      " Hypothesis: tensor([[152.3214],\n",
      "        [184.0079],\n",
      "        [180.8250],\n",
      "        [196.9421],\n",
      "        [140.5596]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9460645914077759 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009716750122606754\n",
      "Epoch: 1488 \n",
      " Hypothesis: tensor([[152.3214],\n",
      "        [184.0079],\n",
      "        [180.8250],\n",
      "        [196.9421],\n",
      "        [140.5597]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9460271596908569 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009716887958347797\n",
      "Epoch: 1489 \n",
      " Hypothesis: tensor([[152.3214],\n",
      "        [184.0080],\n",
      "        [180.8250],\n",
      "        [196.9421],\n",
      "        [140.5597]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9459918737411499 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00971702579408884\n",
      "Epoch: 1490 \n",
      " Hypothesis: tensor([[152.3214],\n",
      "        [184.0080],\n",
      "        [180.8250],\n",
      "        [196.9421],\n",
      "        [140.5597]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9459642171859741 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009717163629829884\n",
      "Epoch: 1491 \n",
      " Hypothesis: tensor([[152.3213],\n",
      "        [184.0080],\n",
      "        [180.8249],\n",
      "        [196.9421],\n",
      "        [140.5598]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9459199905395508 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009717301465570927\n",
      "Epoch: 1492 \n",
      " Hypothesis: tensor([[152.3213],\n",
      "        [184.0080],\n",
      "        [180.8249],\n",
      "        [196.9421],\n",
      "        [140.5598]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9458812475204468 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00971743930131197\n",
      "Epoch: 1493 \n",
      " Hypothesis: tensor([[152.3212],\n",
      "        [184.0080],\n",
      "        [180.8249],\n",
      "        [196.9420],\n",
      "        [140.5598]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9458521008491516 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009717577137053013\n",
      "Epoch: 1494 \n",
      " Hypothesis: tensor([[152.3212],\n",
      "        [184.0081],\n",
      "        [180.8249],\n",
      "        [196.9420],\n",
      "        [140.5598]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9458076357841492 \n",
      " W: tensor([[0.7326],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009717714972794056\n",
      "Epoch: 1495 \n",
      " Hypothesis: tensor([[152.3212],\n",
      "        [184.0081],\n",
      "        [180.8249],\n",
      "        [196.9420],\n",
      "        [140.5599]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9457888603210449 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009717852808535099\n",
      "Epoch: 1496 \n",
      " Hypothesis: tensor([[152.3212],\n",
      "        [184.0081],\n",
      "        [180.8249],\n",
      "        [196.9420],\n",
      "        [140.5599]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.94575035572052 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5969],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009717990644276142\n",
      "Epoch: 1497 \n",
      " Hypothesis: tensor([[152.3211],\n",
      "        [184.0081],\n",
      "        [180.8249],\n",
      "        [196.9420],\n",
      "        [140.5599]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9457246661186218 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009718128480017185\n",
      "Epoch: 1498 \n",
      " Hypothesis: tensor([[152.3211],\n",
      "        [184.0081],\n",
      "        [180.8249],\n",
      "        [196.9420],\n",
      "        [140.5600]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9456846117973328 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009718266315758228\n",
      "Epoch: 1499 \n",
      " Hypothesis: tensor([[152.3211],\n",
      "        [184.0082],\n",
      "        [180.8249],\n",
      "        [196.9420],\n",
      "        [140.5600]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9456480145454407 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009718404151499271\n",
      "Epoch: 1500 \n",
      " Hypothesis: tensor([[152.3210],\n",
      "        [184.0082],\n",
      "        [180.8248],\n",
      "        [196.9420],\n",
      "        [140.5600]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9456154108047485 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009718541987240314\n",
      "Epoch: 1501 \n",
      " Hypothesis: tensor([[152.3210],\n",
      "        [184.0082],\n",
      "        [180.8248],\n",
      "        [196.9420],\n",
      "        [140.5600]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9455702900886536 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009718679822981358\n",
      "Epoch: 1502 \n",
      " Hypothesis: tensor([[152.3210],\n",
      "        [184.0082],\n",
      "        [180.8248],\n",
      "        [196.9420],\n",
      "        [140.5601]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9455428123474121 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.0097188176587224\n",
      "Epoch: 1503 \n",
      " Hypothesis: tensor([[152.3209],\n",
      "        [184.0083],\n",
      "        [180.8248],\n",
      "        [196.9420],\n",
      "        [140.5601]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9455021619796753 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009718955494463444\n",
      "Epoch: 1504 \n",
      " Hypothesis: tensor([[152.3209],\n",
      "        [184.0083],\n",
      "        [180.8248],\n",
      "        [196.9420],\n",
      "        [140.5601]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9454658627510071 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009719093330204487\n",
      "Epoch: 1505 \n",
      " Hypothesis: tensor([[152.3209],\n",
      "        [184.0083],\n",
      "        [180.8248],\n",
      "        [196.9419],\n",
      "        [140.5602]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9454324841499329 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00971923116594553\n",
      "Epoch: 1506 \n",
      " Hypothesis: tensor([[152.3209],\n",
      "        [184.0083],\n",
      "        [180.8248],\n",
      "        [196.9419],\n",
      "        [140.5602]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9453938603401184 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009719369001686573\n",
      "Epoch: 1507 \n",
      " Hypothesis: tensor([[152.3208],\n",
      "        [184.0083],\n",
      "        [180.8248],\n",
      "        [196.9419],\n",
      "        [140.5602]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9453639984130859 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009719506837427616\n",
      "Epoch: 1508 \n",
      " Hypothesis: tensor([[152.3208],\n",
      "        [184.0084],\n",
      "        [180.8248],\n",
      "        [196.9419],\n",
      "        [140.5602]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9453307390213013 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00971964467316866\n",
      "Epoch: 1509 \n",
      " Hypothesis: tensor([[152.3208],\n",
      "        [184.0084],\n",
      "        [180.8248],\n",
      "        [196.9419],\n",
      "        [140.5603]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9452943801879883 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009719782508909702\n",
      "Epoch: 1510 \n",
      " Hypothesis: tensor([[152.3207],\n",
      "        [184.0084],\n",
      "        [180.8248],\n",
      "        [196.9419],\n",
      "        [140.5603]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.945245087146759 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009719920344650745\n",
      "Epoch: 1511 \n",
      " Hypothesis: tensor([[152.3207],\n",
      "        [184.0084],\n",
      "        [180.8248],\n",
      "        [196.9419],\n",
      "        [140.5603]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9452263116836548 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009720058180391788\n",
      "Epoch: 1512 \n",
      " Hypothesis: tensor([[152.3207],\n",
      "        [184.0084],\n",
      "        [180.8247],\n",
      "        [196.9419],\n",
      "        [140.5603]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9451967477798462 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009720196016132832\n",
      "Epoch: 1513 \n",
      " Hypothesis: tensor([[152.3206],\n",
      "        [184.0085],\n",
      "        [180.8247],\n",
      "        [196.9419],\n",
      "        [140.5604]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9451583623886108 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009720333851873875\n",
      "Epoch: 1514 \n",
      " Hypothesis: tensor([[152.3206],\n",
      "        [184.0085],\n",
      "        [180.8247],\n",
      "        [196.9419],\n",
      "        [140.5604]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9451297521591187 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009720471687614918\n",
      "Epoch: 1515 \n",
      " Hypothesis: tensor([[152.3206],\n",
      "        [184.0085],\n",
      "        [180.8247],\n",
      "        [196.9419],\n",
      "        [140.5604]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9450972676277161 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972060952335596\n",
      "Epoch: 1516 \n",
      " Hypothesis: tensor([[152.3206],\n",
      "        [184.0085],\n",
      "        [180.8247],\n",
      "        [196.9419],\n",
      "        [140.5605]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.945046067237854 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009720747359097004\n",
      "Epoch: 1517 \n",
      " Hypothesis: tensor([[152.3205],\n",
      "        [184.0085],\n",
      "        [180.8247],\n",
      "        [196.9418],\n",
      "        [140.5605]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9450145959854126 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009720885194838047\n",
      "Epoch: 1518 \n",
      " Hypothesis: tensor([[152.3205],\n",
      "        [184.0086],\n",
      "        [180.8247],\n",
      "        [196.9418],\n",
      "        [140.5605]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.944989025592804 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972102303057909\n",
      "Epoch: 1519 \n",
      " Hypothesis: tensor([[152.3205],\n",
      "        [184.0086],\n",
      "        [180.8247],\n",
      "        [196.9418],\n",
      "        [140.5606]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.944948673248291 \n",
      " W: tensor([[0.7327],\n",
      "        [0.5968],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009721160866320133\n",
      "Epoch: 1520 \n",
      " Hypothesis: tensor([[152.3204],\n",
      "        [184.0086],\n",
      "        [180.8247],\n",
      "        [196.9418],\n",
      "        [140.5606]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9449103474617004 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009721298702061176\n",
      "Epoch: 1521 \n",
      " Hypothesis: tensor([[152.3204],\n",
      "        [184.0086],\n",
      "        [180.8247],\n",
      "        [196.9418],\n",
      "        [140.5606]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9448768496513367 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972143653780222\n",
      "Epoch: 1522 \n",
      " Hypothesis: tensor([[152.3204],\n",
      "        [184.0086],\n",
      "        [180.8247],\n",
      "        [196.9418],\n",
      "        [140.5607]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9448434710502625 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009721574373543262\n",
      "Epoch: 1523 \n",
      " Hypothesis: tensor([[152.3203],\n",
      "        [184.0087],\n",
      "        [180.8246],\n",
      "        [196.9418],\n",
      "        [140.5607]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9448140263557434 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009721712209284306\n",
      "Epoch: 1524 \n",
      " Hypothesis: tensor([[152.3203],\n",
      "        [184.0087],\n",
      "        [180.8246],\n",
      "        [196.9418],\n",
      "        [140.5607]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.944775402545929 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009721850045025349\n",
      "Epoch: 1525 \n",
      " Hypothesis: tensor([[152.3203],\n",
      "        [184.0087],\n",
      "        [180.8246],\n",
      "        [196.9418],\n",
      "        [140.5607]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9447437524795532 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009721987880766392\n",
      "Epoch: 1526 \n",
      " Hypothesis: tensor([[152.3203],\n",
      "        [184.0087],\n",
      "        [180.8246],\n",
      "        [196.9418],\n",
      "        [140.5608]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9447038769721985 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009722125716507435\n",
      "Epoch: 1527 \n",
      " Hypothesis: tensor([[152.3202],\n",
      "        [184.0087],\n",
      "        [180.8246],\n",
      "        [196.9418],\n",
      "        [140.5608]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9446671605110168 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009722263552248478\n",
      "Epoch: 1528 \n",
      " Hypothesis: tensor([[152.3202],\n",
      "        [184.0088],\n",
      "        [180.8246],\n",
      "        [196.9418],\n",
      "        [140.5608]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9446403384208679 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009722401387989521\n",
      "Epoch: 1529 \n",
      " Hypothesis: tensor([[152.3202],\n",
      "        [184.0088],\n",
      "        [180.8246],\n",
      "        [196.9418],\n",
      "        [140.5608]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9446078538894653 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009722539223730564\n",
      "Epoch: 1530 \n",
      " Hypothesis: tensor([[152.3201],\n",
      "        [184.0088],\n",
      "        [180.8246],\n",
      "        [196.9417],\n",
      "        [140.5609]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.94456946849823 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009722677059471607\n",
      "Epoch: 1531 \n",
      " Hypothesis: tensor([[152.3201],\n",
      "        [184.0088],\n",
      "        [180.8246],\n",
      "        [196.9417],\n",
      "        [140.5609]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.944530189037323 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972281489521265\n",
      "Epoch: 1532 \n",
      " Hypothesis: tensor([[152.3201],\n",
      "        [184.0089],\n",
      "        [180.8246],\n",
      "        [196.9417],\n",
      "        [140.5609]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9445026516914368 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009722952730953693\n",
      "Epoch: 1533 \n",
      " Hypothesis: tensor([[152.3201],\n",
      "        [184.0089],\n",
      "        [180.8246],\n",
      "        [196.9417],\n",
      "        [140.5610]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.944454550743103 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009723090566694736\n",
      "Epoch: 1534 \n",
      " Hypothesis: tensor([[152.3200],\n",
      "        [184.0089],\n",
      "        [180.8245],\n",
      "        [196.9417],\n",
      "        [140.5610]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9444219470024109 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972322840243578\n",
      "Epoch: 1535 \n",
      " Hypothesis: tensor([[152.3200],\n",
      "        [184.0089],\n",
      "        [180.8245],\n",
      "        [196.9417],\n",
      "        [140.5610]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9443944096565247 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009723366238176823\n",
      "Epoch: 1536 \n",
      " Hypothesis: tensor([[152.3200],\n",
      "        [184.0089],\n",
      "        [180.8245],\n",
      "        [196.9417],\n",
      "        [140.5611]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.944355845451355 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009723504073917866\n",
      "Epoch: 1537 \n",
      " Hypothesis: tensor([[152.3199],\n",
      "        [184.0090],\n",
      "        [180.8245],\n",
      "        [196.9417],\n",
      "        [140.5611]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9443098902702332 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009723641909658909\n",
      "Epoch: 1538 \n",
      " Hypothesis: tensor([[152.3199],\n",
      "        [184.0090],\n",
      "        [180.8245],\n",
      "        [196.9417],\n",
      "        [140.5611]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9442782402038574 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009723779745399952\n",
      "Epoch: 1539 \n",
      " Hypothesis: tensor([[152.3199],\n",
      "        [184.0090],\n",
      "        [180.8245],\n",
      "        [196.9417],\n",
      "        [140.5611]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9442459940910339 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009723917581140995\n",
      "Epoch: 1540 \n",
      " Hypothesis: tensor([[152.3198],\n",
      "        [184.0090],\n",
      "        [180.8245],\n",
      "        [196.9417],\n",
      "        [140.5612]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9442253112792969 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009724055416882038\n",
      "Epoch: 1541 \n",
      " Hypothesis: tensor([[152.3198],\n",
      "        [184.0090],\n",
      "        [180.8245],\n",
      "        [196.9416],\n",
      "        [140.5612]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.944180965423584 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009724193252623081\n",
      "Epoch: 1542 \n",
      " Hypothesis: tensor([[152.3198],\n",
      "        [184.0090],\n",
      "        [180.8245],\n",
      "        [196.9416],\n",
      "        [140.5612]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9441503286361694 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5967],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009724331088364124\n",
      "Epoch: 1543 \n",
      " Hypothesis: tensor([[152.3197],\n",
      "        [184.0091],\n",
      "        [180.8245],\n",
      "        [196.9416],\n",
      "        [140.5612]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9441167712211609 \n",
      " W: tensor([[0.7328],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009724468924105167\n",
      "Epoch: 1544 \n",
      " Hypothesis: tensor([[152.3197],\n",
      "        [184.0091],\n",
      "        [180.8244],\n",
      "        [196.9416],\n",
      "        [140.5613]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9440785646438599 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972460675984621\n",
      "Epoch: 1545 \n",
      " Hypothesis: tensor([[152.3197],\n",
      "        [184.0091],\n",
      "        [180.8244],\n",
      "        [196.9416],\n",
      "        [140.5613]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9440452456474304 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009724744595587254\n",
      "Epoch: 1546 \n",
      " Hypothesis: tensor([[152.3197],\n",
      "        [184.0091],\n",
      "        [180.8244],\n",
      "        [196.9416],\n",
      "        [140.5613]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9440155029296875 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009724882431328297\n",
      "Epoch: 1547 \n",
      " Hypothesis: tensor([[152.3196],\n",
      "        [184.0092],\n",
      "        [180.8244],\n",
      "        [196.9416],\n",
      "        [140.5614]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.943976879119873 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972502026706934\n",
      "Epoch: 1548 \n",
      " Hypothesis: tensor([[152.3196],\n",
      "        [184.0092],\n",
      "        [180.8244],\n",
      "        [196.9416],\n",
      "        [140.5614]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9439493417739868 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009725158102810383\n",
      "Epoch: 1549 \n",
      " Hypothesis: tensor([[152.3195],\n",
      "        [184.0092],\n",
      "        [180.8244],\n",
      "        [196.9416],\n",
      "        [140.5614]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9439033269882202 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009725295938551426\n",
      "Epoch: 1550 \n",
      " Hypothesis: tensor([[152.3195],\n",
      "        [184.0092],\n",
      "        [180.8244],\n",
      "        [196.9416],\n",
      "        [140.5614]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9438667297363281 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009725433774292469\n",
      "Epoch: 1551 \n",
      " Hypothesis: tensor([[152.3195],\n",
      "        [184.0092],\n",
      "        [180.8244],\n",
      "        [196.9416],\n",
      "        [140.5615]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9438392519950867 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009725571610033512\n",
      "Epoch: 1552 \n",
      " Hypothesis: tensor([[152.3195],\n",
      "        [184.0093],\n",
      "        [180.8244],\n",
      "        [196.9416],\n",
      "        [140.5615]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9438009262084961 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009725709445774555\n",
      "Epoch: 1553 \n",
      " Hypothesis: tensor([[152.3194],\n",
      "        [184.0093],\n",
      "        [180.8244],\n",
      "        [196.9415],\n",
      "        [140.5615]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9437616467475891 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009725847281515598\n",
      "Epoch: 1554 \n",
      " Hypothesis: tensor([[152.3194],\n",
      "        [184.0093],\n",
      "        [180.8244],\n",
      "        [196.9415],\n",
      "        [140.5616]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9437291026115417 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009725985117256641\n",
      "Epoch: 1555 \n",
      " Hypothesis: tensor([[152.3194],\n",
      "        [184.0093],\n",
      "        [180.8244],\n",
      "        [196.9415],\n",
      "        [140.5616]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9436958432197571 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009726122952997684\n",
      "Epoch: 1556 \n",
      " Hypothesis: tensor([[152.3194],\n",
      "        [184.0094],\n",
      "        [180.8243],\n",
      "        [196.9415],\n",
      "        [140.5616]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9436514973640442 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009726260788738728\n",
      "Epoch: 1557 \n",
      " Hypothesis: tensor([[152.3193],\n",
      "        [184.0094],\n",
      "        [180.8243],\n",
      "        [196.9415],\n",
      "        [140.5617]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9436240196228027 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972639862447977\n",
      "Epoch: 1558 \n",
      " Hypothesis: tensor([[152.3193],\n",
      "        [184.0094],\n",
      "        [180.8243],\n",
      "        [196.9415],\n",
      "        [140.5617]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9435884356498718 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009726536460220814\n",
      "Epoch: 1559 \n",
      " Hypothesis: tensor([[152.3193],\n",
      "        [184.0094],\n",
      "        [180.8243],\n",
      "        [196.9415],\n",
      "        [140.5617]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9435502290725708 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009726674295961857\n",
      "Epoch: 1560 \n",
      " Hypothesis: tensor([[152.3192],\n",
      "        [184.0094],\n",
      "        [180.8243],\n",
      "        [196.9415],\n",
      "        [140.5617]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9435166120529175 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.0097268121317029\n",
      "Epoch: 1561 \n",
      " Hypothesis: tensor([[152.3192],\n",
      "        [184.0094],\n",
      "        [180.8243],\n",
      "        [196.9415],\n",
      "        [140.5618]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9434901475906372 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009726949967443943\n",
      "Epoch: 1562 \n",
      " Hypothesis: tensor([[152.3192],\n",
      "        [184.0095],\n",
      "        [180.8243],\n",
      "        [196.9415],\n",
      "        [140.5618]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9434565305709839 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009727087803184986\n",
      "Epoch: 1563 \n",
      " Hypothesis: tensor([[152.3191],\n",
      "        [184.0095],\n",
      "        [180.8243],\n",
      "        [196.9415],\n",
      "        [140.5618]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.94341641664505 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972722563892603\n",
      "Epoch: 1564 \n",
      " Hypothesis: tensor([[152.3191],\n",
      "        [184.0095],\n",
      "        [180.8243],\n",
      "        [196.9415],\n",
      "        [140.5618]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.943388819694519 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009727363474667072\n",
      "Epoch: 1565 \n",
      " Hypothesis: tensor([[152.3191],\n",
      "        [184.0095],\n",
      "        [180.8243],\n",
      "        [196.9414],\n",
      "        [140.5619]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9433437585830688 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009727501310408115\n",
      "Epoch: 1566 \n",
      " Hypothesis: tensor([[152.3191],\n",
      "        [184.0096],\n",
      "        [180.8243],\n",
      "        [196.9414],\n",
      "        [140.5619]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9433091878890991 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5966],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009727639146149158\n",
      "Epoch: 1567 \n",
      " Hypothesis: tensor([[152.3190],\n",
      "        [184.0096],\n",
      "        [180.8242],\n",
      "        [196.9414],\n",
      "        [140.5620]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.943276584148407 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009727776981890202\n",
      "Epoch: 1568 \n",
      " Hypothesis: tensor([[152.3190],\n",
      "        [184.0096],\n",
      "        [180.8242],\n",
      "        [196.9414],\n",
      "        [140.5620]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9432326555252075 \n",
      " W: tensor([[0.7329],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009727914817631245\n",
      "Epoch: 1569 \n",
      " Hypothesis: tensor([[152.3190],\n",
      "        [184.0096],\n",
      "        [180.8242],\n",
      "        [196.9414],\n",
      "        [140.5620]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9432051777839661 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009728052653372288\n",
      "Epoch: 1570 \n",
      " Hypothesis: tensor([[152.3189],\n",
      "        [184.0096],\n",
      "        [180.8242],\n",
      "        [196.9414],\n",
      "        [140.5620]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9431753158569336 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972819048911333\n",
      "Epoch: 1571 \n",
      " Hypothesis: tensor([[152.3189],\n",
      "        [184.0097],\n",
      "        [180.8242],\n",
      "        [196.9414],\n",
      "        [140.5620]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9431381225585938 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009728328324854374\n",
      "Epoch: 1572 \n",
      " Hypothesis: tensor([[152.3189],\n",
      "        [184.0097],\n",
      "        [180.8242],\n",
      "        [196.9414],\n",
      "        [140.5621]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9430988430976868 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009728466160595417\n",
      "Epoch: 1573 \n",
      " Hypothesis: tensor([[152.3188],\n",
      "        [184.0097],\n",
      "        [180.8242],\n",
      "        [196.9414],\n",
      "        [140.5621]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9430652856826782 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972860399633646\n",
      "Epoch: 1574 \n",
      " Hypothesis: tensor([[152.3188],\n",
      "        [184.0097],\n",
      "        [180.8242],\n",
      "        [196.9414],\n",
      "        [140.5621]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9430338740348816 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009728741832077503\n",
      "Epoch: 1575 \n",
      " Hypothesis: tensor([[152.3188],\n",
      "        [184.0097],\n",
      "        [180.8242],\n",
      "        [196.9413],\n",
      "        [140.5622]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.942995548248291 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009728879667818546\n",
      "Epoch: 1576 \n",
      " Hypothesis: tensor([[152.3187],\n",
      "        [184.0098],\n",
      "        [180.8242],\n",
      "        [196.9413],\n",
      "        [140.5622]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9429620504379272 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00972901750355959\n",
      "Epoch: 1577 \n",
      " Hypothesis: tensor([[152.3187],\n",
      "        [184.0098],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5622]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.942935585975647 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009729155339300632\n",
      "Epoch: 1578 \n",
      " Hypothesis: tensor([[152.3187],\n",
      "        [184.0098],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5623]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9428962469100952 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009729293175041676\n",
      "Epoch: 1579 \n",
      " Hypothesis: tensor([[152.3187],\n",
      "        [184.0098],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5623]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9428539276123047 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009729431010782719\n",
      "Epoch: 1580 \n",
      " Hypothesis: tensor([[152.3186],\n",
      "        [184.0099],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5623]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9428204298019409 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009729568846523762\n",
      "Epoch: 1581 \n",
      " Hypothesis: tensor([[152.3186],\n",
      "        [184.0099],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5623]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9427940249443054 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009729706682264805\n",
      "Epoch: 1582 \n",
      " Hypothesis: tensor([[152.3186],\n",
      "        [184.0099],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5624]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9427556991577148 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009729844518005848\n",
      "Epoch: 1583 \n",
      " Hypothesis: tensor([[152.3185],\n",
      "        [184.0099],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5624]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.942723274230957 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009729982353746891\n",
      "Epoch: 1584 \n",
      " Hypothesis: tensor([[152.3185],\n",
      "        [184.0099],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5624]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9426926374435425 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009730120189487934\n",
      "Epoch: 1585 \n",
      " Hypothesis: tensor([[152.3185],\n",
      "        [184.0099],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5625]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9426456689834595 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009730258025228977\n",
      "Epoch: 1586 \n",
      " Hypothesis: tensor([[152.3184],\n",
      "        [184.0100],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5625]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9426044225692749 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973039586097002\n",
      "Epoch: 1587 \n",
      " Hypothesis: tensor([[152.3184],\n",
      "        [184.0100],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5625]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.942587673664093 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009730533696711063\n",
      "Epoch: 1588 \n",
      " Hypothesis: tensor([[152.3184],\n",
      "        [184.0100],\n",
      "        [180.8241],\n",
      "        [196.9413],\n",
      "        [140.5625]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9425579309463501 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009730671532452106\n",
      "Epoch: 1589 \n",
      " Hypothesis: tensor([[152.3184],\n",
      "        [184.0100],\n",
      "        [180.8240],\n",
      "        [196.9412],\n",
      "        [140.5626]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9425109028816223 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5965],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973080936819315\n",
      "Epoch: 1590 \n",
      " Hypothesis: tensor([[152.3183],\n",
      "        [184.0100],\n",
      "        [180.8240],\n",
      "        [196.9412],\n",
      "        [140.5626]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9424777030944824 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009730947203934193\n",
      "Epoch: 1591 \n",
      " Hypothesis: tensor([[152.3183],\n",
      "        [184.0101],\n",
      "        [180.8240],\n",
      "        [196.9412],\n",
      "        [140.5626]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9424391984939575 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009731085039675236\n",
      "Epoch: 1592 \n",
      " Hypothesis: tensor([[152.3183],\n",
      "        [184.0101],\n",
      "        [180.8240],\n",
      "        [196.9412],\n",
      "        [140.5627]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9424144625663757 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009731222875416279\n",
      "Epoch: 1593 \n",
      " Hypothesis: tensor([[152.3182],\n",
      "        [184.0101],\n",
      "        [180.8240],\n",
      "        [196.9412],\n",
      "        [140.5627]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9423745274543762 \n",
      " W: tensor([[0.7330],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009731360711157322\n",
      "Epoch: 1594 \n",
      " Hypothesis: tensor([[152.3182],\n",
      "        [184.0101],\n",
      "        [180.8240],\n",
      "        [196.9412],\n",
      "        [140.5627]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9423321485519409 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009731498546898365\n",
      "Epoch: 1595 \n",
      " Hypothesis: tensor([[152.3182],\n",
      "        [184.0101],\n",
      "        [180.8240],\n",
      "        [196.9412],\n",
      "        [140.5627]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9423053860664368 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009731636382639408\n",
      "Epoch: 1596 \n",
      " Hypothesis: tensor([[152.3181],\n",
      "        [184.0102],\n",
      "        [180.8240],\n",
      "        [196.9412],\n",
      "        [140.5628]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9422664642333984 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009731774218380451\n",
      "Epoch: 1597 \n",
      " Hypothesis: tensor([[152.3181],\n",
      "        [184.0102],\n",
      "        [180.8240],\n",
      "        [196.9412],\n",
      "        [140.5628]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9422279596328735 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009731912054121494\n",
      "Epoch: 1598 \n",
      " Hypothesis: tensor([[152.3181],\n",
      "        [184.0102],\n",
      "        [180.8240],\n",
      "        [196.9412],\n",
      "        [140.5628]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9422004818916321 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009732049889862537\n",
      "Epoch: 1599 \n",
      " Hypothesis: tensor([[152.3181],\n",
      "        [184.0102],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5629]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9421648979187012 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973218772560358\n",
      "Epoch: 1600 \n",
      " Hypothesis: tensor([[152.3180],\n",
      "        [184.0103],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5629]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9421229362487793 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009732325561344624\n",
      "Epoch: 1601 \n",
      " Hypothesis: tensor([[152.3180],\n",
      "        [184.0103],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5629]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9420905113220215 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009732463397085667\n",
      "Epoch: 1602 \n",
      " Hypothesis: tensor([[152.3180],\n",
      "        [184.0103],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5630]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9420569539070129 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973260123282671\n",
      "Epoch: 1603 \n",
      " Hypothesis: tensor([[152.3179],\n",
      "        [184.0103],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5630]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9420362710952759 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009732739068567753\n",
      "Epoch: 1604 \n",
      " Hypothesis: tensor([[152.3179],\n",
      "        [184.0103],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5630]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9419980049133301 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009732876904308796\n",
      "Epoch: 1605 \n",
      " Hypothesis: tensor([[152.3179],\n",
      "        [184.0103],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5630]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9419566988945007 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009733014740049839\n",
      "Epoch: 1606 \n",
      " Hypothesis: tensor([[152.3178],\n",
      "        [184.0104],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5631]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9419232606887817 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009733152575790882\n",
      "Epoch: 1607 \n",
      " Hypothesis: tensor([[152.3178],\n",
      "        [184.0104],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5631]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9418940544128418 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009733290411531925\n",
      "Epoch: 1608 \n",
      " Hypothesis: tensor([[152.3178],\n",
      "        [184.0104],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5631]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9418556094169617 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009733428247272968\n",
      "Epoch: 1609 \n",
      " Hypothesis: tensor([[152.3177],\n",
      "        [184.0104],\n",
      "        [180.8239],\n",
      "        [196.9411],\n",
      "        [140.5631]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9418221712112427 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009733566083014011\n",
      "Epoch: 1610 \n",
      " Hypothesis: tensor([[152.3177],\n",
      "        [184.0105],\n",
      "        [180.8238],\n",
      "        [196.9411],\n",
      "        [140.5632]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9417837858200073 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009733703918755054\n",
      "Epoch: 1611 \n",
      " Hypothesis: tensor([[152.3177],\n",
      "        [184.0105],\n",
      "        [180.8238],\n",
      "        [196.9410],\n",
      "        [140.5632]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9417506456375122 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009733841754496098\n",
      "Epoch: 1612 \n",
      " Hypothesis: tensor([[152.3177],\n",
      "        [184.0105],\n",
      "        [180.8238],\n",
      "        [196.9410],\n",
      "        [140.5632]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9417208433151245 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5964],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973397959023714\n",
      "Epoch: 1613 \n",
      " Hypothesis: tensor([[152.3176],\n",
      "        [184.0105],\n",
      "        [180.8238],\n",
      "        [196.9410],\n",
      "        [140.5633]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9416731595993042 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009734117425978184\n",
      "Epoch: 1614 \n",
      " Hypothesis: tensor([[152.3176],\n",
      "        [184.0105],\n",
      "        [180.8238],\n",
      "        [196.9410],\n",
      "        [140.5633]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9416346549987793 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009734255261719227\n",
      "Epoch: 1615 \n",
      " Hypothesis: tensor([[152.3176],\n",
      "        [184.0106],\n",
      "        [180.8238],\n",
      "        [196.9410],\n",
      "        [140.5633]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9416022300720215 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973439309746027\n",
      "Epoch: 1616 \n",
      " Hypothesis: tensor([[152.3175],\n",
      "        [184.0106],\n",
      "        [180.8238],\n",
      "        [196.9410],\n",
      "        [140.5633]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9415717124938965 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009734530933201313\n",
      "Epoch: 1617 \n",
      " Hypothesis: tensor([[152.3175],\n",
      "        [184.0106],\n",
      "        [180.8238],\n",
      "        [196.9410],\n",
      "        [140.5634]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9415451884269714 \n",
      " W: tensor([[0.7331],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009734668768942356\n",
      "Epoch: 1618 \n",
      " Hypothesis: tensor([[152.3175],\n",
      "        [184.0106],\n",
      "        [180.8238],\n",
      "        [196.9410],\n",
      "        [140.5634]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.941506028175354 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.0097348066046834\n",
      "Epoch: 1619 \n",
      " Hypothesis: tensor([[152.3174],\n",
      "        [184.0107],\n",
      "        [180.8238],\n",
      "        [196.9410],\n",
      "        [140.5634]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9414674639701843 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009734944440424442\n",
      "Epoch: 1620 \n",
      " Hypothesis: tensor([[152.3174],\n",
      "        [184.0107],\n",
      "        [180.8237],\n",
      "        [196.9410],\n",
      "        [140.5635]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9414410591125488 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009735082276165485\n",
      "Epoch: 1621 \n",
      " Hypothesis: tensor([[152.3174],\n",
      "        [184.0107],\n",
      "        [180.8237],\n",
      "        [196.9410],\n",
      "        [140.5635]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9414017796516418 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009735220111906528\n",
      "Epoch: 1622 \n",
      " Hypothesis: tensor([[152.3174],\n",
      "        [184.0107],\n",
      "        [180.8237],\n",
      "        [196.9409],\n",
      "        [140.5635]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9413663744926453 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009735357947647572\n",
      "Epoch: 1623 \n",
      " Hypothesis: tensor([[152.3173],\n",
      "        [184.0107],\n",
      "        [180.8237],\n",
      "        [196.9409],\n",
      "        [140.5635]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9413341283798218 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009735495783388615\n",
      "Epoch: 1624 \n",
      " Hypothesis: tensor([[152.3173],\n",
      "        [184.0107],\n",
      "        [180.8237],\n",
      "        [196.9409],\n",
      "        [140.5636]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9412959814071655 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009735633619129658\n",
      "Epoch: 1625 \n",
      " Hypothesis: tensor([[152.3173],\n",
      "        [184.0108],\n",
      "        [180.8237],\n",
      "        [196.9409],\n",
      "        [140.5636]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.941262423992157 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.0097357714548707\n",
      "Epoch: 1626 \n",
      " Hypothesis: tensor([[152.3172],\n",
      "        [184.0108],\n",
      "        [180.8237],\n",
      "        [196.9409],\n",
      "        [140.5636]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9412349462509155 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009735909290611744\n",
      "Epoch: 1627 \n",
      " Hypothesis: tensor([[152.3172],\n",
      "        [184.0108],\n",
      "        [180.8237],\n",
      "        [196.9409],\n",
      "        [140.5636]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9411938786506653 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009736047126352787\n",
      "Epoch: 1628 \n",
      " Hypothesis: tensor([[152.3172],\n",
      "        [184.0108],\n",
      "        [180.8237],\n",
      "        [196.9409],\n",
      "        [140.5637]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9411613345146179 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973618496209383\n",
      "Epoch: 1629 \n",
      " Hypothesis: tensor([[152.3171],\n",
      "        [184.0108],\n",
      "        [180.8237],\n",
      "        [196.9409],\n",
      "        [140.5637]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9411338567733765 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009736322797834873\n",
      "Epoch: 1630 \n",
      " Hypothesis: tensor([[152.3171],\n",
      "        [184.0109],\n",
      "        [180.8236],\n",
      "        [196.9409],\n",
      "        [140.5637]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9410845637321472 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009736460633575916\n",
      "Epoch: 1631 \n",
      " Hypothesis: tensor([[152.3171],\n",
      "        [184.0109],\n",
      "        [180.8237],\n",
      "        [196.9409],\n",
      "        [140.5638]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9410565495491028 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973659846931696\n",
      "Epoch: 1632 \n",
      " Hypothesis: tensor([[152.3170],\n",
      "        [184.0109],\n",
      "        [180.8236],\n",
      "        [196.9409],\n",
      "        [140.5638]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9410179853439331 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009736736305058002\n",
      "Epoch: 1633 \n",
      " Hypothesis: tensor([[152.3170],\n",
      "        [184.0109],\n",
      "        [180.8236],\n",
      "        [196.9408],\n",
      "        [140.5638]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9409945607185364 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009736874140799046\n",
      "Epoch: 1634 \n",
      " Hypothesis: tensor([[152.3170],\n",
      "        [184.0109],\n",
      "        [180.8236],\n",
      "        [196.9408],\n",
      "        [140.5638]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9409502744674683 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009737011976540089\n",
      "Epoch: 1635 \n",
      " Hypothesis: tensor([[152.3170],\n",
      "        [184.0110],\n",
      "        [180.8236],\n",
      "        [196.9408],\n",
      "        [140.5639]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9409228563308716 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009737149812281132\n",
      "Epoch: 1636 \n",
      " Hypothesis: tensor([[152.3169],\n",
      "        [184.0110],\n",
      "        [180.8236],\n",
      "        [196.9408],\n",
      "        [140.5639]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9408842921257019 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5963],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009737287648022175\n",
      "Epoch: 1637 \n",
      " Hypothesis: tensor([[152.3169],\n",
      "        [184.0110],\n",
      "        [180.8236],\n",
      "        [196.9408],\n",
      "        [140.5639]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9408424496650696 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009737425483763218\n",
      "Epoch: 1638 \n",
      " Hypothesis: tensor([[152.3169],\n",
      "        [184.0110],\n",
      "        [180.8236],\n",
      "        [196.9408],\n",
      "        [140.5640]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9408069849014282 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009737563319504261\n",
      "Epoch: 1639 \n",
      " Hypothesis: tensor([[152.3168],\n",
      "        [184.0110],\n",
      "        [180.8236],\n",
      "        [196.9408],\n",
      "        [140.5640]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9407795071601868 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009737701155245304\n",
      "Epoch: 1640 \n",
      " Hypothesis: tensor([[152.3168],\n",
      "        [184.0110],\n",
      "        [180.8235],\n",
      "        [196.9408],\n",
      "        [140.5640]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9407365918159485 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009737838990986347\n",
      "Epoch: 1641 \n",
      " Hypothesis: tensor([[152.3168],\n",
      "        [184.0111],\n",
      "        [180.8235],\n",
      "        [196.9408],\n",
      "        [140.5641]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9407030940055847 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973797682672739\n",
      "Epoch: 1642 \n",
      " Hypothesis: tensor([[152.3167],\n",
      "        [184.0111],\n",
      "        [180.8235],\n",
      "        [196.9408],\n",
      "        [140.5641]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9406732320785522 \n",
      " W: tensor([[0.7332],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009738114662468433\n",
      "Epoch: 1643 \n",
      " Hypothesis: tensor([[152.3167],\n",
      "        [184.0111],\n",
      "        [180.8235],\n",
      "        [196.9408],\n",
      "        [140.5641]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9406458735466003 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009738252498209476\n",
      "Epoch: 1644 \n",
      " Hypothesis: tensor([[152.3167],\n",
      "        [184.0111],\n",
      "        [180.8235],\n",
      "        [196.9408],\n",
      "        [140.5641]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9406077265739441 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973839033395052\n",
      "Epoch: 1645 \n",
      " Hypothesis: tensor([[152.3167],\n",
      "        [184.0112],\n",
      "        [180.8235],\n",
      "        [196.9407],\n",
      "        [140.5642]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9405746459960938 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009738528169691563\n",
      "Epoch: 1646 \n",
      " Hypothesis: tensor([[152.3166],\n",
      "        [184.0112],\n",
      "        [180.8235],\n",
      "        [196.9407],\n",
      "        [140.5642]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9405509233474731 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009738666005432606\n",
      "Epoch: 1647 \n",
      " Hypothesis: tensor([[152.3166],\n",
      "        [184.0112],\n",
      "        [180.8235],\n",
      "        [196.9407],\n",
      "        [140.5642]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9405123591423035 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009738803841173649\n",
      "Epoch: 1648 \n",
      " Hypothesis: tensor([[152.3166],\n",
      "        [184.0112],\n",
      "        [180.8235],\n",
      "        [196.9407],\n",
      "        [140.5643]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9404705166816711 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009738941676914692\n",
      "Epoch: 1649 \n",
      " Hypothesis: tensor([[152.3165],\n",
      "        [184.0112],\n",
      "        [180.8235],\n",
      "        [196.9407],\n",
      "        [140.5643]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9404360055923462 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009739079512655735\n",
      "Epoch: 1650 \n",
      " Hypothesis: tensor([[152.3165],\n",
      "        [184.0113],\n",
      "        [180.8235],\n",
      "        [196.9407],\n",
      "        [140.5643]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.940402626991272 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009739217348396778\n",
      "Epoch: 1651 \n",
      " Hypothesis: tensor([[152.3165],\n",
      "        [184.0113],\n",
      "        [180.8235],\n",
      "        [196.9407],\n",
      "        [140.5643]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9403632879257202 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009739355184137821\n",
      "Epoch: 1652 \n",
      " Hypothesis: tensor([[152.3164],\n",
      "        [184.0113],\n",
      "        [180.8234],\n",
      "        [196.9407],\n",
      "        [140.5644]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9403243064880371 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009739493019878864\n",
      "Epoch: 1653 \n",
      " Hypothesis: tensor([[152.3164],\n",
      "        [184.0113],\n",
      "        [180.8234],\n",
      "        [196.9407],\n",
      "        [140.5644]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.940290093421936 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009739630855619907\n",
      "Epoch: 1654 \n",
      " Hypothesis: tensor([[152.3164],\n",
      "        [184.0113],\n",
      "        [180.8234],\n",
      "        [196.9406],\n",
      "        [140.5644]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9402626156806946 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00973976869136095\n",
      "Epoch: 1655 \n",
      " Hypothesis: tensor([[152.3163],\n",
      "        [184.0114],\n",
      "        [180.8234],\n",
      "        [196.9406],\n",
      "        [140.5644]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9402240514755249 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009739906527101994\n",
      "Epoch: 1656 \n",
      " Hypothesis: tensor([[152.3163],\n",
      "        [184.0114],\n",
      "        [180.8234],\n",
      "        [196.9406],\n",
      "        [140.5645]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.940187931060791 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009740044362843037\n",
      "Epoch: 1657 \n",
      " Hypothesis: tensor([[152.3163],\n",
      "        [184.0114],\n",
      "        [180.8234],\n",
      "        [196.9406],\n",
      "        [140.5645]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9401592016220093 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974018219858408\n",
      "Epoch: 1658 \n",
      " Hypothesis: tensor([[152.3163],\n",
      "        [184.0114],\n",
      "        [180.8234],\n",
      "        [196.9406],\n",
      "        [140.5645]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9401199221611023 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009740320034325123\n",
      "Epoch: 1659 \n",
      " Hypothesis: tensor([[152.3162],\n",
      "        [184.0114],\n",
      "        [180.8234],\n",
      "        [196.9406],\n",
      "        [140.5646]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9400849342346191 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5962],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009740457870066166\n",
      "Epoch: 1660 \n",
      " Hypothesis: tensor([[152.3162],\n",
      "        [184.0115],\n",
      "        [180.8234],\n",
      "        [196.9406],\n",
      "        [140.5646]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.940048336982727 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009740595705807209\n",
      "Epoch: 1661 \n",
      " Hypothesis: tensor([[152.3162],\n",
      "        [184.0115],\n",
      "        [180.8234],\n",
      "        [196.9406],\n",
      "        [140.5646]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9400242567062378 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009740733541548252\n",
      "Epoch: 1662 \n",
      " Hypothesis: tensor([[152.3161],\n",
      "        [184.0115],\n",
      "        [180.8233],\n",
      "        [196.9406],\n",
      "        [140.5646]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.939980685710907 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009740871377289295\n",
      "Epoch: 1663 \n",
      " Hypothesis: tensor([[152.3161],\n",
      "        [184.0115],\n",
      "        [180.8233],\n",
      "        [196.9406],\n",
      "        [140.5647]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9399533271789551 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009741009213030338\n",
      "Epoch: 1664 \n",
      " Hypothesis: tensor([[152.3161],\n",
      "        [184.0115],\n",
      "        [180.8233],\n",
      "        [196.9406],\n",
      "        [140.5647]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9399151802062988 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009741147048771381\n",
      "Epoch: 1665 \n",
      " Hypothesis: tensor([[152.3160],\n",
      "        [184.0116],\n",
      "        [180.8233],\n",
      "        [196.9406],\n",
      "        [140.5647]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9398797750473022 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009741284884512424\n",
      "Epoch: 1666 \n",
      " Hypothesis: tensor([[152.3160],\n",
      "        [184.0116],\n",
      "        [180.8233],\n",
      "        [196.9406],\n",
      "        [140.5648]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9398435354232788 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009741422720253468\n",
      "Epoch: 1667 \n",
      " Hypothesis: tensor([[152.3160],\n",
      "        [184.0116],\n",
      "        [180.8233],\n",
      "        [196.9406],\n",
      "        [140.5648]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9398101568222046 \n",
      " W: tensor([[0.7333],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974156055599451\n",
      "Epoch: 1668 \n",
      " Hypothesis: tensor([[152.3159],\n",
      "        [184.0116],\n",
      "        [180.8233],\n",
      "        [196.9405],\n",
      "        [140.5648]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9397662281990051 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009741698391735554\n",
      "Epoch: 1669 \n",
      " Hypothesis: tensor([[152.3159],\n",
      "        [184.0116],\n",
      "        [180.8233],\n",
      "        [196.9405],\n",
      "        [140.5648]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9397338032722473 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009741836227476597\n",
      "Epoch: 1670 \n",
      " Hypothesis: tensor([[152.3159],\n",
      "        [184.0117],\n",
      "        [180.8233],\n",
      "        [196.9405],\n",
      "        [140.5649]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9397004246711731 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974197406321764\n",
      "Epoch: 1671 \n",
      " Hypothesis: tensor([[152.3159],\n",
      "        [184.0117],\n",
      "        [180.8233],\n",
      "        [196.9405],\n",
      "        [140.5649]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9396709203720093 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009742111898958683\n",
      "Epoch: 1672 \n",
      " Hypothesis: tensor([[152.3158],\n",
      "        [184.0117],\n",
      "        [180.8232],\n",
      "        [196.9405],\n",
      "        [140.5649]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.939632773399353 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009742249734699726\n",
      "Epoch: 1673 \n",
      " Hypothesis: tensor([[152.3158],\n",
      "        [184.0117],\n",
      "        [180.8232],\n",
      "        [196.9405],\n",
      "        [140.5650]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9396053552627563 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974238757044077\n",
      "Epoch: 1674 \n",
      " Hypothesis: tensor([[152.3158],\n",
      "        [184.0117],\n",
      "        [180.8232],\n",
      "        [196.9405],\n",
      "        [140.5650]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9395611882209778 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009742525406181812\n",
      "Epoch: 1675 \n",
      " Hypothesis: tensor([[152.3157],\n",
      "        [184.0118],\n",
      "        [180.8232],\n",
      "        [196.9405],\n",
      "        [140.5650]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.939533531665802 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009742663241922855\n",
      "Epoch: 1676 \n",
      " Hypothesis: tensor([[152.3157],\n",
      "        [184.0118],\n",
      "        [180.8232],\n",
      "        [196.9405],\n",
      "        [140.5650]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9394963979721069 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009742801077663898\n",
      "Epoch: 1677 \n",
      " Hypothesis: tensor([[152.3157],\n",
      "        [184.0118],\n",
      "        [180.8232],\n",
      "        [196.9405],\n",
      "        [140.5651]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9394688606262207 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009742938913404942\n",
      "Epoch: 1678 \n",
      " Hypothesis: tensor([[152.3157],\n",
      "        [184.0118],\n",
      "        [180.8232],\n",
      "        [196.9404],\n",
      "        [140.5651]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9394248723983765 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009743076749145985\n",
      "Epoch: 1679 \n",
      " Hypothesis: tensor([[152.3156],\n",
      "        [184.0119],\n",
      "        [180.8232],\n",
      "        [196.9404],\n",
      "        [140.5651]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9393855333328247 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009743214584887028\n",
      "Epoch: 1680 \n",
      " Hypothesis: tensor([[152.3156],\n",
      "        [184.0119],\n",
      "        [180.8232],\n",
      "        [196.9404],\n",
      "        [140.5652]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9393612146377563 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974335242062807\n",
      "Epoch: 1681 \n",
      " Hypothesis: tensor([[152.3155],\n",
      "        [184.0119],\n",
      "        [180.8232],\n",
      "        [196.9404],\n",
      "        [140.5652]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9393210411071777 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009743490256369114\n",
      "Epoch: 1682 \n",
      " Hypothesis: tensor([[152.3155],\n",
      "        [184.0119],\n",
      "        [180.8232],\n",
      "        [196.9404],\n",
      "        [140.5652]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9392895698547363 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5961],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009743628092110157\n",
      "Epoch: 1683 \n",
      " Hypothesis: tensor([[152.3155],\n",
      "        [184.0119],\n",
      "        [180.8231],\n",
      "        [196.9404],\n",
      "        [140.5652]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9392592310905457 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.0097437659278512\n",
      "Epoch: 1684 \n",
      " Hypothesis: tensor([[152.3154],\n",
      "        [184.0119],\n",
      "        [180.8231],\n",
      "        [196.9404],\n",
      "        [140.5653]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9392150640487671 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009743903763592243\n",
      "Epoch: 1685 \n",
      " Hypothesis: tensor([[152.3154],\n",
      "        [184.0120],\n",
      "        [180.8231],\n",
      "        [196.9404],\n",
      "        [140.5653]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9391828775405884 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009744041599333286\n",
      "Epoch: 1686 \n",
      " Hypothesis: tensor([[152.3154],\n",
      "        [184.0120],\n",
      "        [180.8231],\n",
      "        [196.9404],\n",
      "        [140.5653]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9391506910324097 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974417943507433\n",
      "Epoch: 1687 \n",
      " Hypothesis: tensor([[152.3154],\n",
      "        [184.0120],\n",
      "        [180.8231],\n",
      "        [196.9404],\n",
      "        [140.5654]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9391123056411743 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009744317270815372\n",
      "Epoch: 1688 \n",
      " Hypothesis: tensor([[152.3153],\n",
      "        [184.0120],\n",
      "        [180.8231],\n",
      "        [196.9404],\n",
      "        [140.5654]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9390848875045776 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009744455106556416\n",
      "Epoch: 1689 \n",
      " Hypothesis: tensor([[152.3153],\n",
      "        [184.0121],\n",
      "        [180.8231],\n",
      "        [196.9404],\n",
      "        [140.5654]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9390463829040527 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009744592942297459\n",
      "Epoch: 1690 \n",
      " Hypothesis: tensor([[152.3153],\n",
      "        [184.0121],\n",
      "        [180.8231],\n",
      "        [196.9403],\n",
      "        [140.5654]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9390082359313965 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009744730778038502\n",
      "Epoch: 1691 \n",
      " Hypothesis: tensor([[152.3152],\n",
      "        [184.0121],\n",
      "        [180.8231],\n",
      "        [196.9403],\n",
      "        [140.5655]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9389751553535461 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009744868613779545\n",
      "Epoch: 1692 \n",
      " Hypothesis: tensor([[152.3152],\n",
      "        [184.0121],\n",
      "        [180.8231],\n",
      "        [196.9403],\n",
      "        [140.5655]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.938945472240448 \n",
      " W: tensor([[0.7334],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009745006449520588\n",
      "Epoch: 1693 \n",
      " Hypothesis: tensor([[152.3152],\n",
      "        [184.0121],\n",
      "        [180.8230],\n",
      "        [196.9403],\n",
      "        [140.5655]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9389042854309082 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009745144285261631\n",
      "Epoch: 1694 \n",
      " Hypothesis: tensor([[152.3151],\n",
      "        [184.0121],\n",
      "        [180.8230],\n",
      "        [196.9403],\n",
      "        [140.5656]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.938872218132019 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009745282121002674\n",
      "Epoch: 1695 \n",
      " Hypothesis: tensor([[152.3151],\n",
      "        [184.0122],\n",
      "        [180.8230],\n",
      "        [196.9403],\n",
      "        [140.5656]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9388357400894165 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009745419956743717\n",
      "Epoch: 1696 \n",
      " Hypothesis: tensor([[152.3151],\n",
      "        [184.0122],\n",
      "        [180.8230],\n",
      "        [196.9403],\n",
      "        [140.5656]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9388042688369751 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974555779248476\n",
      "Epoch: 1697 \n",
      " Hypothesis: tensor([[152.3151],\n",
      "        [184.0122],\n",
      "        [180.8230],\n",
      "        [196.9403],\n",
      "        [140.5656]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.938775897026062 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009745695628225803\n",
      "Epoch: 1698 \n",
      " Hypothesis: tensor([[152.3150],\n",
      "        [184.0122],\n",
      "        [180.8230],\n",
      "        [196.9403],\n",
      "        [140.5657]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9387397766113281 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009745833463966846\n",
      "Epoch: 1699 \n",
      " Hypothesis: tensor([[152.3150],\n",
      "        [184.0123],\n",
      "        [180.8230],\n",
      "        [196.9403],\n",
      "        [140.5657]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9387073516845703 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974597129970789\n",
      "Epoch: 1700 \n",
      " Hypothesis: tensor([[152.3150],\n",
      "        [184.0123],\n",
      "        [180.8230],\n",
      "        [196.9403],\n",
      "        [140.5657]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9386739730834961 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009746109135448933\n",
      "Epoch: 1701 \n",
      " Hypothesis: tensor([[152.3149],\n",
      "        [184.0123],\n",
      "        [180.8230],\n",
      "        [196.9402],\n",
      "        [140.5658]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9386232495307922 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009746246971189976\n",
      "Epoch: 1702 \n",
      " Hypothesis: tensor([[152.3149],\n",
      "        [184.0123],\n",
      "        [180.8230],\n",
      "        [196.9402],\n",
      "        [140.5658]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9386046528816223 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009746384806931019\n",
      "Epoch: 1703 \n",
      " Hypothesis: tensor([[152.3149],\n",
      "        [184.0123],\n",
      "        [180.8230],\n",
      "        [196.9402],\n",
      "        [140.5658]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9385611414909363 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009746522642672062\n",
      "Epoch: 1704 \n",
      " Hypothesis: tensor([[152.3149],\n",
      "        [184.0124],\n",
      "        [180.8230],\n",
      "        [196.9402],\n",
      "        [140.5658]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9385162591934204 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009746660478413105\n",
      "Epoch: 1705 \n",
      " Hypothesis: tensor([[152.3148],\n",
      "        [184.0124],\n",
      "        [180.8229],\n",
      "        [196.9402],\n",
      "        [140.5659]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9384995698928833 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009746798314154148\n",
      "Epoch: 1706 \n",
      " Hypothesis: tensor([[152.3148],\n",
      "        [184.0124],\n",
      "        [180.8229],\n",
      "        [196.9402],\n",
      "        [140.5659]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9384622573852539 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5960],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009746936149895191\n",
      "Epoch: 1707 \n",
      " Hypothesis: tensor([[152.3148],\n",
      "        [184.0124],\n",
      "        [180.8229],\n",
      "        [196.9402],\n",
      "        [140.5659]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9384231567382812 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009747073985636234\n",
      "Epoch: 1708 \n",
      " Hypothesis: tensor([[152.3147],\n",
      "        [184.0124],\n",
      "        [180.8229],\n",
      "        [196.9402],\n",
      "        [140.5659]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9383997917175293 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009747211821377277\n",
      "Epoch: 1709 \n",
      " Hypothesis: tensor([[152.3147],\n",
      "        [184.0125],\n",
      "        [180.8229],\n",
      "        [196.9402],\n",
      "        [140.5660]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9383519291877747 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974734965711832\n",
      "Epoch: 1710 \n",
      " Hypothesis: tensor([[152.3147],\n",
      "        [184.0125],\n",
      "        [180.8229],\n",
      "        [196.9402],\n",
      "        [140.5660]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9383231997489929 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009747487492859364\n",
      "Epoch: 1711 \n",
      " Hypothesis: tensor([[152.3147],\n",
      "        [184.0125],\n",
      "        [180.8229],\n",
      "        [196.9402],\n",
      "        [140.5660]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9382898211479187 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009747625328600407\n",
      "Epoch: 1712 \n",
      " Hypothesis: tensor([[152.3146],\n",
      "        [184.0125],\n",
      "        [180.8229],\n",
      "        [196.9402],\n",
      "        [140.5661]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9382516741752625 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974776316434145\n",
      "Epoch: 1713 \n",
      " Hypothesis: tensor([[152.3146],\n",
      "        [184.0126],\n",
      "        [180.8229],\n",
      "        [196.9402],\n",
      "        [140.5661]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9382182955741882 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009747901000082493\n",
      "Epoch: 1714 \n",
      " Hypothesis: tensor([[152.3146],\n",
      "        [184.0126],\n",
      "        [180.8229],\n",
      "        [196.9401],\n",
      "        [140.5661]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.938180148601532 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009748038835823536\n",
      "Epoch: 1715 \n",
      " Hypothesis: tensor([[152.3145],\n",
      "        [184.0126],\n",
      "        [180.8229],\n",
      "        [196.9401],\n",
      "        [140.5661]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9381615519523621 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009748176671564579\n",
      "Epoch: 1716 \n",
      " Hypothesis: tensor([[152.3145],\n",
      "        [184.0126],\n",
      "        [180.8228],\n",
      "        [196.9401],\n",
      "        [140.5662]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9381231069564819 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009748314507305622\n",
      "Epoch: 1717 \n",
      " Hypothesis: tensor([[152.3145],\n",
      "        [184.0126],\n",
      "        [180.8228],\n",
      "        [196.9401],\n",
      "        [140.5662]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9380843043327332 \n",
      " W: tensor([[0.7335],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009748452343046665\n",
      "Epoch: 1718 \n",
      " Hypothesis: tensor([[152.3144],\n",
      "        [184.0126],\n",
      "        [180.8228],\n",
      "        [196.9401],\n",
      "        [140.5662]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9380518794059753 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009748590178787708\n",
      "Epoch: 1719 \n",
      " Hypothesis: tensor([[152.3144],\n",
      "        [184.0127],\n",
      "        [180.8228],\n",
      "        [196.9401],\n",
      "        [140.5663]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9380264282226562 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009748728014528751\n",
      "Epoch: 1720 \n",
      " Hypothesis: tensor([[152.3144],\n",
      "        [184.0127],\n",
      "        [180.8228],\n",
      "        [196.9401],\n",
      "        [140.5663]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9379765391349792 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009748865850269794\n",
      "Epoch: 1721 \n",
      " Hypothesis: tensor([[152.3144],\n",
      "        [184.0127],\n",
      "        [180.8228],\n",
      "        [196.9401],\n",
      "        [140.5663]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9379607439041138 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009749003686010838\n",
      "Epoch: 1722 \n",
      " Hypothesis: tensor([[152.3143],\n",
      "        [184.0127],\n",
      "        [180.8228],\n",
      "        [196.9401],\n",
      "        [140.5664]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9379165768623352 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974914152175188\n",
      "Epoch: 1723 \n",
      " Hypothesis: tensor([[152.3143],\n",
      "        [184.0128],\n",
      "        [180.8228],\n",
      "        [196.9401],\n",
      "        [140.5664]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9378833770751953 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009749279357492924\n",
      "Epoch: 1724 \n",
      " Hypothesis: tensor([[152.3143],\n",
      "        [184.0128],\n",
      "        [180.8228],\n",
      "        [196.9401],\n",
      "        [140.5664]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.937850296497345 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009749417193233967\n",
      "Epoch: 1725 \n",
      " Hypothesis: tensor([[152.3142],\n",
      "        [184.0128],\n",
      "        [180.8228],\n",
      "        [196.9400],\n",
      "        [140.5665]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.93780118227005 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974955502897501\n",
      "Epoch: 1726 \n",
      " Hypothesis: tensor([[152.3142],\n",
      "        [184.0128],\n",
      "        [180.8228],\n",
      "        [196.9400],\n",
      "        [140.5665]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9377764463424683 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009749692864716053\n",
      "Epoch: 1727 \n",
      " Hypothesis: tensor([[152.3142],\n",
      "        [184.0128],\n",
      "        [180.8228],\n",
      "        [196.9400],\n",
      "        [140.5665]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9377520680427551 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009749830700457096\n",
      "Epoch: 1728 \n",
      " Hypothesis: tensor([[152.3141],\n",
      "        [184.0129],\n",
      "        [180.8228],\n",
      "        [196.9400],\n",
      "        [140.5665]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9377109408378601 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00974996853619814\n",
      "Epoch: 1729 \n",
      " Hypothesis: tensor([[152.3141],\n",
      "        [184.0129],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5666]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9376726150512695 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5959],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009750106371939182\n",
      "Epoch: 1730 \n",
      " Hypothesis: tensor([[152.3141],\n",
      "        [184.0129],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5666]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9376474618911743 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009750244207680225\n",
      "Epoch: 1731 \n",
      " Hypothesis: tensor([[152.3141],\n",
      "        [184.0129],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5666]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9376228451728821 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009750382043421268\n",
      "Epoch: 1732 \n",
      " Hypothesis: tensor([[152.3140],\n",
      "        [184.0130],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5666]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.937584400177002 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009750519879162312\n",
      "Epoch: 1733 \n",
      " Hypothesis: tensor([[152.3140],\n",
      "        [184.0130],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5667]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9375318288803101 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009750657714903355\n",
      "Epoch: 1734 \n",
      " Hypothesis: tensor([[152.3140],\n",
      "        [184.0130],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5667]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9375044107437134 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009750795550644398\n",
      "Epoch: 1735 \n",
      " Hypothesis: tensor([[152.3139],\n",
      "        [184.0130],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5667]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9374691247940063 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975093338638544\n",
      "Epoch: 1736 \n",
      " Hypothesis: tensor([[152.3139],\n",
      "        [184.0130],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5668]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9374249577522278 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009751071222126484\n",
      "Epoch: 1737 \n",
      " Hypothesis: tensor([[152.3139],\n",
      "        [184.0130],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5668]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9374151229858398 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009751209057867527\n",
      "Epoch: 1738 \n",
      " Hypothesis: tensor([[152.3139],\n",
      "        [184.0131],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5668]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9373703002929688 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975134689360857\n",
      "Epoch: 1739 \n",
      " Hypothesis: tensor([[152.3138],\n",
      "        [184.0131],\n",
      "        [180.8227],\n",
      "        [196.9400],\n",
      "        [140.5668]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9373317956924438 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009751484729349613\n",
      "Epoch: 1740 \n",
      " Hypothesis: tensor([[152.3138],\n",
      "        [184.0131],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5669]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9372937083244324 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009751622565090656\n",
      "Epoch: 1741 \n",
      " Hypothesis: tensor([[152.3138],\n",
      "        [184.0131],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5669]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9372607469558716 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.0097517604008317\n",
      "Epoch: 1742 \n",
      " Hypothesis: tensor([[152.3137],\n",
      "        [184.0132],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5669]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9372272491455078 \n",
      " W: tensor([[0.7336],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009751898236572742\n",
      "Epoch: 1743 \n",
      " Hypothesis: tensor([[152.3137],\n",
      "        [184.0132],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5670]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9371968507766724 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009752036072313786\n",
      "Epoch: 1744 \n",
      " Hypothesis: tensor([[152.3137],\n",
      "        [184.0132],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5670]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9371527433395386 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009752173908054829\n",
      "Epoch: 1745 \n",
      " Hypothesis: tensor([[152.3137],\n",
      "        [184.0132],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5670]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9371341466903687 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009752311743795872\n",
      "Epoch: 1746 \n",
      " Hypothesis: tensor([[152.3136],\n",
      "        [184.0132],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5670]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9371020197868347 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009752449579536915\n",
      "Epoch: 1747 \n",
      " Hypothesis: tensor([[152.3136],\n",
      "        [184.0133],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5671]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9370686411857605 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009752587415277958\n",
      "Epoch: 1748 \n",
      " Hypothesis: tensor([[152.3136],\n",
      "        [184.0133],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5671]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9370303153991699 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009752725251019001\n",
      "Epoch: 1749 \n",
      " Hypothesis: tensor([[152.3135],\n",
      "        [184.0133],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5671]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.93699711561203 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009752863086760044\n",
      "Epoch: 1750 \n",
      " Hypothesis: tensor([[152.3135],\n",
      "        [184.0133],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5672]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9369590878486633 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009753000922501087\n",
      "Epoch: 1751 \n",
      " Hypothesis: tensor([[152.3135],\n",
      "        [184.0134],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5672]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9369226694107056 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975313875824213\n",
      "Epoch: 1752 \n",
      " Hypothesis: tensor([[152.3134],\n",
      "        [184.0134],\n",
      "        [180.8226],\n",
      "        [196.9399],\n",
      "        [140.5672]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9369021654129028 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009753276593983173\n",
      "Epoch: 1753 \n",
      " Hypothesis: tensor([[152.3134],\n",
      "        [184.0134],\n",
      "        [180.8225],\n",
      "        [196.9399],\n",
      "        [140.5672]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9368656873703003 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5958],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009753414429724216\n",
      "Epoch: 1754 \n",
      " Hypothesis: tensor([[152.3134],\n",
      "        [184.0134],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5673]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.93683260679245 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975355226546526\n",
      "Epoch: 1755 \n",
      " Hypothesis: tensor([[152.3134],\n",
      "        [184.0134],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5673]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9367884397506714 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009753690101206303\n",
      "Epoch: 1756 \n",
      " Hypothesis: tensor([[152.3133],\n",
      "        [184.0135],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5673]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.936755359172821 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009753827936947346\n",
      "Epoch: 1757 \n",
      " Hypothesis: tensor([[152.3133],\n",
      "        [184.0135],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5674]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9367173314094543 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009753965772688389\n",
      "Epoch: 1758 \n",
      " Hypothesis: tensor([[152.3133],\n",
      "        [184.0135],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5674]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9366847276687622 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009754103608429432\n",
      "Epoch: 1759 \n",
      " Hypothesis: tensor([[152.3132],\n",
      "        [184.0135],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5674]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9366515874862671 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009754241444170475\n",
      "Epoch: 1760 \n",
      " Hypothesis: tensor([[152.3132],\n",
      "        [184.0135],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5675]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9366124868392944 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009754379279911518\n",
      "Epoch: 1761 \n",
      " Hypothesis: tensor([[152.3132],\n",
      "        [184.0136],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5675]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9365859031677246 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009754517115652561\n",
      "Epoch: 1762 \n",
      " Hypothesis: tensor([[152.3132],\n",
      "        [184.0136],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5675]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9365558624267578 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009754654951393604\n",
      "Epoch: 1763 \n",
      " Hypothesis: tensor([[152.3131],\n",
      "        [184.0136],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5676]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9365087747573853 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009754792787134647\n",
      "Epoch: 1764 \n",
      " Hypothesis: tensor([[152.3131],\n",
      "        [184.0136],\n",
      "        [180.8225],\n",
      "        [196.9398],\n",
      "        [140.5676]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9364814758300781 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975493062287569\n",
      "Epoch: 1765 \n",
      " Hypothesis: tensor([[152.3131],\n",
      "        [184.0136],\n",
      "        [180.8224],\n",
      "        [196.9397],\n",
      "        [140.5676]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9364463090896606 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009755068458616734\n",
      "Epoch: 1766 \n",
      " Hypothesis: tensor([[152.3130],\n",
      "        [184.0137],\n",
      "        [180.8224],\n",
      "        [196.9397],\n",
      "        [140.5676]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9364139437675476 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009755206294357777\n",
      "Epoch: 1767 \n",
      " Hypothesis: tensor([[152.3130],\n",
      "        [184.0137],\n",
      "        [180.8224],\n",
      "        [196.9397],\n",
      "        [140.5676]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9363893270492554 \n",
      " W: tensor([[0.7337],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975534413009882\n",
      "Epoch: 1768 \n",
      " Hypothesis: tensor([[152.3130],\n",
      "        [184.0137],\n",
      "        [180.8224],\n",
      "        [196.9397],\n",
      "        [140.5677]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9363502264022827 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009755481965839863\n",
      "Epoch: 1769 \n",
      " Hypothesis: tensor([[152.3130],\n",
      "        [184.0137],\n",
      "        [180.8224],\n",
      "        [196.9397],\n",
      "        [140.5677]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9363151788711548 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009755619801580906\n",
      "Epoch: 1770 \n",
      " Hypothesis: tensor([[152.3129],\n",
      "        [184.0137],\n",
      "        [180.8224],\n",
      "        [196.9397],\n",
      "        [140.5677]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9362818002700806 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009755757637321949\n",
      "Epoch: 1771 \n",
      " Hypothesis: tensor([[152.3129],\n",
      "        [184.0138],\n",
      "        [180.8224],\n",
      "        [196.9397],\n",
      "        [140.5678]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9362475275993347 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009755895473062992\n",
      "Epoch: 1772 \n",
      " Hypothesis: tensor([[152.3129],\n",
      "        [184.0138],\n",
      "        [180.8224],\n",
      "        [196.9397],\n",
      "        [140.5678]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9361996650695801 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009756033308804035\n",
      "Epoch: 1773 \n",
      " Hypothesis: tensor([[152.3128],\n",
      "        [184.0138],\n",
      "        [180.8224],\n",
      "        [196.9397],\n",
      "        [140.5678]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9361723065376282 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009756171144545078\n",
      "Epoch: 1774 \n",
      " Hypothesis: tensor([[152.3128],\n",
      "        [184.0138],\n",
      "        [180.8224],\n",
      "        [196.9397],\n",
      "        [140.5679]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9361380338668823 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009756308980286121\n",
      "Epoch: 1775 \n",
      " Hypothesis: tensor([[152.3128],\n",
      "        [184.0138],\n",
      "        [180.8223],\n",
      "        [196.9397],\n",
      "        [140.5679]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9361017942428589 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009756446816027164\n",
      "Epoch: 1776 \n",
      " Hypothesis: tensor([[152.3127],\n",
      "        [184.0139],\n",
      "        [180.8223],\n",
      "        [196.9397],\n",
      "        [140.5679]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.936071515083313 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5957],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009756584651768208\n",
      "Epoch: 1777 \n",
      " Hypothesis: tensor([[152.3127],\n",
      "        [184.0139],\n",
      "        [180.8223],\n",
      "        [196.9397],\n",
      "        [140.5679]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9360442161560059 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975672248750925\n",
      "Epoch: 1778 \n",
      " Hypothesis: tensor([[152.3127],\n",
      "        [184.0139],\n",
      "        [180.8223],\n",
      "        [196.9397],\n",
      "        [140.5680]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9360119104385376 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009756860323250294\n",
      "Epoch: 1779 \n",
      " Hypothesis: tensor([[152.3127],\n",
      "        [184.0139],\n",
      "        [180.8223],\n",
      "        [196.9396],\n",
      "        [140.5680]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9359641075134277 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009756998158991337\n",
      "Epoch: 1780 \n",
      " Hypothesis: tensor([[152.3126],\n",
      "        [184.0139],\n",
      "        [180.8223],\n",
      "        [196.9396],\n",
      "        [140.5680]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9359279870986938 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975713599473238\n",
      "Epoch: 1781 \n",
      " Hypothesis: tensor([[152.3126],\n",
      "        [184.0140],\n",
      "        [180.8223],\n",
      "        [196.9396],\n",
      "        [140.5680]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.935912013053894 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009757273830473423\n",
      "Epoch: 1782 \n",
      " Hypothesis: tensor([[152.3126],\n",
      "        [184.0140],\n",
      "        [180.8223],\n",
      "        [196.9396],\n",
      "        [140.5681]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9358760118484497 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009757410734891891\n",
      "Epoch: 1783 \n",
      " Hypothesis: tensor([[152.3125],\n",
      "        [184.0140],\n",
      "        [180.8223],\n",
      "        [196.9396],\n",
      "        [140.5681]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9358416795730591 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009757548570632935\n",
      "Epoch: 1784 \n",
      " Hypothesis: tensor([[152.3125],\n",
      "        [184.0140],\n",
      "        [180.8223],\n",
      "        [196.9396],\n",
      "        [140.5681]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9358026385307312 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009757686406373978\n",
      "Epoch: 1785 \n",
      " Hypothesis: tensor([[152.3125],\n",
      "        [184.0141],\n",
      "        [180.8223],\n",
      "        [196.9396],\n",
      "        [140.5682]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9357702136039734 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975782424211502\n",
      "Epoch: 1786 \n",
      " Hypothesis: tensor([[152.3124],\n",
      "        [184.0141],\n",
      "        [180.8223],\n",
      "        [196.9396],\n",
      "        [140.5682]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9357350468635559 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009757962077856064\n",
      "Epoch: 1787 \n",
      " Hypothesis: tensor([[152.3124],\n",
      "        [184.0141],\n",
      "        [180.8223],\n",
      "        [196.9396],\n",
      "        [140.5682]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9356969594955444 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009758099913597107\n",
      "Epoch: 1788 \n",
      " Hypothesis: tensor([[152.3124],\n",
      "        [184.0141],\n",
      "        [180.8223],\n",
      "        [196.9396],\n",
      "        [140.5683]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9356569051742554 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009758236818015575\n",
      "Epoch: 1789 \n",
      " Hypothesis: tensor([[152.3124],\n",
      "        [184.0141],\n",
      "        [180.8222],\n",
      "        [196.9396],\n",
      "        [140.5683]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9356244802474976 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009758373722434044\n",
      "Epoch: 1790 \n",
      " Hypothesis: tensor([[152.3123],\n",
      "        [184.0142],\n",
      "        [180.8222],\n",
      "        [196.9396],\n",
      "        [140.5683]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9355951547622681 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009758510626852512\n",
      "Epoch: 1791 \n",
      " Hypothesis: tensor([[152.3123],\n",
      "        [184.0142],\n",
      "        [180.8222],\n",
      "        [196.9395],\n",
      "        [140.5683]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9355570673942566 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009758648462593555\n",
      "Epoch: 1792 \n",
      " Hypothesis: tensor([[152.3123],\n",
      "        [184.0142],\n",
      "        [180.8222],\n",
      "        [196.9395],\n",
      "        [140.5684]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9355210065841675 \n",
      " W: tensor([[0.7338],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009758786298334599\n",
      "Epoch: 1793 \n",
      " Hypothesis: tensor([[152.3122],\n",
      "        [184.0142],\n",
      "        [180.8222],\n",
      "        [196.9395],\n",
      "        [140.5684]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9355005025863647 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009758924134075642\n",
      "Epoch: 1794 \n",
      " Hypothesis: tensor([[152.3122],\n",
      "        [184.0142],\n",
      "        [180.8222],\n",
      "        [196.9395],\n",
      "        [140.5684]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9354633092880249 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975906103849411\n",
      "Epoch: 1795 \n",
      " Hypothesis: tensor([[152.3122],\n",
      "        [184.0143],\n",
      "        [180.8222],\n",
      "        [196.9395],\n",
      "        [140.5685]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9354233741760254 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009759198874235153\n",
      "Epoch: 1796 \n",
      " Hypothesis: tensor([[152.3122],\n",
      "        [184.0143],\n",
      "        [180.8222],\n",
      "        [196.9395],\n",
      "        [140.5685]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9353922009468079 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009759336709976196\n",
      "Epoch: 1797 \n",
      " Hypothesis: tensor([[152.3121],\n",
      "        [184.0143],\n",
      "        [180.8221],\n",
      "        [196.9395],\n",
      "        [140.5685]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9353469610214233 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00975947454571724\n",
      "Epoch: 1798 \n",
      " Hypothesis: tensor([[152.3121],\n",
      "        [184.0143],\n",
      "        [180.8221],\n",
      "        [196.9395],\n",
      "        [140.5685]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.935319721698761 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009759612381458282\n",
      "Epoch: 1799 \n",
      " Hypothesis: tensor([[152.3121],\n",
      "        [184.0143],\n",
      "        [180.8221],\n",
      "        [196.9395],\n",
      "        [140.5686]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9352775812149048 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009759749285876751\n",
      "Epoch: 1800 \n",
      " Hypothesis: tensor([[152.3120],\n",
      "        [184.0144],\n",
      "        [180.8221],\n",
      "        [196.9395],\n",
      "        [140.5686]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9352513551712036 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5956],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009759887121617794\n",
      "Epoch: 1801 \n",
      " Hypothesis: tensor([[152.3120],\n",
      "        [184.0144],\n",
      "        [180.8221],\n",
      "        [196.9395],\n",
      "        [140.5686]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9352258443832397 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009760024026036263\n",
      "Epoch: 1802 \n",
      " Hypothesis: tensor([[152.3120],\n",
      "        [184.0144],\n",
      "        [180.8221],\n",
      "        [196.9395],\n",
      "        [140.5686]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9351876378059387 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009760160930454731\n",
      "Epoch: 1803 \n",
      " Hypothesis: tensor([[152.3120],\n",
      "        [184.0144],\n",
      "        [180.8221],\n",
      "        [196.9394],\n",
      "        [140.5687]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9351545572280884 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.0097602978348732\n",
      "Epoch: 1804 \n",
      " Hypothesis: tensor([[152.3119],\n",
      "        [184.0144],\n",
      "        [180.8221],\n",
      "        [196.9394],\n",
      "        [140.5687]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9351183176040649 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009760435670614243\n",
      "Epoch: 1805 \n",
      " Hypothesis: tensor([[152.3119],\n",
      "        [184.0145],\n",
      "        [180.8221],\n",
      "        [196.9394],\n",
      "        [140.5687]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9350814819335938 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009760573506355286\n",
      "Epoch: 1806 \n",
      " Hypothesis: tensor([[152.3119],\n",
      "        [184.0145],\n",
      "        [180.8221],\n",
      "        [196.9394],\n",
      "        [140.5688]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9350539445877075 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009760710410773754\n",
      "Epoch: 1807 \n",
      " Hypothesis: tensor([[152.3118],\n",
      "        [184.0145],\n",
      "        [180.8221],\n",
      "        [196.9394],\n",
      "        [140.5688]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9350215792655945 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009760847315192223\n",
      "Epoch: 1808 \n",
      " Hypothesis: tensor([[152.3118],\n",
      "        [184.0145],\n",
      "        [180.8221],\n",
      "        [196.9394],\n",
      "        [140.5688]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9349835515022278 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009760985150933266\n",
      "Epoch: 1809 \n",
      " Hypothesis: tensor([[152.3118],\n",
      "        [184.0145],\n",
      "        [180.8221],\n",
      "        [196.9394],\n",
      "        [140.5688]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9349562525749207 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009761122055351734\n",
      "Epoch: 1810 \n",
      " Hypothesis: tensor([[152.3117],\n",
      "        [184.0146],\n",
      "        [180.8220],\n",
      "        [196.9394],\n",
      "        [140.5689]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9349180459976196 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009761258959770203\n",
      "Epoch: 1811 \n",
      " Hypothesis: tensor([[152.3117],\n",
      "        [184.0146],\n",
      "        [180.8220],\n",
      "        [196.9394],\n",
      "        [140.5689]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9348681569099426 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009761395864188671\n",
      "Epoch: 1812 \n",
      " Hypothesis: tensor([[152.3117],\n",
      "        [184.0146],\n",
      "        [180.8220],\n",
      "        [196.9394],\n",
      "        [140.5689]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9348495602607727 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00976153276860714\n",
      "Epoch: 1813 \n",
      " Hypothesis: tensor([[152.3116],\n",
      "        [184.0146],\n",
      "        [180.8220],\n",
      "        [196.9393],\n",
      "        [140.5690]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9348078966140747 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009761669673025608\n",
      "Epoch: 1814 \n",
      " Hypothesis: tensor([[152.3116],\n",
      "        [184.0146],\n",
      "        [180.8220],\n",
      "        [196.9393],\n",
      "        [140.5690]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9347637295722961 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009761806577444077\n",
      "Epoch: 1815 \n",
      " Hypothesis: tensor([[152.3116],\n",
      "        [184.0147],\n",
      "        [180.8220],\n",
      "        [196.9393],\n",
      "        [140.5690]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9347314834594727 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009761943481862545\n",
      "Epoch: 1816 \n",
      " Hypothesis: tensor([[152.3116],\n",
      "        [184.0147],\n",
      "        [180.8220],\n",
      "        [196.9393],\n",
      "        [140.5690]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.93470698595047 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009762080386281013\n",
      "Epoch: 1817 \n",
      " Hypothesis: tensor([[152.3115],\n",
      "        [184.0147],\n",
      "        [180.8220],\n",
      "        [196.9393],\n",
      "        [140.5691]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9346688389778137 \n",
      " W: tensor([[0.7339],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009762217290699482\n",
      "Epoch: 1818 \n",
      " Hypothesis: tensor([[152.3115],\n",
      "        [184.0147],\n",
      "        [180.8220],\n",
      "        [196.9393],\n",
      "        [140.5691]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9346445798873901 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00976235419511795\n",
      "Epoch: 1819 \n",
      " Hypothesis: tensor([[152.3115],\n",
      "        [184.0148],\n",
      "        [180.8220],\n",
      "        [196.9393],\n",
      "        [140.5691]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9345976114273071 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009762491099536419\n",
      "Epoch: 1820 \n",
      " Hypothesis: tensor([[152.3114],\n",
      "        [184.0148],\n",
      "        [180.8220],\n",
      "        [196.9393],\n",
      "        [140.5692]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9345722198486328 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009762628003954887\n",
      "Epoch: 1821 \n",
      " Hypothesis: tensor([[152.3114],\n",
      "        [184.0148],\n",
      "        [180.8219],\n",
      "        [196.9393],\n",
      "        [140.5692]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9345359802246094 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009762764908373356\n",
      "Epoch: 1822 \n",
      " Hypothesis: tensor([[152.3114],\n",
      "        [184.0148],\n",
      "        [180.8219],\n",
      "        [196.9393],\n",
      "        [140.5692]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9345029592514038 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009762901812791824\n",
      "Epoch: 1823 \n",
      " Hypothesis: tensor([[152.3113],\n",
      "        [184.0148],\n",
      "        [180.8219],\n",
      "        [196.9393],\n",
      "        [140.5692]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9344696998596191 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009763038717210293\n",
      "Epoch: 1824 \n",
      " Hypothesis: tensor([[152.3113],\n",
      "        [184.0148],\n",
      "        [180.8219],\n",
      "        [196.9393],\n",
      "        [140.5693]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.934443473815918 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5955],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009763175621628761\n",
      "Epoch: 1825 \n",
      " Hypothesis: tensor([[152.3113],\n",
      "        [184.0149],\n",
      "        [180.8219],\n",
      "        [196.9393],\n",
      "        [140.5693]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9343992471694946 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00976331252604723\n",
      "Epoch: 1826 \n",
      " Hypothesis: tensor([[152.3112],\n",
      "        [184.0149],\n",
      "        [180.8219],\n",
      "        [196.9392],\n",
      "        [140.5693]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9343603253364563 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009763449430465698\n",
      "Epoch: 1827 \n",
      " Hypothesis: tensor([[152.3112],\n",
      "        [184.0149],\n",
      "        [180.8219],\n",
      "        [196.9392],\n",
      "        [140.5694]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9343386888504028 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009763586334884167\n",
      "Epoch: 1828 \n",
      " Hypothesis: tensor([[152.3112],\n",
      "        [184.0149],\n",
      "        [180.8219],\n",
      "        [196.9392],\n",
      "        [140.5694]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9343063235282898 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009763723239302635\n",
      "Epoch: 1829 \n",
      " Hypothesis: tensor([[152.3112],\n",
      "        [184.0150],\n",
      "        [180.8219],\n",
      "        [196.9392],\n",
      "        [140.5694]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9342616200447083 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009763860143721104\n",
      "Epoch: 1830 \n",
      " Hypothesis: tensor([[152.3111],\n",
      "        [184.0150],\n",
      "        [180.8219],\n",
      "        [196.9392],\n",
      "        [140.5695]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9342314004898071 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009763997048139572\n",
      "Epoch: 1831 \n",
      " Hypothesis: tensor([[152.3111],\n",
      "        [184.0150],\n",
      "        [180.8218],\n",
      "        [196.9392],\n",
      "        [140.5695]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9342001080513 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00976413395255804\n",
      "Epoch: 1832 \n",
      " Hypothesis: tensor([[152.3111],\n",
      "        [184.0150],\n",
      "        [180.8218],\n",
      "        [196.9392],\n",
      "        [140.5695]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9341610074043274 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009764270856976509\n",
      "Epoch: 1833 \n",
      " Hypothesis: tensor([[152.3111],\n",
      "        [184.0150],\n",
      "        [180.8218],\n",
      "        [196.9392],\n",
      "        [140.5695]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9341344833374023 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009764407761394978\n",
      "Epoch: 1834 \n",
      " Hypothesis: tensor([[152.3110],\n",
      "        [184.0150],\n",
      "        [180.8218],\n",
      "        [196.9392],\n",
      "        [140.5695]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.934101402759552 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009764544665813446\n",
      "Epoch: 1835 \n",
      " Hypothesis: tensor([[152.3110],\n",
      "        [184.0151],\n",
      "        [180.8218],\n",
      "        [196.9392],\n",
      "        [140.5696]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9340594410896301 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009764681570231915\n",
      "Epoch: 1836 \n",
      " Hypothesis: tensor([[152.3110],\n",
      "        [184.0151],\n",
      "        [180.8218],\n",
      "        [196.9391],\n",
      "        [140.5696]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9340304136276245 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009764818474650383\n",
      "Epoch: 1837 \n",
      " Hypothesis: tensor([[152.3109],\n",
      "        [184.0151],\n",
      "        [180.8218],\n",
      "        [196.9391],\n",
      "        [140.5696]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9339971542358398 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009764955379068851\n",
      "Epoch: 1838 \n",
      " Hypothesis: tensor([[152.3109],\n",
      "        [184.0151],\n",
      "        [180.8218],\n",
      "        [196.9391],\n",
      "        [140.5697]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9339538812637329 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00976509228348732\n",
      "Epoch: 1839 \n",
      " Hypothesis: tensor([[152.3109],\n",
      "        [184.0152],\n",
      "        [180.8218],\n",
      "        [196.9391],\n",
      "        [140.5697]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9339208602905273 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009765229187905788\n",
      "Epoch: 1840 \n",
      " Hypothesis: tensor([[152.3108],\n",
      "        [184.0152],\n",
      "        [180.8218],\n",
      "        [196.9391],\n",
      "        [140.5697]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9338973164558411 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009765366092324257\n",
      "Epoch: 1841 \n",
      " Hypothesis: tensor([[152.3108],\n",
      "        [184.0152],\n",
      "        [180.8218],\n",
      "        [196.9391],\n",
      "        [140.5698]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.933851420879364 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009765502996742725\n",
      "Epoch: 1842 \n",
      " Hypothesis: tensor([[152.3108],\n",
      "        [184.0152],\n",
      "        [180.8217],\n",
      "        [196.9391],\n",
      "        [140.5698]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9338338971138 \n",
      " W: tensor([[0.7340],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009765639901161194\n",
      "Epoch: 1843 \n",
      " Hypothesis: tensor([[152.3107],\n",
      "        [184.0152],\n",
      "        [180.8217],\n",
      "        [196.9391],\n",
      "        [140.5698]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9337930679321289 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009765776805579662\n",
      "Epoch: 1844 \n",
      " Hypothesis: tensor([[152.3107],\n",
      "        [184.0153],\n",
      "        [180.8217],\n",
      "        [196.9391],\n",
      "        [140.5698]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9337509274482727 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00976591370999813\n",
      "Epoch: 1845 \n",
      " Hypothesis: tensor([[152.3107],\n",
      "        [184.0153],\n",
      "        [180.8217],\n",
      "        [196.9391],\n",
      "        [140.5699]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.933721661567688 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.0097660506144166\n",
      "Epoch: 1846 \n",
      " Hypothesis: tensor([[152.3107],\n",
      "        [184.0153],\n",
      "        [180.8217],\n",
      "        [196.9391],\n",
      "        [140.5699]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9337000846862793 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009766187518835068\n",
      "Epoch: 1847 \n",
      " Hypothesis: tensor([[152.3106],\n",
      "        [184.0153],\n",
      "        [180.8217],\n",
      "        [196.9391],\n",
      "        [140.5699]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9336553812026978 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5954],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009766324423253536\n",
      "Epoch: 1848 \n",
      " Hypothesis: tensor([[152.3106],\n",
      "        [184.0153],\n",
      "        [180.8217],\n",
      "        [196.9391],\n",
      "        [140.5699]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9336231350898743 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009766461327672005\n",
      "Epoch: 1849 \n",
      " Hypothesis: tensor([[152.3106],\n",
      "        [184.0154],\n",
      "        [180.8217],\n",
      "        [196.9391],\n",
      "        [140.5700]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9335907697677612 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009766598232090473\n",
      "Epoch: 1850 \n",
      " Hypothesis: tensor([[152.3105],\n",
      "        [184.0154],\n",
      "        [180.8217],\n",
      "        [196.9390],\n",
      "        [140.5700]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9335517883300781 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009766735136508942\n",
      "Epoch: 1851 \n",
      " Hypothesis: tensor([[152.3105],\n",
      "        [184.0154],\n",
      "        [180.8217],\n",
      "        [196.9390],\n",
      "        [140.5700]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.933512806892395 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00976687204092741\n",
      "Epoch: 1852 \n",
      " Hypothesis: tensor([[152.3105],\n",
      "        [184.0154],\n",
      "        [180.8217],\n",
      "        [196.9390],\n",
      "        [140.5701]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9334865808486938 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009767008945345879\n",
      "Epoch: 1853 \n",
      " Hypothesis: tensor([[152.3105],\n",
      "        [184.0154],\n",
      "        [180.8217],\n",
      "        [196.9390],\n",
      "        [140.5701]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.933448314666748 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009767145849764347\n",
      "Epoch: 1854 \n",
      " Hypothesis: tensor([[152.3104],\n",
      "        [184.0155],\n",
      "        [180.8217],\n",
      "        [196.9390],\n",
      "        [140.5701]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9334182739257812 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009767282754182816\n",
      "Epoch: 1855 \n",
      " Hypothesis: tensor([[152.3104],\n",
      "        [184.0155],\n",
      "        [180.8216],\n",
      "        [196.9390],\n",
      "        [140.5702]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9333723187446594 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009767419658601284\n",
      "Epoch: 1856 \n",
      " Hypothesis: tensor([[152.3104],\n",
      "        [184.0155],\n",
      "        [180.8216],\n",
      "        [196.9390],\n",
      "        [140.5702]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9333537220954895 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009767556563019753\n",
      "Epoch: 1857 \n",
      " Hypothesis: tensor([[152.3103],\n",
      "        [184.0155],\n",
      "        [180.8216],\n",
      "        [196.9390],\n",
      "        [140.5702]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9333184957504272 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009767693467438221\n",
      "Epoch: 1858 \n",
      " Hypothesis: tensor([[152.3103],\n",
      "        [184.0155],\n",
      "        [180.8216],\n",
      "        [196.9390],\n",
      "        [140.5702]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9332767724990845 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00976783037185669\n",
      "Epoch: 1859 \n",
      " Hypothesis: tensor([[152.3103],\n",
      "        [184.0156],\n",
      "        [180.8216],\n",
      "        [196.9390],\n",
      "        [140.5703]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9332385063171387 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009767967276275158\n",
      "Epoch: 1860 \n",
      " Hypothesis: tensor([[152.3102],\n",
      "        [184.0156],\n",
      "        [180.8216],\n",
      "        [196.9390],\n",
      "        [140.5703]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9332112073898315 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009768104180693626\n",
      "Epoch: 1861 \n",
      " Hypothesis: tensor([[152.3102],\n",
      "        [184.0156],\n",
      "        [180.8216],\n",
      "        [196.9390],\n",
      "        [140.5703]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9331876635551453 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009768241085112095\n",
      "Epoch: 1862 \n",
      " Hypothesis: tensor([[152.3102],\n",
      "        [184.0156],\n",
      "        [180.8216],\n",
      "        [196.9389],\n",
      "        [140.5703]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9331487417221069 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009768377989530563\n",
      "Epoch: 1863 \n",
      " Hypothesis: tensor([[152.3102],\n",
      "        [184.0157],\n",
      "        [180.8216],\n",
      "        [196.9389],\n",
      "        [140.5704]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9331046938896179 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009768514893949032\n",
      "Epoch: 1864 \n",
      " Hypothesis: tensor([[152.3101],\n",
      "        [184.0157],\n",
      "        [180.8216],\n",
      "        [196.9389],\n",
      "        [140.5704]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9330774545669556 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.0097686517983675\n",
      "Epoch: 1865 \n",
      " Hypothesis: tensor([[152.3101],\n",
      "        [184.0157],\n",
      "        [180.8215],\n",
      "        [196.9389],\n",
      "        [140.5704]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9330401420593262 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009768788702785969\n",
      "Epoch: 1866 \n",
      " Hypothesis: tensor([[152.3101],\n",
      "        [184.0157],\n",
      "        [180.8215],\n",
      "        [196.9389],\n",
      "        [140.5705]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9330072402954102 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009768925607204437\n",
      "Epoch: 1867 \n",
      " Hypothesis: tensor([[152.3100],\n",
      "        [184.0157],\n",
      "        [180.8215],\n",
      "        [196.9389],\n",
      "        [140.5705]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.932965099811554 \n",
      " W: tensor([[0.7341],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009769062511622906\n",
      "Epoch: 1868 \n",
      " Hypothesis: tensor([[152.3100],\n",
      "        [184.0157],\n",
      "        [180.8215],\n",
      "        [196.9389],\n",
      "        [140.5705]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9329468607902527 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009769199416041374\n",
      "Epoch: 1869 \n",
      " Hypothesis: tensor([[152.3100],\n",
      "        [184.0158],\n",
      "        [180.8215],\n",
      "        [196.9389],\n",
      "        [140.5706]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9328941106796265 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009769336320459843\n",
      "Epoch: 1870 \n",
      " Hypothesis: tensor([[152.3100],\n",
      "        [184.0158],\n",
      "        [180.8215],\n",
      "        [196.9389],\n",
      "        [140.5706]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9328609704971313 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009769473224878311\n",
      "Epoch: 1871 \n",
      " Hypothesis: tensor([[152.3099],\n",
      "        [184.0158],\n",
      "        [180.8215],\n",
      "        [196.9389],\n",
      "        [140.5706]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9328426122665405 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5953],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00976961012929678\n",
      "Epoch: 1872 \n",
      " Hypothesis: tensor([[152.3099],\n",
      "        [184.0158],\n",
      "        [180.8215],\n",
      "        [196.9389],\n",
      "        [140.5706]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9328081011772156 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009769747033715248\n",
      "Epoch: 1873 \n",
      " Hypothesis: tensor([[152.3098],\n",
      "        [184.0158],\n",
      "        [180.8215],\n",
      "        [196.9388],\n",
      "        [140.5706]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9327732920646667 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009769883938133717\n",
      "Epoch: 1874 \n",
      " Hypothesis: tensor([[152.3098],\n",
      "        [184.0159],\n",
      "        [180.8215],\n",
      "        [196.9388],\n",
      "        [140.5707]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9327281713485718 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009770020842552185\n",
      "Epoch: 1875 \n",
      " Hypothesis: tensor([[152.3098],\n",
      "        [184.0159],\n",
      "        [180.8215],\n",
      "        [196.9388],\n",
      "        [140.5707]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9327009916305542 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009770157746970654\n",
      "Epoch: 1876 \n",
      " Hypothesis: tensor([[152.3098],\n",
      "        [184.0159],\n",
      "        [180.8215],\n",
      "        [196.9388],\n",
      "        [140.5707]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9326766729354858 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009770294651389122\n",
      "Epoch: 1877 \n",
      " Hypothesis: tensor([[152.3097],\n",
      "        [184.0159],\n",
      "        [180.8214],\n",
      "        [196.9388],\n",
      "        [140.5708]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9326327443122864 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00977043155580759\n",
      "Epoch: 1878 \n",
      " Hypothesis: tensor([[152.3097],\n",
      "        [184.0160],\n",
      "        [180.8214],\n",
      "        [196.9388],\n",
      "        [140.5708]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9326013326644897 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009770568460226059\n",
      "Epoch: 1879 \n",
      " Hypothesis: tensor([[152.3097],\n",
      "        [184.0160],\n",
      "        [180.8214],\n",
      "        [196.9388],\n",
      "        [140.5708]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9325681924819946 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009770705364644527\n",
      "Epoch: 1880 \n",
      " Hypothesis: tensor([[152.3097],\n",
      "        [184.0160],\n",
      "        [180.8214],\n",
      "        [196.9388],\n",
      "        [140.5709]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9325254559516907 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009770842269062996\n",
      "Epoch: 1881 \n",
      " Hypothesis: tensor([[152.3096],\n",
      "        [184.0160],\n",
      "        [180.8214],\n",
      "        [196.9388],\n",
      "        [140.5709]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9324932098388672 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009770979173481464\n",
      "Epoch: 1882 \n",
      " Hypothesis: tensor([[152.3096],\n",
      "        [184.0160],\n",
      "        [180.8214],\n",
      "        [196.9388],\n",
      "        [140.5709]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.932472825050354 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009771116077899933\n",
      "Epoch: 1883 \n",
      " Hypothesis: tensor([[152.3096],\n",
      "        [184.0161],\n",
      "        [180.8214],\n",
      "        [196.9388],\n",
      "        [140.5709]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9324231147766113 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009771252982318401\n",
      "Epoch: 1884 \n",
      " Hypothesis: tensor([[152.3095],\n",
      "        [184.0161],\n",
      "        [180.8214],\n",
      "        [196.9388],\n",
      "        [140.5710]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.932397723197937 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.00977138988673687\n",
      "Epoch: 1885 \n",
      " Hypothesis: tensor([[152.3095],\n",
      "        [184.0161],\n",
      "        [180.8214],\n",
      "        [196.9388],\n",
      "        [140.5710]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.932348906993866 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009771526791155338\n",
      "Epoch: 1886 \n",
      " Hypothesis: tensor([[152.3095],\n",
      "        [184.0161],\n",
      "        [180.8214],\n",
      "        [196.9388],\n",
      "        [140.5710]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.932327151298523 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009771663695573807\n",
      "Epoch: 1887 \n",
      " Hypothesis: tensor([[152.3094],\n",
      "        [184.0161],\n",
      "        [180.8214],\n",
      "        [196.9387],\n",
      "        [140.5710]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9323029518127441 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009771800599992275\n",
      "Epoch: 1888 \n",
      " Hypothesis: tensor([[152.3094],\n",
      "        [184.0162],\n",
      "        [180.8214],\n",
      "        [196.9387],\n",
      "        [140.5711]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9322553873062134 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6811]], requires_grad=True) \n",
      " b: 0.009771937504410744\n",
      "Epoch: 1889 \n",
      " Hypothesis: tensor([[152.3094],\n",
      "        [184.0162],\n",
      "        [180.8213],\n",
      "        [196.9387],\n",
      "        [140.5711]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9322317838668823 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009772074408829212\n",
      "Epoch: 1890 \n",
      " Hypothesis: tensor([[152.3094],\n",
      "        [184.0162],\n",
      "        [180.8213],\n",
      "        [196.9387],\n",
      "        [140.5711]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9321988224983215 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977221131324768\n",
      "Epoch: 1891 \n",
      " Hypothesis: tensor([[152.3093],\n",
      "        [184.0162],\n",
      "        [180.8213],\n",
      "        [196.9387],\n",
      "        [140.5712]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9321548342704773 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977234821766615\n",
      "Epoch: 1892 \n",
      " Hypothesis: tensor([[152.3093],\n",
      "        [184.0162],\n",
      "        [180.8213],\n",
      "        [196.9387],\n",
      "        [140.5712]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9321276545524597 \n",
      " W: tensor([[0.7342],\n",
      "        [0.5952],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009772485122084618\n",
      "Epoch: 1893 \n",
      " Hypothesis: tensor([[152.3093],\n",
      "        [184.0163],\n",
      "        [180.8213],\n",
      "        [196.9387],\n",
      "        [140.5712]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9320904612541199 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5952],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009772622026503086\n",
      "Epoch: 1894 \n",
      " Hypothesis: tensor([[152.3092],\n",
      "        [184.0163],\n",
      "        [180.8213],\n",
      "        [196.9387],\n",
      "        [140.5713]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9320514798164368 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5952],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009772758930921555\n",
      "Epoch: 1895 \n",
      " Hypothesis: tensor([[152.3092],\n",
      "        [184.0163],\n",
      "        [180.8213],\n",
      "        [196.9387],\n",
      "        [140.5713]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9320222735404968 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5952],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009772895835340023\n",
      "Epoch: 1896 \n",
      " Hypothesis: tensor([[152.3092],\n",
      "        [184.0163],\n",
      "        [180.8213],\n",
      "        [196.9387],\n",
      "        [140.5713]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9319862127304077 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009773032739758492\n",
      "Epoch: 1897 \n",
      " Hypothesis: tensor([[152.3091],\n",
      "        [184.0163],\n",
      "        [180.8213],\n",
      "        [196.9387],\n",
      "        [140.5713]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.931962788105011 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977316964417696\n",
      "Epoch: 1898 \n",
      " Hypothesis: tensor([[152.3091],\n",
      "        [184.0164],\n",
      "        [180.8213],\n",
      "        [196.9387],\n",
      "        [140.5714]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9319295883178711 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009773306548595428\n",
      "Epoch: 1899 \n",
      " Hypothesis: tensor([[152.3091],\n",
      "        [184.0164],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5714]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.931884765625 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009773443453013897\n",
      "Epoch: 1900 \n",
      " Hypothesis: tensor([[152.3091],\n",
      "        [184.0164],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5714]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9318515062332153 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009773580357432365\n",
      "Epoch: 1901 \n",
      " Hypothesis: tensor([[152.3090],\n",
      "        [184.0164],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5715]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.931816577911377 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009773717261850834\n",
      "Epoch: 1902 \n",
      " Hypothesis: tensor([[152.3090],\n",
      "        [184.0164],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5715]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9317930936813354 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009773854166269302\n",
      "Epoch: 1903 \n",
      " Hypothesis: tensor([[152.3090],\n",
      "        [184.0164],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5715]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.93175208568573 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977399107068777\n",
      "Epoch: 1904 \n",
      " Hypothesis: tensor([[152.3089],\n",
      "        [184.0165],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5715]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9317024350166321 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977412797510624\n",
      "Epoch: 1905 \n",
      " Hypothesis: tensor([[152.3089],\n",
      "        [184.0165],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5716]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9316752552986145 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009774264879524708\n",
      "Epoch: 1906 \n",
      " Hypothesis: tensor([[152.3089],\n",
      "        [184.0165],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5716]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9316507577896118 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009774401783943176\n",
      "Epoch: 1907 \n",
      " Hypothesis: tensor([[152.3089],\n",
      "        [184.0165],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5716]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9316070675849915 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009774538688361645\n",
      "Epoch: 1908 \n",
      " Hypothesis: tensor([[152.3088],\n",
      "        [184.0166],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5717]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9315749406814575 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009774675592780113\n",
      "Epoch: 1909 \n",
      " Hypothesis: tensor([[152.3088],\n",
      "        [184.0166],\n",
      "        [180.8212],\n",
      "        [196.9386],\n",
      "        [140.5717]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9315476417541504 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009774812497198582\n",
      "Epoch: 1910 \n",
      " Hypothesis: tensor([[152.3088],\n",
      "        [184.0166],\n",
      "        [180.8212],\n",
      "        [196.9385],\n",
      "        [140.5717]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9315087199211121 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977494940161705\n",
      "Epoch: 1911 \n",
      " Hypothesis: tensor([[152.3087],\n",
      "        [184.0166],\n",
      "        [180.8211],\n",
      "        [196.9385],\n",
      "        [140.5717]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9314854741096497 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009775086306035519\n",
      "Epoch: 1912 \n",
      " Hypothesis: tensor([[152.3087],\n",
      "        [184.0166],\n",
      "        [180.8211],\n",
      "        [196.9385],\n",
      "        [140.5718]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9314473271369934 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009775223210453987\n",
      "Epoch: 1913 \n",
      " Hypothesis: tensor([[152.3087],\n",
      "        [184.0166],\n",
      "        [180.8211],\n",
      "        [196.9385],\n",
      "        [140.5718]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9314200282096863 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009775360114872456\n",
      "Epoch: 1914 \n",
      " Hypothesis: tensor([[152.3086],\n",
      "        [184.0167],\n",
      "        [180.8211],\n",
      "        [196.9385],\n",
      "        [140.5718]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9313761591911316 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009775497019290924\n",
      "Epoch: 1915 \n",
      " Hypothesis: tensor([[152.3086],\n",
      "        [184.0167],\n",
      "        [180.8211],\n",
      "        [196.9385],\n",
      "        [140.5719]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9313381314277649 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009775633923709393\n",
      "Epoch: 1916 \n",
      " Hypothesis: tensor([[152.3086],\n",
      "        [184.0167],\n",
      "        [180.8211],\n",
      "        [196.9385],\n",
      "        [140.5719]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9313052296638489 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009775770828127861\n",
      "Epoch: 1917 \n",
      " Hypothesis: tensor([[152.3085],\n",
      "        [184.0167],\n",
      "        [180.8211],\n",
      "        [196.9385],\n",
      "        [140.5719]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9312613606452942 \n",
      " W: tensor([[0.7343],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977590773254633\n",
      "Epoch: 1918 \n",
      " Hypothesis: tensor([[152.3085],\n",
      "        [184.0168],\n",
      "        [180.8211],\n",
      "        [196.9385],\n",
      "        [140.5719]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9312435388565063 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5951],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009776044636964798\n",
      "Epoch: 1919 \n",
      " Hypothesis: tensor([[152.3085],\n",
      "        [184.0168],\n",
      "        [180.8211],\n",
      "        [196.9385],\n",
      "        [140.5720]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9311977624893188 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009776181541383266\n",
      "Epoch: 1920 \n",
      " Hypothesis: tensor([[152.3085],\n",
      "        [184.0168],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5720]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9311666488647461 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009776318445801735\n",
      "Epoch: 1921 \n",
      " Hypothesis: tensor([[152.3084],\n",
      "        [184.0168],\n",
      "        [180.8210],\n",
      "        [196.9385],\n",
      "        [140.5720]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.931140124797821 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009776455350220203\n",
      "Epoch: 1922 \n",
      " Hypothesis: tensor([[152.3084],\n",
      "        [184.0168],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5721]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9310954809188843 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009776592254638672\n",
      "Epoch: 1923 \n",
      " Hypothesis: tensor([[152.3084],\n",
      "        [184.0169],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5721]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9310720562934875 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977672915905714\n",
      "Epoch: 1924 \n",
      " Hypothesis: tensor([[152.3083],\n",
      "        [184.0169],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5721]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9310391545295715 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009776866063475609\n",
      "Epoch: 1925 \n",
      " Hypothesis: tensor([[152.3083],\n",
      "        [184.0169],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5721]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9309865236282349 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009777002967894077\n",
      "Epoch: 1926 \n",
      " Hypothesis: tensor([[152.3083],\n",
      "        [184.0169],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5722]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9309611320495605 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009777139872312546\n",
      "Epoch: 1927 \n",
      " Hypothesis: tensor([[152.3083],\n",
      "        [184.0169],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5722]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.930937647819519 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009777276776731014\n",
      "Epoch: 1928 \n",
      " Hypothesis: tensor([[152.3082],\n",
      "        [184.0170],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5722]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9309045076370239 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009777413681149483\n",
      "Epoch: 1929 \n",
      " Hypothesis: tensor([[152.3082],\n",
      "        [184.0170],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5723]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9308608770370483 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009777550585567951\n",
      "Epoch: 1930 \n",
      " Hypothesis: tensor([[152.3082],\n",
      "        [184.0170],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5723]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9308317303657532 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977768748998642\n",
      "Epoch: 1931 \n",
      " Hypothesis: tensor([[152.3081],\n",
      "        [184.0170],\n",
      "        [180.8210],\n",
      "        [196.9384],\n",
      "        [140.5723]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.930801510810852 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009777824394404888\n",
      "Epoch: 1932 \n",
      " Hypothesis: tensor([[152.3081],\n",
      "        [184.0170],\n",
      "        [180.8209],\n",
      "        [196.9384],\n",
      "        [140.5723]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9307719469070435 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009777961298823357\n",
      "Epoch: 1933 \n",
      " Hypothesis: tensor([[152.3081],\n",
      "        [184.0171],\n",
      "        [180.8209],\n",
      "        [196.9384],\n",
      "        [140.5724]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9307311773300171 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009778098203241825\n",
      "Epoch: 1934 \n",
      " Hypothesis: tensor([[152.3080],\n",
      "        [184.0171],\n",
      "        [180.8209],\n",
      "        [196.9383],\n",
      "        [140.5724]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9306951761245728 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009778235107660294\n",
      "Epoch: 1935 \n",
      " Hypothesis: tensor([[152.3080],\n",
      "        [184.0171],\n",
      "        [180.8209],\n",
      "        [196.9383],\n",
      "        [140.5724]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9306678771972656 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009778372012078762\n",
      "Epoch: 1936 \n",
      " Hypothesis: tensor([[152.3080],\n",
      "        [184.0171],\n",
      "        [180.8209],\n",
      "        [196.9383],\n",
      "        [140.5724]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9306297302246094 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977850891649723\n",
      "Epoch: 1937 \n",
      " Hypothesis: tensor([[152.3080],\n",
      "        [184.0172],\n",
      "        [180.8209],\n",
      "        [196.9383],\n",
      "        [140.5725]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9305947422981262 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009778645820915699\n",
      "Epoch: 1938 \n",
      " Hypothesis: tensor([[152.3079],\n",
      "        [184.0172],\n",
      "        [180.8209],\n",
      "        [196.9383],\n",
      "        [140.5725]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9305626153945923 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009778782725334167\n",
      "Epoch: 1939 \n",
      " Hypothesis: tensor([[152.3079],\n",
      "        [184.0172],\n",
      "        [180.8209],\n",
      "        [196.9383],\n",
      "        [140.5725]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9305294156074524 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009778919629752636\n",
      "Epoch: 1940 \n",
      " Hypothesis: tensor([[152.3079],\n",
      "        [184.0172],\n",
      "        [180.8209],\n",
      "        [196.9383],\n",
      "        [140.5726]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9304925799369812 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009779056534171104\n",
      "Epoch: 1941 \n",
      " Hypothesis: tensor([[152.3078],\n",
      "        [184.0172],\n",
      "        [180.8209],\n",
      "        [196.9383],\n",
      "        [140.5726]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9304604530334473 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009779193438589573\n",
      "Epoch: 1942 \n",
      " Hypothesis: tensor([[152.3078],\n",
      "        [184.0173],\n",
      "        [180.8208],\n",
      "        [196.9383],\n",
      "        [140.5726]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9304221868515015 \n",
      " W: tensor([[0.7344],\n",
      "        [0.5950],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009779330343008041\n",
      "Epoch: 1943 \n",
      " Hypothesis: tensor([[152.3078],\n",
      "        [184.0173],\n",
      "        [180.8208],\n",
      "        [196.9383],\n",
      "        [140.5726]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9303953051567078 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00977946724742651\n",
      "Epoch: 1944 \n",
      " Hypothesis: tensor([[152.3078],\n",
      "        [184.0173],\n",
      "        [180.8208],\n",
      "        [196.9382],\n",
      "        [140.5727]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9303514361381531 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009779604151844978\n",
      "Epoch: 1945 \n",
      " Hypothesis: tensor([[152.3077],\n",
      "        [184.0173],\n",
      "        [180.8208],\n",
      "        [196.9383],\n",
      "        [140.5727]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9303239583969116 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009779741056263447\n",
      "Epoch: 1946 \n",
      " Hypothesis: tensor([[152.3077],\n",
      "        [184.0173],\n",
      "        [180.8208],\n",
      "        [196.9382],\n",
      "        [140.5727]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.930298924446106 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009779877960681915\n",
      "Epoch: 1947 \n",
      " Hypothesis: tensor([[152.3077],\n",
      "        [184.0173],\n",
      "        [180.8208],\n",
      "        [196.9382],\n",
      "        [140.5728]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9302619099617004 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009780014865100384\n",
      "Epoch: 1948 \n",
      " Hypothesis: tensor([[152.3076],\n",
      "        [184.0174],\n",
      "        [180.8208],\n",
      "        [196.9382],\n",
      "        [140.5728]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9302180409431458 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009780151769518852\n",
      "Epoch: 1949 \n",
      " Hypothesis: tensor([[152.3076],\n",
      "        [184.0174],\n",
      "        [180.8208],\n",
      "        [196.9382],\n",
      "        [140.5728]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9301857948303223 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00978028867393732\n",
      "Epoch: 1950 \n",
      " Hypothesis: tensor([[152.3076],\n",
      "        [184.0174],\n",
      "        [180.8208],\n",
      "        [196.9382],\n",
      "        [140.5728]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9301586151123047 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00978042557835579\n",
      "Epoch: 1951 \n",
      " Hypothesis: tensor([[152.3075],\n",
      "        [184.0174],\n",
      "        [180.8208],\n",
      "        [196.9382],\n",
      "        [140.5729]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9301234483718872 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009780562482774258\n",
      "Epoch: 1952 \n",
      " Hypothesis: tensor([[152.3075],\n",
      "        [184.0175],\n",
      "        [180.8208],\n",
      "        [196.9382],\n",
      "        [140.5729]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9300855398178101 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009780699387192726\n",
      "Epoch: 1953 \n",
      " Hypothesis: tensor([[152.3075],\n",
      "        [184.0175],\n",
      "        [180.8208],\n",
      "        [196.9382],\n",
      "        [140.5729]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9300583600997925 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009780836291611195\n",
      "Epoch: 1954 \n",
      " Hypothesis: tensor([[152.3074],\n",
      "        [184.0175],\n",
      "        [180.8208],\n",
      "        [196.9382],\n",
      "        [140.5730]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9300252199172974 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009780973196029663\n",
      "Epoch: 1955 \n",
      " Hypothesis: tensor([[152.3074],\n",
      "        [184.0175],\n",
      "        [180.8207],\n",
      "        [196.9382],\n",
      "        [140.5730]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9299825429916382 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009781110100448132\n",
      "Epoch: 1956 \n",
      " Hypothesis: tensor([[152.3074],\n",
      "        [184.0175],\n",
      "        [180.8207],\n",
      "        [196.9381],\n",
      "        [140.5730]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9299436807632446 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.0097812470048666\n",
      "Epoch: 1957 \n",
      " Hypothesis: tensor([[152.3074],\n",
      "        [184.0175],\n",
      "        [180.8207],\n",
      "        [196.9381],\n",
      "        [140.5730]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9299114346504211 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009781383909285069\n",
      "Epoch: 1958 \n",
      " Hypothesis: tensor([[152.3073],\n",
      "        [184.0176],\n",
      "        [180.8207],\n",
      "        [196.9381],\n",
      "        [140.5731]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9298785924911499 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009781520813703537\n",
      "Epoch: 1959 \n",
      " Hypothesis: tensor([[152.3073],\n",
      "        [184.0176],\n",
      "        [180.8207],\n",
      "        [196.9381],\n",
      "        [140.5731]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9298404455184937 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009781657718122005\n",
      "Epoch: 1960 \n",
      " Hypothesis: tensor([[152.3073],\n",
      "        [184.0176],\n",
      "        [180.8207],\n",
      "        [196.9381],\n",
      "        [140.5731]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9298073053359985 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009781794622540474\n",
      "Epoch: 1961 \n",
      " Hypothesis: tensor([[152.3072],\n",
      "        [184.0176],\n",
      "        [180.8207],\n",
      "        [196.9381],\n",
      "        [140.5732]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9297733306884766 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009781931526958942\n",
      "Epoch: 1962 \n",
      " Hypothesis: tensor([[152.3072],\n",
      "        [184.0177],\n",
      "        [180.8207],\n",
      "        [196.9381],\n",
      "        [140.5732]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9297402501106262 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009782068431377411\n",
      "Epoch: 1963 \n",
      " Hypothesis: tensor([[152.3072],\n",
      "        [184.0177],\n",
      "        [180.8207],\n",
      "        [196.9381],\n",
      "        [140.5732]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9297217130661011 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00978220533579588\n",
      "Epoch: 1964 \n",
      " Hypothesis: tensor([[152.3071],\n",
      "        [184.0177],\n",
      "        [180.8206],\n",
      "        [196.9381],\n",
      "        [140.5732]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9296748042106628 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009782342240214348\n",
      "Epoch: 1965 \n",
      " Hypothesis: tensor([[152.3071],\n",
      "        [184.0177],\n",
      "        [180.8206],\n",
      "        [196.9381],\n",
      "        [140.5733]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9296438097953796 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5949],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009782479144632816\n",
      "Epoch: 1966 \n",
      " Hypothesis: tensor([[152.3071],\n",
      "        [184.0177],\n",
      "        [180.8206],\n",
      "        [196.9381],\n",
      "        [140.5733]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9296067953109741 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009782616049051285\n",
      "Epoch: 1967 \n",
      " Hypothesis: tensor([[152.3071],\n",
      "        [184.0178],\n",
      "        [180.8206],\n",
      "        [196.9381],\n",
      "        [140.5733]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9295815229415894 \n",
      " W: tensor([[0.7345],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009782752953469753\n",
      "Epoch: 1968 \n",
      " Hypothesis: tensor([[152.3070],\n",
      "        [184.0178],\n",
      "        [180.8206],\n",
      "        [196.9381],\n",
      "        [140.5733]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9295493364334106 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009782889857888222\n",
      "Epoch: 1969 \n",
      " Hypothesis: tensor([[152.3070],\n",
      "        [184.0178],\n",
      "        [180.8206],\n",
      "        [196.9380],\n",
      "        [140.5734]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9295104742050171 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00978302676230669\n",
      "Epoch: 1970 \n",
      " Hypothesis: tensor([[152.3070],\n",
      "        [184.0178],\n",
      "        [180.8206],\n",
      "        [196.9380],\n",
      "        [140.5734]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9294706583023071 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009783163666725159\n",
      "Epoch: 1971 \n",
      " Hypothesis: tensor([[152.3069],\n",
      "        [184.0178],\n",
      "        [180.8206],\n",
      "        [196.9380],\n",
      "        [140.5734]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9294384717941284 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009783300571143627\n",
      "Epoch: 1972 \n",
      " Hypothesis: tensor([[152.3069],\n",
      "        [184.0179],\n",
      "        [180.8206],\n",
      "        [196.9380],\n",
      "        [140.5735]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9294053912162781 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009783437475562096\n",
      "Epoch: 1973 \n",
      " Hypothesis: tensor([[152.3069],\n",
      "        [184.0179],\n",
      "        [180.8206],\n",
      "        [196.9380],\n",
      "        [140.5735]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9293723106384277 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009783574379980564\n",
      "Epoch: 1974 \n",
      " Hypothesis: tensor([[152.3069],\n",
      "        [184.0179],\n",
      "        [180.8206],\n",
      "        [196.9380],\n",
      "        [140.5735]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.929336428642273 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009783711284399033\n",
      "Epoch: 1975 \n",
      " Hypothesis: tensor([[152.3068],\n",
      "        [184.0179],\n",
      "        [180.8206],\n",
      "        [196.9380],\n",
      "        [140.5736]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9293092489242554 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009783848188817501\n",
      "Epoch: 1976 \n",
      " Hypothesis: tensor([[152.3068],\n",
      "        [184.0179],\n",
      "        [180.8205],\n",
      "        [196.9380],\n",
      "        [140.5736]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9292634129524231 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00978398509323597\n",
      "Epoch: 1977 \n",
      " Hypothesis: tensor([[152.3068],\n",
      "        [184.0180],\n",
      "        [180.8205],\n",
      "        [196.9380],\n",
      "        [140.5736]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9292507171630859 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009784121997654438\n",
      "Epoch: 1978 \n",
      " Hypothesis: tensor([[152.3067],\n",
      "        [184.0180],\n",
      "        [180.8205],\n",
      "        [196.9380],\n",
      "        [140.5736]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.929212749004364 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009784258902072906\n",
      "Epoch: 1979 \n",
      " Hypothesis: tensor([[152.3067],\n",
      "        [184.0180],\n",
      "        [180.8205],\n",
      "        [196.9380],\n",
      "        [140.5737]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9291739463806152 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009784395806491375\n",
      "Epoch: 1980 \n",
      " Hypothesis: tensor([[152.3067],\n",
      "        [184.0180],\n",
      "        [180.8205],\n",
      "        [196.9379],\n",
      "        [140.5737]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9291273355484009 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009784532710909843\n",
      "Epoch: 1981 \n",
      " Hypothesis: tensor([[152.3066],\n",
      "        [184.0181],\n",
      "        [180.8205],\n",
      "        [196.9379],\n",
      "        [140.5737]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9290941953659058 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009784669615328312\n",
      "Epoch: 1982 \n",
      " Hypothesis: tensor([[152.3066],\n",
      "        [184.0181],\n",
      "        [180.8205],\n",
      "        [196.9379],\n",
      "        [140.5737]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9290757179260254 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00978480651974678\n",
      "Epoch: 1983 \n",
      " Hypothesis: tensor([[152.3066],\n",
      "        [184.0181],\n",
      "        [180.8205],\n",
      "        [196.9379],\n",
      "        [140.5738]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9290347099304199 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009784943424165249\n",
      "Epoch: 1984 \n",
      " Hypothesis: tensor([[152.3065],\n",
      "        [184.0181],\n",
      "        [180.8205],\n",
      "        [196.9379],\n",
      "        [140.5738]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9289997220039368 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009785080328583717\n",
      "Epoch: 1985 \n",
      " Hypothesis: tensor([[152.3065],\n",
      "        [184.0181],\n",
      "        [180.8205],\n",
      "        [196.9379],\n",
      "        [140.5738]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9289576411247253 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009785217233002186\n",
      "Epoch: 1986 \n",
      " Hypothesis: tensor([[152.3065],\n",
      "        [184.0181],\n",
      "        [180.8205],\n",
      "        [196.9379],\n",
      "        [140.5739]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9289366006851196 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009785354137420654\n",
      "Epoch: 1987 \n",
      " Hypothesis: tensor([[152.3065],\n",
      "        [184.0182],\n",
      "        [180.8204],\n",
      "        [196.9379],\n",
      "        [140.5739]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9288997650146484 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009785491041839123\n",
      "Epoch: 1988 \n",
      " Hypothesis: tensor([[152.3064],\n",
      "        [184.0182],\n",
      "        [180.8204],\n",
      "        [196.9379],\n",
      "        [140.5739]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9288695454597473 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009785627946257591\n",
      "Epoch: 1989 \n",
      " Hypothesis: tensor([[152.3064],\n",
      "        [184.0182],\n",
      "        [180.8204],\n",
      "        [196.9379],\n",
      "        [140.5739]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9288256764411926 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5948],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00978576485067606\n",
      "Epoch: 1990 \n",
      " Hypothesis: tensor([[152.3064],\n",
      "        [184.0182],\n",
      "        [180.8204],\n",
      "        [196.9379],\n",
      "        [140.5740]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9288042187690735 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009785901755094528\n",
      "Epoch: 1991 \n",
      " Hypothesis: tensor([[152.3064],\n",
      "        [184.0182],\n",
      "        [180.8204],\n",
      "        [196.9379],\n",
      "        [140.5740]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9287606477737427 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009786038659512997\n",
      "Epoch: 1992 \n",
      " Hypothesis: tensor([[152.3063],\n",
      "        [184.0183],\n",
      "        [180.8204],\n",
      "        [196.9379],\n",
      "        [140.5740]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9287362098693848 \n",
      " W: tensor([[0.7346],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009786175563931465\n",
      "Epoch: 1993 \n",
      " Hypothesis: tensor([[152.3063],\n",
      "        [184.0183],\n",
      "        [180.8204],\n",
      "        [196.9379],\n",
      "        [140.5741]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9287041425704956 \n",
      " W: tensor([[0.7347],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009786312468349934\n",
      "Epoch: 1994 \n",
      " Hypothesis: tensor([[152.3063],\n",
      "        [184.0183],\n",
      "        [180.8204],\n",
      "        [196.9378],\n",
      "        [140.5741]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9286595582962036 \n",
      " W: tensor([[0.7347],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009786449372768402\n",
      "Epoch: 1995 \n",
      " Hypothesis: tensor([[152.3062],\n",
      "        [184.0183],\n",
      "        [180.8204],\n",
      "        [196.9378],\n",
      "        [140.5741]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9286273717880249 \n",
      " W: tensor([[0.7347],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.00978658627718687\n",
      "Epoch: 1996 \n",
      " Hypothesis: tensor([[152.3062],\n",
      "        [184.0184],\n",
      "        [180.8204],\n",
      "        [196.9378],\n",
      "        [140.5741]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9285942912101746 \n",
      " W: tensor([[0.7347],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009786723181605339\n",
      "Epoch: 1997 \n",
      " Hypothesis: tensor([[152.3062],\n",
      "        [184.0184],\n",
      "        [180.8204],\n",
      "        [196.9378],\n",
      "        [140.5742]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9285621643066406 \n",
      " W: tensor([[0.7347],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009786860086023808\n",
      "Epoch: 1998 \n",
      " Hypothesis: tensor([[152.3061],\n",
      "        [184.0184],\n",
      "        [180.8204],\n",
      "        [196.9378],\n",
      "        [140.5742]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9285292625427246 \n",
      " W: tensor([[0.7347],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009786996990442276\n",
      "Epoch: 1999 \n",
      " Hypothesis: tensor([[152.3061],\n",
      "        [184.0184],\n",
      "        [180.8204],\n",
      "        [196.9378],\n",
      "        [140.5742]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9284904599189758 \n",
      " W: tensor([[0.7347],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009787133894860744\n",
      "Epoch: 2000 \n",
      " Hypothesis: tensor([[152.3061],\n",
      "        [184.0184],\n",
      "        [180.8203],\n",
      "        [196.9378],\n",
      "        [140.5743]], grad_fn=<AddBackward0>) \n",
      " Cost: 0.9284473657608032 \n",
      " W: tensor([[0.7347],\n",
      "        [0.5947],\n",
      "        [0.6812]], requires_grad=True) \n",
      " b: 0.009787270799279213\n"
     ]
    }
   ],
   "source": [
    "optimizer= optim.SGD([W,b], lr=1e-6)\n",
    "nb_epochs= 2000\n",
    "\n",
    "for epoch in range(nb_epochs+1): #+1하는이유 = 그래야 2000번째것이 나오니깐\n",
    "\n",
    "    y=x_data.matmul(W)+b #y=예측값 \n",
    "    cost=torch.mean((y-t_data)**2) #cost = 비용함수\n",
    "\n",
    "    optimizer.zero_grad()  #gradient를 초기화하는것\n",
    "    cost.backward() #비용함수 계산\n",
    "    optimizer.step()  #w,b업데이트\n",
    "    print('Epoch:', epoch, '\\n', 'Hypothesis:', y, '\\n' ,'Cost:', cost.item(), '\\n', 'W:', W, '\\n', 'b:',b.item())\n",
    "\n",
    "    # if epoch % 100==0:\n",
    "    #     print('EPOCH {:4d}/{}, Hypothesis:{}, Cost:{:.6f}'\\\n",
    "    #             .format(epoch, nb_epochs, y.squeeze().detach(), cost.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
