{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배열(array): 데이터를 효율적으로 저장하고 다루기 위해 사용.\\\n",
    " NumPy를 사용하여 배열을 생성하고, 배열 간의 연산이나 슬라이싱과 같은 다양한 작업을 수행할 수 있음.\n",
    "\n",
    "- 텐서(Tensor): 주로 머신러닝과 딥러닝에서 데이터를 다루기 위해 사용.\\\n",
    " 딥러닝 모델은 주로 텐서를 입력으로 받고, 텐서를 출력으로 내보냄.\\\n",
    "  딥러닝 라이브러리인 파이토치와 텐서플로는 텐서를 생성하고, 뉴럴 네트워크의 학습과 예측 과정에서 텐서를 다루는 기능을 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2], [3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.FloatTensor([[1,2,3], [4,5,6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([[1,2,],[3,4]]))\n",
    "# print(torch.tensor([[1,2,],[3,4]]), device=\"cuda:0\") #cuda tenser만드는것\n",
    "print(torch.tensor([[1,2,],[3,4]], dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "ft=torch.FloatTensor([[1,2,],[3,4]])\n",
    "lt=torch.LongTensor([[1,2],[3,4]])\n",
    "bt=torch.ByteTensor([[1,2],[3,4]]) #8bit로 작성되었단 결과가 나옴\n",
    "print(ft)\n",
    "print(lt)\n",
    "print(bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]] <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nparray\n",
    "import numpy as np\n",
    "\n",
    "x=np.arange(0,9).reshape(3,3)\n",
    "print(x, type(x))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#array->torch로 변환\n",
    "tx=torch.from_numpy(x)\n",
    "print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#torch->nparray\n",
    "nx=tx.numpy()\n",
    "print(nx, type(nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "tx #<- long형인데  float형으로, float형인데 long형으로\n",
    "ft=tx.float()\n",
    "print(ft)\n",
    "lt= ft.long()\n",
    "print(lt)\n",
    "bt=ft.byte()\n",
    "print(bt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  2],\n",
       "        [ 3,  4],\n",
       "        [ 5,  6]],\n",
       "\n",
       "       [[ 7,  8],\n",
       "        [ 9, 10],\n",
       "        [11, 12]]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na=np.arange(1,13).reshape(2,3,2) #(z, y, x축)\n",
    "na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=torch.from_numpy(na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2])\n",
      "torch.Size([2, 3, 2])\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(t.size())  # 텐서의 크기를 출력합니다.\n",
    "\n",
    "print(t.shape)  # 텐서의 모양(shape)을 튜플 형태로 출력합니다.\n",
    "\n",
    "print(t.size(0))  # 텐서의 첫 번째 차원의 크기를 출력합니다.\n",
    "\n",
    "print(t.shape[2])  # 텐서의 세 번째 차원의 크기를 출력합니다.\n",
    "\n",
    "print(t.dim())  # 텐서의 차원 수를 출력합니다.\n",
    "\n",
    "print(len(t.shape))  # 텐서의 차원 수를 출력합니다. t.dim()과 동일한 결과입니다.\n",
    "\n",
    "print(t.size(-1))  # 텐서의 마지막 차원의 크기를 출력합니다.\n",
    "\n",
    "print(t.shape[-1])  # 텐서의 마지막 차원의 크기를 출력합니다. t.size(-1)과 동일한 결과입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[False, False],\n",
      "        [False, False]])\n",
      "tensor([[  1.,   4.],\n",
      "        [ 27., 256.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1, 2], [3, 4]])  # 2x2 크기의 텐서 a를 생성합니다.\n",
    "b = torch.Tensor([[1, 2], [3, 4]])  # 2x2 크기의 텐서 b를 생성합니다.\n",
    "\n",
    "print(a + b)  # 행렬 덧셈: 텐서 a와 b의 요소를 더하여 결과를 출력합니다.\n",
    "print(a - b)  # 행렬 뺄셈: 텐서 a와 b의 요소를 빼서 결과를 출력합니다.\n",
    "print(a * b)  # 요소별 곱셈: 텐서 a와 b의 요소를 곱하여 결과를 출력합니다.\n",
    "print(a / b)  # 요소별 나눗셈: 텐서 a와 b의 요소를 나누어 결과를 출력합니다.\n",
    "print(a == b)  # 요소별 비교 (같음): 텐서 a와 b의 요소가 같은지 비교하여 True 또는 False를 출력합니다.\n",
    "print(a != b)  # 요소별 비교 (다름): 텐서 a와 b의 요소가 다른지 비교하여 True 또는 False를 출력합니다.\n",
    "print(a ** b)  # 요소별 거듭제곱: 텐서 a의 요소를 텐서 b의 요소만큼 제곱하여 결과를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3.],\n",
      "        [4., 5.]])\n",
      "tensor([[-2., -1.],\n",
      "        [ 0.,  1.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "tensor([[0., 1.],\n",
      "        [1., 2.]])\n",
      "tensor([[False,  True],\n",
      "        [False, False]])\n",
      "tensor([[False, False],\n",
      "        [ True,  True]])\n",
      "tensor([[ True, False],\n",
      "        [ True,  True]])\n"
     ]
    }
   ],
   "source": [
    "print(a+1)\n",
    "print(a-3)\n",
    "print(a*2)\n",
    "print(a/2)\n",
    "print(a//2)  #// 연산자는 파이썬에서 사용되는 정수 나눗셈(나누기 연산의 몫을 계산)을 수행하는 연산자\n",
    "print(a==2)\n",
    "print(a>2)\n",
    "print(a!=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([1, 2])\n",
      "tensor([[2., 4.],\n",
      "        [4., 6.]])\n"
     ]
    }
   ],
   "source": [
    "c=torch.tensor([1,2])\n",
    "print(a)\n",
    "print(c)\n",
    "print(a+c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n",
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "print(a*b) #<- 같은 행 열의 자리의값들끼리 곱함\n",
    "print(a)\n",
    "print(a.mul_(b)) # (= print(a*b)과 같은 기능이지만, a의 값이 변해버림\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n",
      "tensor(30.)\n",
      "tensor([10., 20.])\n",
      "tensor([ 5., 25.])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a.sum())\n",
    "print(a.sum(dim=0)) #열끼리 더하기\n",
    "print(a.sum(dim=1)) #행끼리 더하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[ 5,  6],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [11, 12]]], dtype=torch.int32)\n",
      "tensor([[15, 18],\n",
      "        [21, 24]])\n",
      "tensor([[ 4,  6],\n",
      "        [12, 14],\n",
      "        [20, 22]])\n",
      "tensor([[ 3,  7],\n",
      "        [11, 15],\n",
      "        [19, 23]])\n",
      "tensor([[ 3,  7],\n",
      "        [11, 15],\n",
      "        [19, 23]])\n"
     ]
    }
   ],
   "source": [
    "t = np.arange(1, 13).reshape(3, 2, 2)  # 1부터 12까지의 숫자를 3x2x2 크기의 다차원 배열로 생성합니다. # (z, y, x축) 순서로 다차원 배열이 구성됩니다.\n",
    "t = torch.from_numpy(t)  # NumPy 배열을 파이토치 텐서로 변환합니다.\n",
    "\n",
    "print(t)  # 3차원 텐서를 출력합니다.\n",
    "print(t.sum(dim=0))  # 첫 번째 축(z 축)을 따라 텐서의 합을 계산합니다.\n",
    "print(t.sum(dim=1))  # 두 번째 축(y 축)을 따라 텐서의 합을 계산합니다.\n",
    "print(t.sum(dim=2))  # 세 번째 축(x 축)을 따라 텐서의 합을 계산합니다.\n",
    "print(t.sum(dim=-1))  # 마지막 축(x 축)을 따라 텐서의 합을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.]],\n",
      "\n",
      "        [[ 5.,  6.],\n",
      "         [ 7.,  8.]],\n",
      "\n",
      "        [[ 9., 10.],\n",
      "         [11., 12.]]])\n",
      "tensor(6.5000)\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[ 2.,  3.],\n",
      "        [ 6.,  7.],\n",
      "        [10., 11.]])\n",
      "tensor([[ 1.5000,  3.5000],\n",
      "        [ 5.5000,  7.5000],\n",
      "        [ 9.5000, 11.5000]])\n",
      "tensor([[ 1.5000,  3.5000],\n",
      "        [ 5.5000,  7.5000],\n",
      "        [ 9.5000, 11.5000]])\n"
     ]
    }
   ],
   "source": [
    "t=t.float()\n",
    "print(t)\n",
    "print(t.mean())\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=1))\n",
    "print(t.mean(dim=2))\n",
    "print(t.mean(dim=-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2  4]\n",
      "  [ 6  8]\n",
      "  [10 12]]\n",
      "\n",
      " [[14 16]\n",
      "  [18 20]\n",
      "  [22 24]]] \n",
      "\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5]],\n",
      "\n",
      "        [[ 6,  7],\n",
      "         [ 8,  9],\n",
      "         [10, 11]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a=np.arange(12).reshape(2,3,2)\n",
    "b=np.arange(2,14).reshape(2,3,2)\n",
    "c=np.arange(1,7).reshape(3,2)\n",
    "\n",
    "#원래의 NumPy 배열이 변경되지 않기를 원한다면 torch.tensor(a)를 사용하여 새로운 텐서를 생성하는 방법을 선택해야 합니다.\n",
    "# 이렇게 하면 NumPy 배열과 텐서가 독립적인 메모리를 사용하므로 한쪽의 변경이 다른 쪽에 영향을 주지 않습니다.\n",
    "d=torch.tensor(a) #값이 바뀌지는 않음\n",
    "d=torch.from_numpy(a) #값이 바뀜\n",
    "\n",
    "print(a+b,\"\\n\\n\",c)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(1,13)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]]) \n",
      "\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]]) \n",
      "\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]]) \n",
      "\n",
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4],\n",
      "         [ 5,  6]],\n",
      "\n",
      "        [[ 7,  8],\n",
      "         [ 9, 10],\n",
      "         [11, 12]]]) \n",
      "\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n"
     ]
    }
   ],
   "source": [
    "print(x.view(3,4),\"\\n\")\n",
    "print(x.reshape(3,4),\"\\n\")\n",
    "print(x.view(4,3),\"\\n\")\n",
    "print(x.view(2,3,2),\"\\n\")\n",
    "print(x.view(-1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  2],\n",
       "          [ 3,  4],\n",
       "          [ 5,  6]],\n",
       "\n",
       "         [[ 7,  8],\n",
       "          [ 9, 10],\n",
       "          [11, 12]]]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(1,13)\n",
    "x1=x.view(1,2,3,2)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  2],\n",
       "          [ 3,  4],\n",
       "          [ 5,  6]],\n",
       "\n",
       "         [[ 7,  8],\n",
       "          [ 9, 10],\n",
       "          [11, 12]]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#squeeze()-> 1차원인 것들을 제거\n",
    "#unsqueeze() ->1차원을 추가하는것\n",
    "\n",
    "# x1= x1.unsqueeze(1)\n",
    "x1\n",
    "x1= x1.squeeze(1)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2],\n",
       "         [ 3,  4],\n",
       "         [ 5,  6]],\n",
       "\n",
       "        [[ 7,  8],\n",
       "         [ 9, 10],\n",
       "         [11, 12]]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2=x.view(2,3,2)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4],\n",
      "         [ 5,  6]],\n",
      "\n",
      "        [[ 7,  8],\n",
      "         [ 9, 10],\n",
      "         [11, 12]]])\n",
      "tensor([ 9, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x2=x.view(2,3,2)\n",
    "print(x2)\n",
    "x2\n",
    "#x[a,b] -> a=z축의 순서/ b=해당 차원의 행순서\n",
    "print(x2[1,1]) #2번째 z축의 2번째 행\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mFloatTensor(\u001b[39m4\u001b[39m,\u001b[39m10\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m splits\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39msplit(\u001b[39m3\u001b[39m,dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m splits:\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'splits' is not defined"
     ]
    }
   ],
   "source": [
    "x=torch.FloatTensor(4,10)\n",
    "splits.x.split(3,dim=1)\n",
    "for s in splits:\n",
    "    print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[ 5,  6],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [11, 12]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2],\n",
       "         [ 3,  4]],\n",
       "\n",
       "        [[ 5,  6],\n",
       "         [ 7,  8]],\n",
       "\n",
       "        [[ 9, 10],\n",
       "         [11, 12]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(1,13).view(3,2,2)\n",
    "print(x)\n",
    "index=torch.LongTensor([0,1])\n",
    "\n",
    "\n",
    "\n",
    "y=x.index_select(dim=1, index=index)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3.],\n",
       "         [4., 5., 6.],\n",
       "         [7., 8., 9.]]),\n",
       " tensor([[10., 11., 12.],\n",
       "         [13., 14., 15.],\n",
       "         [16., 17., 18.]]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.FloatTensor([[1,2,3], [4,5,6], [7,8,9]])\n",
    "y= torch.FloatTensor([[10,11,12], [13,14,15], [16,17,18]])\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.],\n",
      "        [13., 14., 15.],\n",
      "        [16., 17., 18.]])\n",
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "z=torch.cat([x,y], dim=0)\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3., 10., 11., 12.],\n",
      "        [ 4.,  5.,  6., 13., 14., 15.],\n",
      "        [ 7.,  8.,  9., 16., 17., 18.]])\n",
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "z=torch.cat([x,y], dim=1)\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[10., 11., 12.],\n",
      "         [13., 14., 15.],\n",
      "         [16., 17., 18.]]])\n"
     ]
    }
   ],
   "source": [
    "z=torch.stack([x,y])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1., 10.],\n",
      "         [ 2., 11.],\n",
      "         [ 3., 12.]],\n",
      "\n",
      "        [[ 4., 13.],\n",
      "         [ 5., 14.],\n",
      "         [ 6., 15.]],\n",
      "\n",
      "        [[ 7., 16.],\n",
      "         [ 8., 17.],\n",
      "         [ 9., 18.]]]) \n",
      "\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[10., 11., 12.],\n",
      "         [13., 14., 15.],\n",
      "         [16., 17., 18.]]]) \n",
      "\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [10., 11., 12.]],\n",
      "\n",
      "        [[ 4.,  5.,  6.],\n",
      "         [13., 14., 15.]],\n",
      "\n",
      "        [[ 7.,  8.,  9.],\n",
      "         [16., 17., 18.]]]) \n",
      "\n",
      "tensor([[[ 1., 10.],\n",
      "         [ 2., 11.],\n",
      "         [ 3., 12.]],\n",
      "\n",
      "        [[ 4., 13.],\n",
      "         [ 5., 14.],\n",
      "         [ 6., 15.]],\n",
      "\n",
      "        [[ 7., 16.],\n",
      "         [ 8., 17.],\n",
      "         [ 9., 18.]]])\n"
     ]
    }
   ],
   "source": [
    "print(z,\"\\n\")\n",
    "z=torch.stack([x,y], dim=0)\n",
    "print(z,\"\\n\")\n",
    "z=torch.stack([x,y], dim=1)\n",
    "print(z,\"\\n\")\n",
    "z=torch.stack([x,y], dim=2)\n",
    "print(z)\n",
    "\n",
    "\n",
    "#torch.stack([x, y], dim=0):\n",
    "#x와 y를 첫 번째 차원(0번 축)을 기준으로 쌓아서 새로운 3차원 텐서 z를 만듭니다.\n",
    "\n",
    "#torch.stack([x, y], dim=1):\n",
    "#x와 y를 두 번째 차원(1번 축)을 기준으로 쌓아서 새로운 3차원 텐서 z를 만듭니다.\n",
    "\n",
    "#torch.stack([x, y], dim=2):\n",
    "#x와 y를 세 번째 차원(2번 축)을 기준으로 쌓아서 새로운 4차원 텐서 z를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.]],\n",
      "\n",
      "        [[3., 4.]]])\n",
      "torch.Size([2, 1, 2]) \n",
      "\n",
      "\n",
      "\n",
      "tensor([[[1., 2.],\n",
      "         [1., 2.],\n",
      "         [1., 2.]],\n",
      "\n",
      "        [[3., 4.],\n",
      "         [3., 4.],\n",
      "         [3., 4.]]])\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "x= torch.FloatTensor([[[1,2]], [[3,4]]])\n",
    "print(x)\n",
    "print(x.size(),\"\\n\\n\\n\")\n",
    "\n",
    "y=x.expand(2,3,2)\n",
    "print(y)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 7, 8, 5, 6, 2, 1, 9, 3])\n",
      "torch.Size([10]) \n",
      "\n",
      "\n",
      "\n",
      "tensor([[[ 0, 15,  7],\n",
      "         [24, 26, 18],\n",
      "         [ 2,  6, 19]],\n",
      "\n",
      "        [[13, 17,  3],\n",
      "         [20, 12,  9],\n",
      "         [14, 22, 16]],\n",
      "\n",
      "        [[ 1,  5,  4],\n",
      "         [23, 21,  8],\n",
      "         [11, 25, 10]]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randperm(10)\n",
    "print(x)\n",
    "print(x.size(),\"\\n\\n\\n\")\n",
    "\n",
    "x3=torch.randperm(3**3).view(3,3,-1)\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[16, 13,  1],\n",
      "         [22,  0, 26],\n",
      "         [14, 10, 12]],\n",
      "\n",
      "        [[ 5, 15, 17],\n",
      "         [ 4, 18, 20],\n",
      "         [11,  9,  2]],\n",
      "\n",
      "        [[ 8,  6, 24],\n",
      "         [ 3, 19, 25],\n",
      "         [23,  7, 21]]])\n",
      "tensor([[0, 2, 0],\n",
      "        [2, 2, 0],\n",
      "        [2, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "x3=torch.randperm(3**3).view(3,3,-1)\n",
    "print(x3)\n",
    "# y=x3.argmax(dim=-1)\n",
    "# print(y)\n",
    "# y=x3.argmax(dim=0)\n",
    "# print(y)\n",
    "# y=x3.argmax(dim=1)\n",
    "# print(y)\n",
    "y=x3.argmax(dim=2)\n",
    "print(y)\n",
    "\n",
    "#dim=0 =>z축\n",
    "#dim=1 =>y축\n",
    "#dim=2 =>x축\n",
    "\n",
    "#y[0,0]-> 첫번째 차원에 1행에서 가장큰 값은 16이 나옴, 16의 위치는 0\n",
    "#y[0,1]->첫번째 차원에 2행에서 가장큰 값은 26이 나옴, 26의 위치는 2\n",
    "#y[0,3]->첫번째 차원에 2행에서 가장큰 값은 14이 나옴, 14의 위치는 0\n",
    "#그래서 y의 첫번째 행이 [0,2,0]이됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
